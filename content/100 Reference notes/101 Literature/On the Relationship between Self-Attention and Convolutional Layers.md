---
authors:
- '[[Jean-Baptiste Cordonnier]]'
- '[[Andreas Loukas]]'
- '[[Martin Jaggi]]'
tags:
- transformers
- convolutions
- theory
year: 2020
url: https://arxiv.org/abs/1911.03584
date: '2019-11-08'
---

> [!info] Abstract
> Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, [[Stand-Alone Self-Attention in Vision Models]] showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? **This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer.** Our numerical experiments then show that **self-attention layers attend to pixel-grid patterns similarly to CNN layers, corroborating our analysis**. 

## Notes

- [ ] Note to self: fully read article, it looks fun ‚è´ #personal 