---
authors:
- '[[Bo Zhao]]'
- '[[Robert M Gower]]'
- '[[Robin Walters]]'
- '[[Rose Yu]]'
year: 2023
tags:
- equivariance
- relaxed_equivariance
- dl_theory
url: https://arxiv.org/abs/2305.13404
date: '2023-05-22'
---

> [!info] Abstract
> In overparametrized models, different values of the parameters may result in the same loss value. Parameter space symmetries are transformations that change the model parameters but leave the loss invariant. Teleportation applies such transformations to accelerate optimization. However, the exact mechanism behind this algorithm's success is not well understood. In this paper, we show that teleportation not only speeds up optimization in the short-term, but gives overall faster time to convergence. Additionally, we show that teleporting to minima with different curvatures improves generalization and provide insights on the connection between the curvature of the minima and generalization ability. Finally, we show that integrating teleportation into a wide range of optimization algorithms and optimization-based meta-learning improves convergence.

