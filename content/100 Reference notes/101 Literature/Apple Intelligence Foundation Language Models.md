---
authors:
  - "[[Tom Gunter]]"
  - "[[Zirui Wang]]"
  - "[[Chong Wang]]"
  - "[[Ruoming Pang]]"
  - "[[Andy Narayanan]]"
  - "[[Aonan Zhang]]"
  - "[[Bowen Zhang]]"
  - "[[Chen Chen]]"
  - "[[Chung-Cheng Chiu]]"
  - "[[David Qiu]]"
  - "[[Deepak Gopinath]]"
  - "[[Dian Ang Yap]]"
  - "[[Dong Yin]]"
  - "[[Feng Nan]]"
  - "[[Floris Weers]]"
  - "[[Guoli Yin]]"
  - "[[Haoshuo Huang]]"
  - "[[Jianyu Wang]]"
  - "[[Jiarui Lu]]"
  - "[[John Peebles]]"
  - "[[Ke Ye]]"
  - "[[Mark Lee]]"
  - "[[Nan Du]]"
  - "[[Qibin Chen]]"
  - "[[Quentin Keunebroek]]"
  - "[[Sam Wiseman]]"
  - "[[Syd Evans]]"
  - "[[Tao Lei]]"
  - "[[Vivek Rathod]]"
  - "[[Xiang Kong]]"
  - "[[Xianzhi Du]]"
  - "[[Yanghao Li]]"
  - "[[Yongqiang Wang]]"
  - "[[Yuan Gao]]"
  - "[[Zaid Ahmed]]"
  - "[[Zhaoyang Xu]]"
  - "[[Zhiyun Lu]]"
  - "[[Al Rashid]]"
  - "[[Albin Madappally Jose]]"
  - "[[Alec Doane]]"
  - "[[Alfredo Bencomo]]"
  - "[[Allison Vanderby]]"
  - "[[Andrew Hansen]]"
  - "[[Ankur Jain]]"
  - "[[Anupama Mann Anupama]]"
  - "[[Areeba Kamal]]"
  - "[[Bugu Wu]]"
  - "[[Carolina Brum]]"
  - "[[Charlie Maalouf]]"
  - "[[Chinguun Erdenebileg]]"
  - "[[Chris Dulhanty]]"
  - "[[Dominik Moritz]]"
  - "[[Doug Kang]]"
  - "[[Eduardo Jimenez]]"
  - "[[Evan Ladd]]"
  - "[[Fangping Shi]]"
  - "[[Felix Bai]]"
  - "[[Frank Chu]]"
  - "[[Fred Hohman]]"
  - "[[Hadas Kotek]]"
  - "[[Hannah Gillis Coleman]]"
  - "[[Jane Li]]"
  - "[[Jeffrey Bigham]]"
  - "[[Jeffery Cao]]"
  - "[[Jeff Lai]]"
  - "[[Jessica Cheung]]"
  - "[[Jiulong Shan]]"
  - "[[Joe Zhou]]"
  - "[[John Li]]"
  - "[[Jun Qin]]"
  - "[[Karanjeet Singh]]"
  - "[[Karla Vega]]"
  - "[[Kelvin Zou]]"
  - "[[Laura Heckman]]"
  - "[[Lauren Gardiner]]"
  - "[[Margit Bowler]]"
  - "[[Maria Cordell]]"
  - "[[Meng Cao]]"
  - "[[Nicole Hay]]"
  - "[[Nilesh Shahdadpuri]]"
  - "[[Otto Godwin]]"
  - "[[Pranay Dighe]]"
  - "[[Pushyami Rachapudi]]"
  - "[[Ramsey Tantawi]]"
  - "[[Roman Frigg]]"
  - "[[Sam Davarnia]]"
  - "[[Sanskruti Shah]]"
  - "[[Saptarshi Guha]]"
  - "[[Sasha Sirovica]]"
  - "[[Shen Ma]]"
  - "[[Shuang Ma]]"
  - "[[Simon Wang]]"
  - "[[Sulgi Kim]]"
  - "[[Suma Jayaram]]"
  - "[[Vaishaal Shankar]]"
  - "[[Varsha Paidi]]"
  - "[[Vivek Kumar]]"
  - "[[Xin Wang]]"
  - "[[Xin Zheng]]"
  - "[[Walker Cheng ]]"
  - "[[Yael Shrager]]"
  - "[[Yang Ye]]"
  - "[[Yasu Tanaka]]"
  - "[[Yihao Guo]]"
  - "[[Yunsong Meng]]"
  - "[[Zhao Tang Luo]]"
  - "[[Zhi Ouyang]]"
  - "[[Alp Aygar]]"
  - "[[Alvin Wan]]"
  - "[[Andrew Walkingshaw]]"
  - "[[Andy Narayanan]]"
  - "[[Antonie Lin]]"
  - "[[Arsalan Farooq]]"
  - "[[Brent Ramerth]]"
  - "[[Colorado Reed]]"
  - "[[Chris Bartels]]"
  - "[[Chris Chaney]]"
  - "[[David Riazati]]"
  - "[[Eric Liang Yang]]"
  - "[[Erin Feldman]]"
  - "[[Gabriel Hochstrasser]]"
  - "[[Guillaume Seguin]]"
  - "[[Irina Belousova]]"
  - "[[Joris Pelemans]]"
  - "[[Karen Yang]]"
  - "[[Keivan Alizadeh Vahid]]"
  - "[[Liangliang Cao]]"
  - "[[Mahyar Najibi]]"
  - "[[Marco Zuliani]]"
  - "[[Max Horton]]"
  - "[[Minsik Cho]]"
  - "[[Nikhil Bhendawade]]"
  - "[[Patrick Dong]]"
  - "[[Piotr Maj]]"
  - "[[Pulkit Agrawal]]"
  - "[[Qi Shan]]"
  - "[[Qichen Fu]]"
  - "[[Regan Poston]]"
  - "[[Sam Xu]]"
  - "[[Shuangning Liu]]"
  - "[[Sushma Rao]]"
  - "[[Tashweena Heeramun]]"
  - "[[Thomas Merth]]"
  - "[[Uday Rayala]]"
  - "[[Victor Cui]]"
  - "[[Vivek Rangarajan Sridhar]]"
  - "[[Wencong Zhang]]"
  - "[[Wenqi Zhang]]"
  - "[[Wentao Wu]]"
  - "[[Xingyu Zhou]]"
  - "[[Xinwen Liu]]"
  - "[[Yang Zhao]]"
  - "[[Yin Xia]]"
  - "[[Zhile Ren]]"
  - "[[Zhongzheng Ren]]"
year: 2024
tags:
  - paper
  - efficient_dl
url: https://arxiv.org/abs/2407.21075
share: true
---
> [!tldr] Abstract
> We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.

## Notes

> “A shared input/output embedding matrix [Press and Wolf, 2016] to reduce memory usage for parameters.” (Gunter et al., 2024, p. 2)

This reminds me of the [[Residual stream]] interpretation of transformers.

> “The model is compressed and quantized, on average under 4-bit-perweight, after the post-training stages (details of the quantization scheme will be discussed later). The quantized model often shows a moderate level of quality loss. Therefore, instead of directly passing the quantized model to application teams for feature development, we attach a set of parameter-efficient [[LoRa Adapter]]s for quality recovery. We make sure that these LoRA adapters training recipes are consistent with pre-training and post-training processes. Then, products will fine-tune their own feature-specific LoRA adapters by initializing the adapter weights from the accuracy-recovery adapters, while keeping the quantized base model frozen.” (Gunter et al., 2024, p. 16)

So the recipe is:
- Pre-training/Post-training
- Compression? and Quantization (leads to accuracy loss)
- LoRa fine-tuning to recover accuracy, call it LoRa Recovery, I'll assume this 
- For a specific task, initialize LoRa adapter to the LoRa Recovery
Some details:
- Rank 16 LoRa
- Does each LoRa adapter also share the same precision as the underlying weight block/matrix? I suppose so

“Specifically, our AFM-on-device model running on Apple Neural Engine (ANE) uses [[Bit Palettization]]: for projection weights, every 16 columns/rows share the same quantization constants (i.e., lookup tables) and are quantized using K-means with 16 unique values (4-bit).” (Gunter et al., 2024, p. 17)
