---
authors:
- '[[Jifeng Dai]]'
- '[[Haozhi Qi]]'
- '[[Yuwen Xiong]]'
- '[[Yi Li]]'
- '[[Guodong Zhang]]'
- '[[Han Hu]]'
- '[[Yichen Wei]]'
year: 2017
tags:
- paper
- cnn
- computer_vision
url: http://arxiv.org/abs/1703.06211
date: '2017-03-17'
---

> [!tldr] Abstract
> Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the ﬁxed geometric structures in their building modules. In this work, we introduce two new modules to enhance the transformation modeling capability of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from the target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the performance of our approach. For the ﬁrst time, we show that learning dense spatial transformation in deep CNNs is effective for sophisticated vision tasks such as object detection and semantic segmentation. The code is released at https://github.com/ msracver/Deformable-ConvNets.



## Notes

[Zotero Link](zotero://select/library/items/RNBLERSA)


