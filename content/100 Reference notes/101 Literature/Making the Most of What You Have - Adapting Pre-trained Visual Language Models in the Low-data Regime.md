---
authors: 
- "[[Chuhan Zhang]]"
- "[[Antoine Miech]]"
- "[[Jiajun Shen]]"
- "[[Jean-Baptiste Alayrac]]"
- "[[Pauline Luc]]"

year:  2023
tags:
  - paper
url: http://arxiv.org/abs/2305.02297
share: false
---


> [!tldr] Abstract
> Large-scale visual language models are widely used as pre-trained models and then adapted for various downstream tasks. While humans are known to efﬁciently learn new tasks from a few examples, deep learning models struggle with adaptation from few examples. In this work, we look into task adaptation in the low-data regime, and provide a thorough study of the existing adaptation methods for generative Visual Language Models. And we show important beneﬁts of self-labelling, i.e. using the model’s own predictions to self-improve when having access to a larger number of unlabelled images of the same distribution. Our study demonstrates signiﬁcant gains using our proposed task adaptation pipeline across a wide range of visual language tasks such as visual classiﬁcation (ImageNet), visual captioning (COCO), detailed visual captioning (Localised Narratives) and visual question answering (VQAv2).



## Notes

[Zotero Link](zotero://select/library/items/326TW5UW)


