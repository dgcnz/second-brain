---
authors: 
- "[[Lei Wang]]"
- "[[Juergen Gall]]"
- "[[Tat-Jun Chin]]"
- "[[Imari Sato]]"
- "[[Rama Chellappa]]"
- "[[Chuhan Zhang]]"
- "[[Ankush Gupta]]"
- "[[Andrew Zisserman]]"

year:  2023
tags:
  - paper
url: https://link.springer.com/10.1007/978-3-031-26316-3_23
share: false
---


> [!tldr] Abstract
> The objective of this work is to learn an object-centric video representation, with the aim of improving transferability to novel tasks, i.e., tasks diﬀerent from the pre-training task of action classiﬁcation. To this end, we introduce a new object-centric video recognition model based on a transformer architecture. The model learns a set of object-centric summary vectors for the video, and uses these vectors to fuse the visual and spatio-temporal trajectory ‘modalities’ of the video clip. We also introduce a novel trajectory contrast loss to further enhance objectness in these summary vectors.



## Notes

[Zotero Link](zotero://select/library/items/C5QHKYW3)


