---
tags:
  - efficient_dl
  - transformers
date: 2024-01-01
---

Key idea:
- Introduce extra transformer block that is initialized to be the identity function and train that. 

From [[Parameter Efficient Fine-tuning of Self-supervised ViTs without Catastrophic Forgetting]]

> We introduce the concept of Block Expansion for fine-tuning pre-trained ViTs, building upon an idea that was recently proposed for language modelsÂ [[27](https://arxiv.org/html/2404.17245v1#bib.bib27)]Â but has yet to be explored in vision. This technique is used to augment the capacity of a model without altering its initial output. In a ViT model comprised of sequential transformer blocksÂ ($\phi_0,\phi_1,â€¦,\phi_N$), Block Expansion adds an identity blockÂ ($\phi_{id}$)Â after a set of transformer blocks such thatÂ $\phi_{id}(x)=x$, meaning it returns the input as its output, ensuring the modelâ€™s output remains unchanged immediately after expansion. To expand a model fromÂ ğ‘Â toÂ ğ‘â€²Â blocks, the original blocks are first grouped into sets containingÂ ğ‘€Â blocks each. Within each set, an identity copy of the topmost block is created and placed on top, effectively increasing the modelâ€™s depth without initially changing its behavior. In each newly expanded block, two linear layers are zero-initialized to enable identity mapping, as shown in FigureÂ [1](https://arxiv.org/html/2404.17245v1#S1.F1 "Figure 1 â€£ 1 Introduction â€£ Parameter Efficient Fine-tuning of Self-supervised ViTs without Catastrophic Forgetting")Â (c). These newly added blocks are only fine-tuned with the new data while the remaining blocks are frozen.