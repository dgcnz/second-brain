{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>{{ blog_content }}</p>"},{"location":"000%20Zettelkasten/2D%20Convolutions/","title":"2D Convolutions","text":"<p>Fully comprehensive resource with animations: Conv2d</p>","tags":["cnn"]},{"location":"000%20Zettelkasten/Are%20less%20inductive%20biases%20better%20or%20worse%3F/","title":"Are less inductive biases better or worse?","text":"<p>There's a general consensus that less inductive biases are better, intuitively because it helps optimization by allowing for more hardware-friendly architectures, etc.</p> <p>First, An image is worth 16x16 words - Transformers for image recognition at scale shows that ViTs, with minimal inductive biases, outperform ConvNets. ViTs have: - No translational equivariance baked in - No locality inductive bias enforced     - Although positional encodings exist and fixed sinusoidal encodings can be used, they are mostly learned and randomly/zero initialized. They show that Vision Transformers scale better than ConvNets and Mixed Architectures (Convolutional stems + Transformer).</p> <p>A ConvNet for the 2020s proves that ResNets are outdated and improves the network with recent advances to match ViTs performance. </p> <p>The Lie derivative for measuring learned equivariance shows surprising result: ViTs exhibit more translational equivariance after training than ConvNets, as measured per their Lie Derivative.</p> <p>An Image is Worth More Than 16x16 Patches - Exploring Transformers on Individual Pixels tackles the toy question of dropping the convolutional stem that does the patchification in ViTs, with the intention of further reducing inductive biases. They prove that the resulting model (although too computationally intensive to be used in practice), competes with ViTs.</p> <p>How do vision transformers work? argues that the benefit of Vision Transformers is not that they have less inductive biases, but that the their operations are input dependent (see Input-dependent convolutions) and that Self Attention acts as a smoothing mechanism (that helps with better training dynamics on the large data regimes). They ablate this decision by constraining ViTs attention to be local, outperforming ViTs with global attention both in small and large data regimes. This is a strong indication that locality constraints are useful.  </p> <p>Learning with Unmasked Tokens Drives Stronger Vision Learners implicitly counter-argues How do vision transformers work? by noticing that MIM-trained ViTs exhibit localized attention maps and \"fixing\" it. Their approach outperforms other MIM-trained ViTs, so locality as good inductive bias is not definitely answered.</p>","tags":["dl_theory","question"]},{"location":"000%20Zettelkasten/Are%20less%20inductive%20biases%20better%20or%20worse%3F/#vits-vs-dense-prediction-tasks","title":"ViTs vs Dense prediction tasks","text":"<p>A ConvNet for the 2020s mentions that ViTs struggle on dense prediction tasks and they require hierarchical architectural choices (Swin Transformer) to do well. These choices re-introduce inductive biases.</p> <p>However, there's recent promising work that is (I think) successfully dropping these constraints: - Exploring Plain Vision Transformer Backbones for Object Detection - SimPLR - A Simple and Plain Transformer for Scaling-Efficient Object Detection and Segmentation</p>","tags":["dl_theory","question"]},{"location":"000%20Zettelkasten/Bit%20Palettization/","title":"Bit Palettization","text":"<p>Seems to be similar to K-Means-based Quantization.</p> <p>[...] we use 6-bit palettization, a type of quantization that compresses model weights from a 16-bit floating-point representation to just 6 bits per parameter. The name \u201cpalettization\u201d refers to a technique similar to the one used in computer graphics to work with a limited set of colors: the color table (or \u201cpalette\u201d) contains a fixed number of colors, and the colors in the image are replaced with the indexes of the closest colors available in the palette. This immediately provides the benefit of drastically reducing storage size, and thus reducing download time and on-device disk use.</p> <p> References: - https://huggingface.co/blog/stable-diffusion-xl-coreml#what-is-mixed-bit-palettization - https://huggingface.co/blog/fast-diffusers-coreml</p> <p>Notes: - Multiplying by this weight matrix intuitively should be slower, it would be interesting to see what is the tradeoff speed vs memory. This tweet Tweet - Stable Diffusion XL on iPhone with Core ML! suggests that it runs faster than the non-quantized alternative.</p>","tags":["efficient_dl","transformers"]},{"location":"000%20Zettelkasten/Block%20Expansion/","title":"Block Expansion","text":"<p>Key idea: - Introduce extra transformer block that is initialized to be the identity function and train that. </p> <p>From Parameter Efficient Fine-tuning of Self-supervised ViTs without Catastrophic Forgetting</p> <p>We introduce the concept of Block Expansion for fine-tuning pre-trained ViTs, building upon an idea that was recently proposed for language models\u00a0[27]\u00a0but has yet to be explored in vision. This technique is used to augment the capacity of a model without altering its initial output. In a ViT model comprised of sequential transformer blocks\u00a0(\\(\\phi_0,\\phi_1,\u2026,\\phi_N\\)), Block Expansion adds an identity block\u00a0(\\(\\phi_{id}\\))\u00a0after a set of transformer blocks such that\u00a0\\(\\phi_{id}(x)=x\\), meaning it returns the input as its output, ensuring the model\u2019s output remains unchanged immediately after expansion. To expand a model from\u00a0\ud835\udc41\u00a0to\u00a0\ud835\udc41\u2032\u00a0blocks, the original blocks are first grouped into sets containing\u00a0\ud835\udc40\u00a0blocks each. Within each set, an identity copy of the topmost block is created and placed on top, effectively increasing the model\u2019s depth without initially changing its behavior. In each newly expanded block, two linear layers are zero-initialized to enable identity mapping, as shown in Figure\u00a01\u00a0(c). These newly added blocks are only fine-tuned with the new data while the remaining blocks are frozen.</p>","tags":["efficient_dl","transformers"]},{"location":"000%20Zettelkasten/Convergence%20rate%20and%20Hessian%20spectra/","title":"Convergence rate and Hessian spectra","text":"<ul> <li>Remember: If a Hessian matrix is positive definite everywhere, then the function is convex =&gt; bad neg eigenvalues</li> <li>Large eigenvalues of the  Metrics for flatness Some metrics, such as the maximum Hessian eigenvalue, measure the worstcase loss increase under an adversarial perturbation to the weights [10, 16], while other proposed metrics, such as the Hessian trace, measure the expected loss increase under random perturbations to the weights.</li> </ul>","tags":["optimizability"]},{"location":"000%20Zettelkasten/Depthwise%20separable%20convolutions/","title":"Depthwise separable convolutions","text":"<p>Splits the computation into two steps:\u00a0depthwise convolution\u00a0applies a single convolutional filter per each input channel and\u00a0pointwise convolution\u00a0is used to create a linear combination of the output of the depthwise convolution.</p> <p>Related ideas are often used to reduce the size/complexity of convolutional layers. It reduces expressivity of convolutions but its less parameters. For example Exploiting Redundancy - Separable Group Convolutional Networks on Lie Groups</p> <p>Also used in (ConvNext) A ConvNet for the 2020s</p> <p></p>","tags":["cnn"]},{"location":"000%20Zettelkasten/Do%20Vision%20Foundation%20models%20exist%3F/","title":"Do Vision Foundation models exist?","text":"","tags":["question","foundation_models","computer_vision"]},{"location":"000%20Zettelkasten/Do%20Vision%20Foundation%20models%20exist%3F/#object-detection","title":"Object detection","text":"<p>Research using DINOv2 as a backbone for object detection:</p> <p>DINOv2 \u274c - Poor Object Detection Performance with DINOv2 Backbone and Faster R-CNN Head on Cityscapes Dataset     - Using mask rcnn head but still relevant, maybe dinov2 is not a good object detection backbone?</p> <p>DINOv2 \u2705</p> <p>\"NVIDIA has also released a foundational model called NV-Dinov2, which is available through the NVIDIA AI Enterprise program. NV-Dinov2 is a visual foundational model trained on an NVIDIA proprietary large scale dataset.\" NV-DINOv2 - NVIDIA provides CLIP VIT and DINO VIT backbones for object detection and segmentation (closed source)     - This signals that it is not only possible but actually useful in production (the tao toolkit specifically markets to providing enterprise-ready vision transformers)     - However it also very specifically states the inferior performance of vits compared with specifically trained dense-prediction networks:         &gt; \"To mitigate the inferior performance of a standard vision transformer (ViT) on dense prediction tasks, TAO supports the\u00a0ViT-Adapter_\u00a0architecture. This allows a powerful ViT that has learned rich semantic representations from a large corpus of data to achieve comparable performance to vision-specific transformers on dense prediction tasks.\"</p> <ul> <li> <p>Exploring Plain Vision Transformer Backbones for Object Detection</p> <ul> <li>VitDET with DINO backbone gh issue<ul> <li>There's some caveats but they are fixable</li> </ul> </li> </ul> </li> <li> <p>SimPLR - A Simple and Plain Transformer for Scaling-Efficient Object Detection and Segmentation</p> <ul> <li>Improves over ViTDet</li> </ul> </li> </ul>","tags":["question","foundation_models","computer_vision"]},{"location":"000%20Zettelkasten/Equivariance%20Initialization/","title":"Equivariance Initialization","text":"<p>Related: - Priors over Neural Network weights</p>","tags":["dl_theory"]},{"location":"000%20Zettelkasten/Group%20Axioms/","title":"Group Axioms","text":"<p>A group is a non-empty set \\(G\\) together with a binary operation on \\(G\\) (\\(\\cdot\\)), that fulfills the following axioms: 1. Associativity: For all \\(a, b, c \\in G\\), one has \\((a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)\\) 2. Identity element: There exists an element \\(e\\in G\\) such that, for every \\(a \\in G\\), \\(e \\cdot a = a\\) and \\(a \\cdot e = a\\) 3. Inverse element: For each \\(a\\in G\\), there exists a unique element \\(b\\in G\\) such that \\(a \\cdot b = e\\) and \\(b \\cdot a = e\\), where \\(e\\) is the identity element. The inverse of \\(a\\) is denoted as \\(a^{-1}\\)</p>","tags":["math"]},{"location":"000%20Zettelkasten/Group%20direct%20product/","title":"Group direct product","text":"<p>Given groups \\(G\\) (with operation *) and \\(H\\) (with operation \\(\\Delta\\)), the direct product \\(G \\times H\\) is defined as follows: 1. The underlying set is the Cartesian product, \\(G \\times H\\). That is, the ordered pairs \\((g, h)\\), where \\(g \\in G\\) and \\(h \\in H\\).  2. The binary operation on \\(G \\times H\\) is defined component-wise.</p> \\[  (g_1, h_1) \\cdot (g_2, h_2) = (g_1 * g_2, h_1 \\Delta h_2) \\] <p>The resulting algebraic object satisfies the Group Axioms.</p>","tags":["math"]},{"location":"000%20Zettelkasten/Hardware-specific%20structured%20pruning/","title":"Hardware specific structured pruning","text":"<p>Key Idea</p> <p>Some GPU architectures can take advantage of specific sparsity patterns.</p> <p>According to this the training procedure would look as follows:</p> <p>NVIDIA has developed a simple and universal recipe for sparsifying deep neural networks for inference\u00a0using this 2:4 structured sparsity pattern. The network is first trained using dense weights, then fine-grained structured pruning is applied, and finally the remaining non-zero weights are fine-tuned with additional training steps. This method results in virtually no loss in inferencing accuracy based on evaluation across dozens of networks spanning vision, object detection, segmentation, natural language modeling, and translation.</p> <p> References: - TinyML and Efficient Deep Learning Computing - Lecture 3 - https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/ - https://developer.nvidia.com/blog/structured-sparsity-in-the-nvidia-ampere-architecture-and-applications-in-search-engines/</p>","tags":["efficient_dl","hardware_aware_dl"]},{"location":"000%20Zettelkasten/Input-dependent%20convolutions/","title":"Input dependent convolutions","text":"<ul> <li>How do vision transformers work? states that the key advantage of Self Attention over Convolutions is not the long range dependencies (global attention) but rather its data specificity (aka input dependency)</li> <li>This is related to Mamba - Linear-Time Sequence Modeling with Selective State Spaces's insight :<ul> <li>\"We identify a key limitation of prior models: the ability to efficiently select data in an input-dependent manner (i.e. focus on or ignore particular inputs).\"</li> </ul> </li> </ul> <p>There most likely is work on input-dependent convolutions: - [ ] CKConv - Continuous Kernel Convolution For Sequential Data is probably related, but haven't read it in full.  Check this. - [ ] Review literature on input-dependent convolutions</p>","tags":["cnn","theory"]},{"location":"000%20Zettelkasten/K-Means-based%20Quantization/","title":"K Means based Quantization","text":"<p>Perform clustering on weights, and replace weights with cluster <code>int</code> index matrix (to which cluster each weight entry belongs to) and a list of <code>float</code> centroids.</p> <p>Storing integers consumes less memory while you can keep fully precision on the float centroids (although you lose precision because it does not necessarily correspond to an actual value in the previous weight matrix).</p> <p>Resources: - https://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html - TinyML and Efficient Deep Learning Computing - Lecture 5 </p>","tags":["efficient_dl"]},{"location":"000%20Zettelkasten/KV%20Cache/","title":"KV Cache","text":"<p> From: TinyML and Efficient Deep Learning Computing - Lecture 12</p>","tags":["efficient_dl","transformers"]},{"location":"000%20Zettelkasten/Linear%20Quantization/","title":"Linear Quantization","text":"<p>Visualization </p> <p>Then, for each layer in your network (linear, conv, etc), you represent the matrices involved like the previous formulation, do some arithmetic to see what you can precompute and zero-out and voil\u00e1</p>","tags":["efficient_dl"]},{"location":"000%20Zettelkasten/Masked%20Image%20Modelling/","title":"Masked Image Modelling","text":"<p>It seems like MIM objectives are becoming a strong learning objective for vision foundation models. Right now it seems to be the closest answer to: Do Vision Foundation models exist?</p> <p>However, intuitively it seems a bit like a weak signal, as it focuses on individual patches/pixels, without much consideration to semantic information. This is echoed on Learning with Unmasked Tokens Drives Stronger Vision Learners:</p> <p>However, MIM strategies often encounter challenges, such as local dependency on attention to understand entire context of an image. For example, liu\u00a0et al.\u00a0[36]\u00a0revealed that MAE\u00a0[22], a state-of-the-art MIM method, exhibits shorter average attention distances. Furthermore, we observe that attention map patterns by MAE substantiate extremely local behavior (See Fig.\u00a01) indeed. In other words, the MAE-trained attention mechanism less integrates information across the entire image pixels and tends to focus on specific input regions. This is presumably attributed to MIM-pretraining, primarily dedicated to predicting low-level pixel details (e.g., color or texture) without a comprehensive understanding of less-regional information (e.g., the input structure or shape).</p> <p>Related papers: - Learning with Unmasked Tokens Drives Stronger Vision Learners - DINOv2 - Learning Robust Visual Features without Supervision - Learning with Unmasked Tokens Drives Stronger Vision Learners - What Do Self-Supervised Vision Transformers Learn? \ud83d\udea8</p>","tags":["foundation_models","computer_vision"]},{"location":"000%20Zettelkasten/Maximal%20pruning%20and%20functional%20recovery/","title":"Maximal pruning and functional recovery","text":"<p>Key Idea</p> <p>You can iteratively prune and finetune the network weights and still maintain performance up to some pruning ratio.</p> <p> Reference: - TinyML and Efficient Deep Learning Computing - Lecture 3 - Learning both Weights and Connections for Efficient Neural Networks</p>","tags":["dl_theory","efficient_dl"]},{"location":"000%20Zettelkasten/Mean%20Attention%20Distance/","title":"Mean Attention Distance","text":"<p>Introduced in An image is worth 16x16 words - Transformers for image recognition at scale.</p> <p>From What Do Self-Supervised Vision Transformers Learn?</p> <p>\u201cAttention distance is defined as the average distance between the query tokens and key tokens considering their self-attention weights. Therefore, it conceptually corresponds to the size of the receptive fields in CNNs.\u201d (Park et al., 2023, p. 3)</p> <p>Key Observation</p> <p>Can be used to measure what is the how much local or global information is a transformer using. See What Do Self-Supervised Vision Transformers Learn?.</p>","tags":["dl_theory","transformers"]},{"location":"000%20Zettelkasten/Multiple%20global%20minima/","title":"Multiple global minima","text":"<p>We expect loss functions for deep networks to have a large family of equivalent global minima.</p> <ul> <li>Fully connected networks: permutation of the hidden units</li> <li>Convolutional networks: permuting the channels and convolution kernels appropriately.</li> <li>...</li> </ul> <p>The above modifications all produce the same output for every input. However, the global minimum only depends on the output at the training data points. </p> <p>In overparameterized networks, there will also be families of solutions that behave identically at the data points but differently between them. All of these are also global minima.</p> <p>References: - Understanding Deep Learning - Chapter 20 (20.3.1)</p>","tags":["optimizability","dl_theory"]},{"location":"000%20Zettelkasten/Neural%20Network%20Quantization/","title":"Neural Network Quantization","text":"<p>Related: - HuggingFace Docs - A survey of quantization methods for efficient neural network inference - A recent (2024) work by Han et al: AWQ - Activation-aware Weight Quantization for LLM Compression and Acceleration</p>","tags":["quantization","efficient_dl"]},{"location":"000%20Zettelkasten/Non-translationally%20equivariant%20convolutions/","title":"Non translationally equivariant convolutions","text":"<p>I'm not sure if this makes sense at all, just tracking paper ideas lmao</p> <p>See: - Input-dependent convolutions - How do vision transformers work?</p>","tags":["cnn","convolutions","equivariance","partial_equivariance"]},{"location":"000%20Zettelkasten/Positive%20Logic%20Programs/","title":"Positive Logic Programs","text":"","tags":["knowledge_representation"]},{"location":"000%20Zettelkasten/Positive%20Logic%20Programs/#positive-logic-programs","title":"Positive logic programs","text":"<p>Two components:  1. Facts:  <code>a.</code> 2. Rules:  <code>a :- b, c, d</code> , which is the same as <code>b \u2227 c \u2227 d \u2192 a</code></p> <p>This is a positive logic program: <pre><code>rainy(amsterdam).\nrainy(vienna).\nwet(X) :- rainy(X). # eq: \u2200x. (Rainy(x) \u2192 Wet(x))\n</code></pre></p>","tags":["knowledge_representation"]},{"location":"000%20Zettelkasten/Positive%20Logic%20Programs/#database-semantics","title":"Database semantics","text":"<p>Assumptions 1. Domain closure: The objects mentioned are the only objects. 2. Unique-names assumption: Two variables can't refer to the same object 3. Closed-world assumption: Whatever we don't know is false</p> What does the database semantics allow us to do? <ol> <li>We can specify a relation by the set of inputs that are true</li> <li>We can specify objects simply by the terms that point to them</li> <li>We don't have to explicitly define what function symbols mean</li> </ol> <p>Thus, an interpretation is a set that defines which atoms are true. The remainder are false.</p>","tags":["knowledge_representation"]},{"location":"000%20Zettelkasten/Positive%20Logic%20Programs/#models","title":"Models","text":"What is a model? <p>A model is an interpretation which makes all rules of a program true.</p> <p>However, we're not interested in all models, we want the highest expressivity at the lowest information.</p> What is the definition of a minimal model? <p>A model is minimal if no strict subset exist that is also a model.</p> How do you construct a minimal model? <p>Start with facts and add new literals that are on the lhs of a rule where all body is in M. <pre><code>M = {f for f in facts}\nwhile True:\n    for head, body in rules:\n        if all(l in M for l in body):\n            M.add(l)\n</code></pre></p> What is the definition of a supported model? <p>A model is supported if all its atoms are supported. An atom of a model is supported if it appears as a head where the body is true.</p> What properties does minimal models and supported models have for positive logic programs? <p>For positive logic programs:</p> <ul> <li>Minimal models are unique</li> <li>A minimal model is also a supported model (but not necessarily viceversa)</li> </ul>","tags":["knowledge_representation"]},{"location":"000%20Zettelkasten/Positive%20Logic%20Programs/#normal-logic-programs","title":"Normal logic programs","text":"<p>Now we allow negation.</p> <pre><code>a :- b_1, ..., b_n, not c_1, ..., not c_m.\n</code></pre> Do properties of minimal models for PL still hold for NL? Why? <p>No, negation removes allows for non-uniqueness of minimal models.</p>","tags":["knowledge_representation"]},{"location":"000%20Zettelkasten/Priors%20over%20Neural%20Network%20weights/","title":"Priors over Neural Network weights","text":"<p>From Understanding Deep Learning - Chapter 10, 1d convolutions can be represented as weight matrices from a MLP with a specific prior where the diagonals are the same (d). </p> <p>Rotationally equivariant convolutions can be implemented by isotropic filters (a prior on the conv2d weight): </p>","tags":["dl_theory","equivariance"]},{"location":"000%20Zettelkasten/Representation%20%28Group%20Theory%29/","title":"Representation (Group Theory)","text":"<p>Property required: $$ p(g)p(h) = p(g \\cdot h) $$</p> <p>A representation of a group action can be a linear operator like: $$ p(\\theta) = [sin(\\theta) ...] $$</p>","tags":["math","group_theory"]},{"location":"000%20Zettelkasten/Residual%20stream/","title":"Residual stream","text":"<p>\"A transformer\u00a0starts with a token embedding, followed by a series of \u201cresidual blocks\u201d, and finally a token unembedding. Each residual block consists of an attention layer, followed by an MLP layer. Both the attention and MLP layers each \u201cread\u201d their input from the residual stream (by performing a linear projection), and then \u201cwrite\u201d their result to the residual stream by adding a linear projection back in.\u00a0Each attention layer consists of multiple heads, which operate in parallel.\" A Mathematical Framework for Transformer Circuits</p> <p></p>","tags":["mechinterp","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/A%20Brief%20Review%20of%20Hypernetworks%20in%20Deep%20Learning/","title":"A Brief Review of Hypernetworks in Deep Learning","text":"Properties authors Vinod Kumar Chahuan, Jiandong Zhou, Ping Lu, Soheila Molaei, David A. Clifton year 2023 url https://arxiv.org/abs/2306.06955 <p>Abstract</p> <p>Hypernetworks, or hypernets in short, are neural networks that generate weights for another neural network, known as the target network. They have emerged as a powerful deep learning technique that allows for greater flexibility, adaptability, dynamism, faster training, information sharing, and model compression etc. Hypernets have shown promising results in a variety of deep learning problems, including continual learning, causal inference, transfer learning, weight pruning, uncertainty quantification, zero-shot learning, natural language processing, and reinforcement learning etc. Despite their success across different problem settings, currently, there is no review available to inform the researchers about the developments and to help in utilizing hypernets. To fill this gap, we review the progress in hypernets. We present an illustrative example to train deep neural networks using hypernets and propose categorizing hypernets based on five design criteria as inputs, outputs, variability of inputs and outputs, and architecture of hypernets. We also review applications of hypernets across different deep learning problem settings, followed by a discussion of general scenarios where hypernets can be effectively employed. Finally, we discuss the challenges and future directions that remain under-explored in the field of hypernets. We believe that hypernetworks have the potential to revolutionize the field of deep learning. They offer a new way to design and train neural networks, and they have the potential to improve the performance of deep learning models on a variety of tasks. Through this review, we aim to inspire further advancements in deep learning through hypernetworks.</p>","tags":["paper","hypernetworks"]},{"location":"100%20Reference%20notes/101%20Literature/A%20ConvNet%20for%20the%202020s/","title":"A ConvNet for the 2020s","text":"Properties authors Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie year 2022 url https://arxiv.org/abs/2201.03545 <p>Abstract</p> <p>The \"Roaring 20s\" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually \"modernize\" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.</p>","tags":["cnn","foundation_models","computer_vision","dl_theory","paper"]},{"location":"100%20Reference%20notes/101%20Literature/A%20ConvNet%20for%20the%202020s/#notes","title":"Notes","text":"<p>Authors modernize ConvNets with SOTA architectural choices and training recipes to achieve SOTA ViT performance on dense prediction tasks (Object Detection, etc). { width=\"500\" }</p> <p>Important limitation, scaling laws for ConvNext are not proved to be as good as ViTs, although they also mention that they are promising:</p> <p>These findings are encouraging but not yet completely convincing \u2014 our exploration thus far has been limited to a small scale, but vision Transformers\u2019 scaling behavior is what truly distinguishes them.</p> <p>Table 1. Classification accuracy on ImageNet-1K. Similar to Transformers, ConvNeXt also shows promising scaling behavior with higher-capacity models and a larger (pre-training) dataset.</p> <ul> <li> What are the follow ups for this paper regarding scaling laws of modern convnets when compared to vits?</li> </ul> <p>One of the main motivations of this paper is that ViTs were not very good at dense prediction tasks such as object detection:</p> <p>A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks.</p>","tags":["cnn","foundation_models","computer_vision","dl_theory","paper"]},{"location":"100%20Reference%20notes/101%20Literature/A%20Hierarchy%20of%20Graph%20Neural%20Networks%20Based%20on%20Learnable%20Local%20Features/","title":"A Hierarchy of Graph Neural Networks Based on Learnable Local Features","text":"Properties authors Michael Linghzhi Li, Meng Dong, Jiawei Zhou, Alexander M. Rush year 2019 url https://arxiv.org/abs/1911.05256 <p>Abstract</p> <p>Graph neural networks (GNNs) are a powerful tool to learn representations on graphs by iteratively aggregating features from node neighbourhoods. Many variant models have been proposed, but there is limited understanding on both how to compare different architectures and how to construct GNNs systematically. Here, we propose a hierarchy of GNNs based on their aggregation regions. We derive theoretical results about the discriminative power and feature representation capabilities of each class. Then, we show how this framework can be utilized to systematically construct arbitrarily powerful GNNs. As an example, we construct a simple architecture that exceeds the expressiveness of the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theory on both synthetic and real-world benchmarks, and demonstrate our example's theoretical power translates to strong results on node classification, graph classification, and graph regression tasks.</p> <p>Interesting insight: - \u201cUsing this hierarchy, we can derive theoretical results which provide insight into GNNs. For example, we show that no matter how many layers are added, networks which only aggregate over immediate neighbors cannot learn the number of triangles in a node\u2019s neighbourhood\u201d (Li et al., 2019, p. 1) </p> <p>HOWEVER: - you can bypass this by encoding geometric information like position and orientation, see Fast, Expressive SE(n) Equivariant Networks through Weight-Sharing in Position-Orientation Space slides</p> <p>Michael Lingzhi Li,\u00a0Meng Dong,\u00a0Jiawei Zhou,\u00a0Alexander M. Rush</p>","tags":["gcn","graphs","gnn","paper"]},{"location":"100%20Reference%20notes/101%20Literature/A%20Mathematical%20Framework%20for%20Transformer%20Circuits/","title":"A Mathematical Framework for Transformer Circuits","text":"Properties authors Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, Christopher Olah year 2021 url https://transformer-circuits.pub/2021/framework/index.html <p>Abstract</p> <p>Transformer [1] language models are an emerging technology that is gaining increasingly broad real-world use, for example in systems like GPT-3 [2], LaMDA\u00a0[3], Codex\u00a0[4], Meena\u00a0[5], Gopher\u00a0[6], and similar models. \u00a0However, as these models scale, their open-endedness and high capacity creates an increasing scope for unexpected and sometimes harmful behaviors. \u00a0Even years after a large model is trained, both creators and users routinely discover model capabilities \u2013 including problematic behaviors \u2013 they were previously unaware of.</p> <p>One avenue for addressing these issues is\u00a0mechanistic interpretability, attempting to reverse engineer the detailed computations performed by transformers, similar to how a programmer might try to reverse engineer complicated binaries into human-readable source code. \u00a0If this were possible, it could potentially provide a more systematic approach to explaining current safety problems, identifying new ones, and perhaps even anticipating the safety problems of powerful future models that have not yet been built. \u00a0A previous project, the\u00a0Distill\u00a0Circuits\u00a0thread\u00a0[7], has attempted to reverse engineer vision models, but so far there hasn\u2019t been a comparable project for transformers or language models. </p> <p>In this paper, we attempt to take initial, very preliminary steps towards reverse-engineering transformers. \u00a0Given the incredible complexity and size of modern language models, we have found it most fruitful to start with the simplest possible models and work our way up from there. \u00a0Our aim is to discover simple algorithmic patterns, motifs, or frameworks that can subsequently be applied to larger and more complex models. \u00a0Specifically, in this paper we will study\u00a0transformers with two layers or less which have only attention blocks\u00a0\u2013 this is in contrast to a large, modern transformer like GPT-3, which has 96 layers and alternates attention blocks with MLP blocks.</p> <p>We find that by conceptualizing the operation of transformers in a new but mathematically equivalent way, we are able to make sense of these small models and gain significant understanding of how they operate internally. \u00a0Of particular note, we find that specific attention heads that we term \u201cinduction heads\u201d can explain in-context learning in these small models, and that these heads only develop in models with at least two attention layers. \u00a0We also go through some examples of these heads operating in action on specific data.</p> <p>We don\u2019t attempt to apply to our insights to larger models in this first paper, but in a\u00a0forthcoming paper, we will show that both our mathematical framework for understanding transformers, and the concept of induction heads, continues to be at least partially relevant for much larger and more realistic models \u2013 though we remain a very long way from being able to fully reverse engineer such models.</p>","tags":["paper","mechinterp","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/A%20general%20theory%20of%20correct%2C%20incorrect%2C%20and%20extrinsic%20equivariance/","title":"A general theory of correct, incorrect, and extrinsic equivariance","text":"Properties authors Dian Wang, Xupeng Zhu, Jung Yeon Park, Mingxi Jia, Guanang Su, Robert Platt, Robin Walters year 2024 url https://proceedings.neurips.cc/paper_files/paper/2023/hash/7dc7793c89b93887e126a86f22ef63c6-Abstract-Conference.html <p>Abstract</p> <p>Although equivariant machine learning has proven effective at many tasks, success depends heavily on the assumption that the ground truth function is symmetric over the entire domain matching the symmetry in an equivariant neural network. A missing piece in the equivariant learning literature is the analysis of equivariant networks when symmetry exists only partially in the domain. In this work, we present a general theory for such a situation. We propose pointwise definitions of correct, incorrect, and extrinsic equivariance, which allow us to quantify continuously the degree of each type of equivariance a function displays. We then study the impact of various degrees of incorrect or extrinsic symmetry on model error. We prove error lower bounds for invariant or equivariant networks in classification or regression settings with partially incorrect symmetry. We also analyze the potentially harmful effects of extrinsic equivariance. Experiments validate these results in three different environments.</p>","tags":["equivariance","relaxed_equivariance","dl_theory","paper"]},{"location":"100%20Reference%20notes/101%20Literature/A%20survey%20of%20quantization%20methods%20for%20efficient%20neural%20network%20inference/","title":"A survey of quantization methods for efficient neural network inference","text":"Properties authors Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer year 2021 url https://arxiv.org/abs/2103.13630 <p>Abstract</p> <p>As soon as abstract mathematical computations were adapted to computation on digital computers, the problem of efficient representation, manipulation, and communication of the numerical values in those computations arose. Strongly related to the problem of numerical representation is the problem of quantization: in what manner should a set of continuous real-valued numbers be distributed over a fixed discrete set of numbers to minimize the number of bits required and also to maximize the accuracy of the attendant computations? This perennial problem of quantization is particularly relevant whenever memory and/or computational resources are severely restricted, and it has come to the forefront in recent years due to the remarkable performance of Neural Network models in computer vision, natural language processing, and related areas. Moving from floating-point representations to low-precision fixed integer values represented in four bits or less holds the potential to reduce the memory footprint and latency by a factor of 16x; and, in fact, reductions of 4x to 8x are often realized in practice in these applications. Thus, it is not surprising that quantization has emerged recently as an important and very active sub-area of research in the efficient implementation of computations associated with Neural Networks. In this article, we survey approaches to the problem of quantizing the numerical values in deep Neural Network computations, covering the advantages/disadvantages of current methods. With this survey and its organization, we hope to have presented a useful snapshot of the current research in quantization for Neural Networks and to have given an intelligent organization to ease the evaluation of future research in this area.</p>","tags":["paper","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/AWQ%20-%20Activation-aware%20Weight%20Quantization%20for%20LLM%20Compression%20and%20Acceleration/","title":"AWQ   Activation aware Weight Quantization for LLM Compression and Acceleration","text":"Properties authors Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, Song Han year 2023 url https://arxiv.org/abs/2306.00978 <p>Abstract</p> <p>Large language models (LLMs) have fundamentally transformed the capabilities of numerous applications, from natural language processing to more intricate domain-specific tasks in robotics and autonomous driving. Moreover, the importance of on-device LLMs has grown significantly in the recent years. Running LLMs on edge devices not only promises reduced latency and improved user experience but also aligns with the increasing need for user privacy, as data processing can occur locally. However, the astronomical model sizes of modern LLMs and constraints of the edge devices, primarily in terms of memory size and bandwidth, pose significant deployment challenges. In this paper, we propose Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only quantization. Our method is based on the observation that weights are not equally important: protecting only 1% of salient weights can greatly reduce quantization error. We then propose to search for the optimal per-channel scaling that protects the salient weights by observing the activation, not weights. AWQ does not rely on any backpropagation or reconstruction, so it can well preserve LLMs' generalization ability on different domains and modalities, without overfitting to the calibration set. AWQ outperforms existing work on various language modeling and domain-specific benchmarks (coding and math). Thanks to better generalization, it achieves excellent quantization performance for instruction-tuned LMs and, for the first time, multi-modal LMs. Alongside AWQ, we implement TinyChat, an efficient and flexible inference framework tailored for on-device LLM/VLMs, offering more than 3x speedup over the Huggingface FP16 implementation on both desktop and mobile GPUs. It also democratizes the deployment of the 70B Llama-2 model on mobile GPUs.</p>","tags":["paper","efficient_dl","quantization"]},{"location":"100%20Reference%20notes/101%20Literature/Adapting%20Vision%20Foundation%20Models%20for%20Plant%20Phenotyping/","title":"Adapting Vision Foundation Models for Plant Phenotyping","text":"Properties authors Feng Chen, Mario Valerio Giuffrida, Sotirios A. Tsaftaris year 2023 url https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Chen_Adapting_Vision_Foundation_Models_for_Plant_Phenotyping_ICCVW_2023_paper.html <p>Abstract</p> <p>Foundation models are large models pre-trained on tremendous amount of data. They can be typically adapted to diverse downstream tasks with minimal effort. However, as foundation models are usually pre-trained on images or texts sourced from the Internet, their performance in specialized domains, such as plant phenotyping, comes into question. In addition, fully fine-tuning foundation models is time-consuming and requires high computational power. This paper investigates the efficient adaptation of foundation models for plant phenotyping settings and tasks. We perform extensive experiments on fine-tuning three foundation models, MAE, DINO, and DINOv2 on three essential plant phenotyping tasks: leaf counting, instance segmentation, and disease classification. In particular, the pre-trained backbones are kept frozen, while two distinct fine-tuning methods are evaluated, namely adapter tuning (using LoRA) and decoder tuning. The experimental results show that a foundation model can be efficiently adapted to multiple plant phenotyping tasks, yielding similar performance as the state-of-the-art (SoTA) models specifically designed or trained for each task. Despite exhibiting great transferability over different tasks, the fine-tuned foundation models perform slightly worse than the SoTA task-specific models in some scenarios, which requires further investigation.</p>","tags":["paper","peft","foundation_models"]},{"location":"100%20Reference%20notes/101%20Literature/Adapting%20Vision%20Foundation%20Models%20for%20Plant%20Phenotyping/#notes","title":"Notes","text":"<p>Motivation / Problem</p> <p>Foundation models struggle with specialized data like (plant phenotyping, cancer predictions)</p> <p>Research question</p> <p>Which efficient fine-tuning technique is most promising for adapting foundation models (MAE, DINO, DINOv2) in specialized data? </p> <p>Methods</p> <p>Benchmarked fine-tuning methods include decoder fine-tuning (aka linear probing) and adapter tuning (linear probing + LoRa)</p> <p>Results</p> <ol> <li>LoRa consistently beats DT</li> <li>VFM w/ LoRa are often competitive fully-trained/finetuned SOTA</li> <li>It's not clear that one vfm beats another, each model (DINO, DINOv2, MAE) have metrics and tasks where they shine</li> <li>LoRa can help dampen issues of data scarcity, domain shifts and class imbalance</li> </ol>","tags":["paper","peft","foundation_models"]},{"location":"100%20Reference%20notes/101%20Literature/An%20Image%20is%20Worth%20More%20Than%2016x16%20Patches%20-%20Exploring%20Transformers%20on%20Individual%20Pixels/","title":"An Image is Worth More Than 16x16 Patches   Exploring Transformers on Individual Pixels","text":"Properties authors Duy-Kien Nguyen, Mahmoud Assran, Unnat Jain, Martin R. Oswald, Cees G. M. Snoek, Xinlei Chen year 2024 url https://arxiv.org/abs/2406.09415v1 <p>Abstract</p> <p>This work does not introduce a new method. Instead, we present an interesting finding that questions the necessity of the inductive bias -- locality in modern computer vision architectures. Concretely, we find that vanilla Transformers can operate by directly treating each individual pixel as a token and achieve highly performant results. This is substantially different from the popular design in Vision Transformer, which maintains the inductive bias from ConvNets towards local neighborhoods (e.g. by treating each 16x16 patch as a token). We mainly showcase the effectiveness of pixels-as-tokens across three well-studied tasks in computer vision: supervised learning for object classification, self-supervised learning via masked autoencoding, and image generation with diffusion models. Although directly operating on individual pixels is less computationally practical, we believe the community must be aware of this surprising piece of knowledge when devising the next generation of neural architectures for computer vision.</p> <p>Comments: - Seems to contradict How do vision transformers work? in their position that inductive biases do improve vits.      - [ ] Might be useful to check this.</p>","tags":["paper","dl_theory","vit"]},{"location":"100%20Reference%20notes/101%20Literature/An%20Investigation%20into%20Neural%20Net%20Optimization%20via%20Hessian%20Eigenvalue%20Density/","title":"An Investigation into Neural Net Optimization via Hessian Eigenvalue Density","text":"Properties authors Behrooz Ghorbani, Shankar Krishnan, Ying Xiao <p>Abstract</p> <p>To understand the dynamics of optimization in deep neural networks, we develop a tool to study the evolution of the entire Hessian spectrum throughout the optimization process. Using this, we study a number of hypotheses concerning smoothness, curvature, and sharpness in the deep learning literature. We then thoroughly analyze a crucial structural feature of the spectra: in nonbatch normalized networks, we observe the rapid appearance of large isolated eigenvalues in the spectrum, along with a surprising concentration of the gradient in the corresponding eigenspaces. In batch normalized networks, these two effects are almost absent. We characterize these effects, and explain how they affect optimization speed through both theory and experiments. As part of this work, we adapt advanced tools from numerical linear algebra that allow scalable and accurate estimation of the entire Hessian spectrum of ImageNet-scale neural networks; this technique may be of independent interest in other applications</p>","tags":["dl_theory","optimizability","optimization","paper"]},{"location":"100%20Reference%20notes/101%20Literature/An%20image%20is%20worth%2016x16%20words%20-%20Transformers%20for%20image%20recognition%20at%20scale/","title":"An image is worth 16x16 words   Transformers for image recognition at scale","text":"Properties authors Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby url https://arxiv.org/abs/2010.11929 year 2020 <p>Abstract</p> <p>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</p>","tags":["vit","transformers","paper"]},{"location":"100%20Reference%20notes/101%20Literature/An%20image%20is%20worth%2016x16%20words%20-%20Transformers%20for%20image%20recognition%20at%20scale/#notes","title":"Notes","text":"","tags":["vit","transformers","paper"]},{"location":"100%20Reference%20notes/101%20Literature/An%20image%20is%20worth%2016x16%20words%20-%20Transformers%20for%20image%20recognition%20at%20scale/#regarding-inductive-biases","title":"Regarding inductive biases","text":"<p>Inductive bias. We note that Vision Transformer has much less image-specific inductive bias than CNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are baked into each layer throughout the whole model. In ViT, only MLP layers are local and translationally equivariant, while the self-attention layers are global. The two-dimensional neighborhood structure is used very sparingly: in the beginning of the model by cutting the image into patches and at fine-tuning time for adjusting the position embeddings for images of different resolution (as described below). Other than that, the position embeddings at initialization time carry no information about the 2D positions of the patches and all spatial relations between the patches have to be learned from scratch.</p> <p>Interesting insight about Hybrid ViTs (40 conv layers + transformer blocks):  - It is better on small data regimes but shows no improvement on large data regimes. </p>","tags":["vit","transformers","paper"]},{"location":"100%20Reference%20notes/101%20Literature/Apple%20Intelligence%20Foundation%20Language%20Models/","title":"Apple Intelligence Foundation Language Models","text":"Properties authors Tom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Ke Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Sam Wiseman, Syd Evans, Tao Lei, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, Anupama Mann Anupama, Areeba Kamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Erdenebileg, Chris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd, Fangping Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah Gillis Coleman, Jane Li, Jeffrey Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Maria Cordell, Meng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar, Xin Wang, Xin Zheng, Walker Cheng , Yael Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yunsong Meng, Zhao Tang Luo, Zhi Ouyang, Alp Aygar, Alvin Wan, Andrew Walkingshaw, Andy Narayanan, Antonie Lin, Arsalan Farooq, Brent Ramerth, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang Yang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, Irina Belousova, Joris Pelemans, Karen Yang, Keivan Alizadeh Vahid, Liangliang Cao, Mahyar Najibi, Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qichen Fu, Regan Poston, Sam Xu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui, Vivek Rangarajan Sridhar, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xingyu Zhou, Xinwen Liu, Yang Zhao, Yin Xia, Zhile Ren, Zhongzheng Ren year 2024 url https://arxiv.org/abs/2407.21075 <p>Abstract</p> <p>We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.</p>","tags":["paper","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/Apple%20Intelligence%20Foundation%20Language%20Models/#notes","title":"Notes","text":"<p>\u201cA shared input/output embedding matrix [Press and Wolf, 2016] to reduce memory usage for parameters.\u201d (Gunter et al., 2024, p. 2)</p> <p>This reminds me of the Residual stream interpretation of transformers.</p> <p>\u201cThe model is compressed and quantized, on average under 4-bit-perweight, after the post-training stages (details of the quantization scheme will be discussed later). The quantized model often shows a moderate level of quality loss. Therefore, instead of directly passing the quantized model to application teams for feature development, we attach a set of parameter-efficient LoRa Adapters for quality recovery. We make sure that these LoRA adapters training recipes are consistent with pre-training and post-training processes. Then, products will fine-tune their own feature-specific LoRA adapters by initializing the adapter weights from the accuracy-recovery adapters, while keeping the quantized base model frozen.\u201d (Gunter et al., 2024, p. 16)</p> <p>So the recipe is: - Pre-training/Post-training - Compression? and Quantization (leads to accuracy loss) - LoRa fine-tuning to recover accuracy, call it LoRa Recovery, I'll assume this  - For a specific task, initialize LoRa adapter to the LoRa Recovery Some details: - Rank 16 LoRa - Does each LoRa adapter also share the same precision as the underlying weight block/matrix? I suppose so</p> <p>\u201cSpecifically, our AFM-on-device model running on Apple Neural Engine (ANE) uses Bit Palettization: for projection weights, every 16 columns/rows share the same quantization constants (i.e., lookup tables) and are quantized using K-means with 16 unique values (4-bit).\u201d (Gunter et al., 2024, p. 17)</p>","tags":["paper","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/Approximately%20equivariant%20networks%20for%20imperfectly%20symmetric%20dynamics/","title":"Approximately equivariant networks for imperfectly symmetric dynamics","text":"Properties authors Rui Wang, Robin Walters, Rose Yu year 2022 url https://proceedings.mlr.press/v162/wang22aa.html <p>Abstract</p> <p>Incorporating symmetry as an inductive bias into neural network architecture has led to improvements in generalization, data efficiency, and physical consistency in dynamics modeling. Methods such as CNNs or equivariant neural networks use weight tying to enforce symmetries such as shift invariance or rotational equivariance. However, despite the fact that physical laws obey many symmetries, real-world dynamical data rarely conforms to strict mathematical symmetry either due to noisy or incomplete data or to symmetry breaking features in the underlying dynamical system. We explore approximately equivariant networks which are biased towards preserving symmetry but are not strictly constrained to do so. By relaxing equivariance constraints, we find that our models can outperform both baselines with no symmetry bias and baselines with overly strict symmetry in both simulated turbulence domains and real-world multi-stream jet flow.</p>","tags":["relaxed_equivariance","equivariance","dl_theory","paper"]},{"location":"100%20Reference%20notes/101%20Literature/Approximation-Generalization%20Trade-offs%20under%20%28Approximate%29%20Group%20Equivariance/","title":"Approximation Generalization Trade offs under (Approximate) Group Equivariance","text":"Properties authors Mircea Petrache, Shubhendu Trivedi","tags":["paper"]},{"location":"100%20Reference%20notes/101%20Literature/Autoequivariant%20Network%20Search%20via%20Group%20Decomposition/","title":"Autoequivariant Network Search via Group Decomposition","text":"Properties authors Sourya Basu","tags":["paper"]},{"location":"100%20Reference%20notes/101%20Literature/Battle%20of%20the%20Backbones%20-%20A%20Large-Scale%20Comparison%20of%20Pretrained%20Models%20across%20Computer%20Vision%20Tasks/","title":"Battle of the Backbones   A Large Scale Comparison of Pretrained Models across Computer Vision Tasks","text":"Properties authors Micah Goldblum, Hossein Souri, Renkun Ni, Manli Shu, Viraj Prabhu, Gowthami Somepalli, Prithvijt Chattopadhyay, Mark Ibrahim, Adrien Bardes, Judy Hoffman, Rama Chellappa, Andrew Gordon Wilson, Tom Goldstein year 2023 url https://arxiv.org/abs/2310.19909 <p>Abstract</p> <p>Neural network based computer vision systems are typically built on a backbone, a pretrained or randomly initialized feature extractor. Several years ago, the default option was an ImageNet-trained convolutional neural network. However, the recent past has seen the emergence of countless backbones pretrained using various algorithms and datasets. While this abundance of choice has led to performance increases for a range of systems, it is difficult for practitioners to make informed decisions about which backbone to choose. Battle of the Backbones (BoB) makes this choice easier by benchmarking a diverse suite of pretrained models, including vision-language models, those trained via self-supervised learning, and the Stable Diffusion backbone, across a diverse set of computer vision tasks ranging from classification to object detection to OOD generalization and more. Furthermore, BoB sheds light on promising directions for the research community to advance computer vision by illuminating strengths and weakness of existing approaches through a comprehensive analysis conducted on more than 1500 training runs. While vision transformers (ViTs) and self-supervised learning (SSL) are increasingly popular, we find that convolutional neural networks pretrained in a supervised fashion on large training sets still perform best on most tasks among the models we consider. Moreover, in apples-to-apples comparisons on the same architectures and similarly sized pretraining datasets, we find that SSL backbones are highly competitive, indicating that future works should perform SSL pretraining with advanced architectures and larger pretraining datasets. We release the raw results of our experiments along with code that allows researchers to put their own backbones through the gauntlet here:\u00a0this https URL</p>","tags":["paper","foundation_models","computer_vision","vit","transformers","cnn"]},{"location":"100%20Reference%20notes/101%20Literature/Battle%20of%20the%20Backbones%20-%20A%20Large-Scale%20Comparison%20of%20Pretrained%20Models%20across%20Computer%20Vision%20Tasks/#notes","title":"Notes","text":"<p>It would be nice to see an update with DINOv2 - Learning Robust Visual Features without Supervision and EVA-02 - A Visual Representation for Neon Genesis.</p> <p>A performance comparison of ViTs and CNNs. Modern architectures strongly outperform vanilla ViTs. We see in Table 2 that the best performing backbone (ConvNeXt-Base) is convolutional, with a hierarchical transformer (SwinV2-Base) being a close second. The latter transformer architecture incorporates a strong spatial inductive bias. These findings suggest that the community should move past vanilla ViTs which are still used frequently. As a caveat, we do not evaluate very large models, and it is possible that ViTs might outperform their more advanced variants or convolutional networks at larger scales.</p> <p>Battle of the \u201csmall\u201d backbones. Keeping limited resources in mind, we also compare the \u201csmall\u201d subset of backbones in BoB (&lt; 30M parameters) \u2013 with ViT-Small, ConvNeXt-Tiny, Swin-Tiny and ResNet-50 architectures. Overall, we find Supervised ConvNeXt-T trained on IN-1k to be the best, followed by Supervised SwinV2-T trained on IN-1k and DINO ViT-S trained on IN-1k. Interestingly, supervised learning again dominates, and backbones pretrained on just IN-1k outperform ones trained on a considerably more diverse and larger dataset (MiDaS).</p> <p>Object Detection &amp; Segmentation. For object detection and instance segmentation, we find \u201cSupervised ConvNeXt-Base trained on IN-21K\u201d &gt; \u201cSupervised SwinV2-Base trained on IN-21k (finetuned on IN-1k)\u201d &gt; \u201cSupervised ConvNeXt-Base trained on IN-1k\u201d.</p> <p>These results are probably outdated since many foundation models already beat Swinv2 - SimPLR - A Simple and Plain Transformer for Scaling-Efficient Object Detection and Segmentation - Exploring Plain Vision Transformer Backbones for Object Detection</p>","tags":["paper","foundation_models","computer_vision","vit","transformers","cnn"]},{"location":"100%20Reference%20notes/101%20Literature/Block%20Transformer%20-%20Global-to-Local%20Language%20Modeling%20for%20Fast%20Inference/","title":"Block Transformer   Global to Local Language Modeling for Fast Inference","text":"Properties authors Namgyu Ho, Sangmin Bae, Taehyeon Kim, Hyunjik Jo, Yireun Kim, Tal Schuster, Adam Fisch, James Thorne, Se-Young Yun year 2024 url https://arxiv.org/abs/2406.02657 <p>Abstract</p> <p>This paper presents the Block Transformer architecture which adopts hierarchical global-to-local modeling to autoregressive transformers to mitigate the inference bottlenecks of self-attention. To apply self-attention, the key-value (KV) cache of all previous sequences must be retrieved from memory at every decoding step. Thereby, this KV cache IO becomes a significant bottleneck in batch inference. We notice that these costs stem from applying self-attention on the global context, therefore we isolate the expensive bottlenecks of global modeling to lower layers and apply fast local modeling in upper layers. To mitigate the remaining costs in the lower layers, we aggregate input tokens into fixed size blocks and then apply self-attention at this coarse level. Context information is aggregated into a single embedding to enable upper layers to decode the next block of tokens, without global attention. Free of global attention bottlenecks, the upper layers can fully utilize the compute hardware to maximize inference throughput. By leveraging global and local modules, the Block Transformer architecture demonstrates 10-20x gains in inference throughput compared to vanilla transformers with equivalent perplexity. Our work introduces a new approach to optimize language model inference through novel application of global-to-local modeling. Code is available at https://github.com/itsnamgyu/block-transformer.</p>","tags":["efficient_dl","transformers","paper"]},{"location":"100%20Reference%20notes/101%20Literature/BoxeR%20-%20Box-Attention%20for%202D%20and%203D%20Transformers/","title":"BoxeR   Box Attention for 2D and 3D Transformers","text":"Properties authors Duy-Kien Nguyen, Jihong Ju, Olaf Booij, Martin R. Oswald, Cees G. M. Snoek year 2021 url https://arxiv.org/abs/2111.13087 <p>Abstract</p> <p>In this paper, we propose a simple attention mechanism, we call box-attention. It enables spatial interaction between grid features, as sampled from boxes of interest, and improves the learning capability of transformers for several vision tasks. Specifically, we present BoxeR, short for Box Transformer, which attends to a set of boxes by predicting their transformation from a reference window on an input feature map. The BoxeR computes attention weights on these boxes by considering its grid structure. Notably, BoxeR-2D naturally reasons about box information within its attention module, making it suitable for end-to-end instance detection and segmentation tasks. By learning invariance to rotation in the box-attention module, BoxeR-3D is capable of generating discriminative information from a bird's-eye view plane for 3D end-to-end object detection. Our experiments demonstrate that the proposed BoxeR-2D achieves state-of-the-art results on COCO detection and instance segmentation. Besides, BoxeR-3D improves over the end-to-end 3D object detection baseline and already obtains a compelling performance for the vehicle category of Waymo Open, without any class-specific optimization. Code is available at\u00a0this https URL.</p>","tags":["paper","transformers","object_detection"]},{"location":"100%20Reference%20notes/101%20Literature/Building%20on%20Efficient%20Foundations%20-%20Effectively%20Training%20LLMs%20with%20Structured%20Feedforward%20Layers/","title":"Building on Efficient Foundations   Effectively Training LLMs with Structured Feedforward Layers","text":"Properties authors Xiuying Wei, Skander Moalla, Razvan Pascanu, Caglar Gulcehre year 2024 url https://arxiv.org/abs/2406.16450v1 <p>Abstract</p> <p>State-of-the-art results in large language models (LLMs) often rely on scale, which becomes computationally expensive. This has sparked a research agenda to reduce these models' parameter count and computational costs without significantly impacting their performance. Our study focuses on transformer-based LLMs, specifically targeting the computationally intensive feedforward networks (FFN), which are less studied than attention blocks. We consider three candidate linear layer approximations in the FFN by combining efficient low-rank and block-diagonal matrices. In contrast to many previous works that examined these approximations, our study i) explores these structures from the training-from-scratch perspective, ii) scales up to 1.3B parameters, and iii) is conducted within recent Transformer-based LLMs rather than convolutional architectures. We first demonstrate they can lead to actual computational gains in various scenarios, including online decoding when using a pre-merge technique. Additionally, we propose a novel training regime, called \\textit{self-guided training}, aimed at improving the poor training dynamics that these approximations exhibit when used from initialization. Experiments on the large RefinedWeb dataset show that our methods are both efficient and effective for training and inference. Interestingly, these structured FFNs exhibit steeper scaling curves than the original models. Further applying self-guided training to the structured matrices with 32\\% FFN parameters and 2.5\u00d7\u00a0speed-up enables only a 0.4 perplexity increase under the same training FLOPs. Finally, we develop the wide and structured networks surpassing the current medium-sized and large-sized Transformer in perplexity and throughput performance. Our code is available at \\url{this https URL}.</p>","tags":["paper","efficient_dl","llm","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/Building%20on%20Efficient%20Foundations%20-%20Effectively%20Training%20LLMs%20with%20Structured%20Feedforward%20Layers/#notes","title":"Notes","text":"<ul> <li> Note to self: Read this in depth \u23eb  #personal</li> </ul>","tags":["paper","efficient_dl","llm","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/CKConv%20-%20Continuous%20Kernel%20Convolution%20For%20Sequential%20Data/","title":"CKConv   Continuous Kernel Convolution For Sequential Data","text":"Properties authors David W. Romero, Anna Kuzina, Erik J. Bekkers, Jakub M. Tomczak, Mark Hoogendoorn year 2021 url https://arxiv.org/abs/2102.02611 <p>Abstract</p> <p>Conventional neural architectures for sequential data present important limitations. Recurrent networks suffer from exploding and vanishing gradients, small effective memory horizons, and must be trained sequentially. Convolutional networks are unable to handle sequences of unknown size and their memory horizon must be defined a priori. In this work, we show that all these problems can be solved by formulating convolutional kernels in CNNs as continuous functions. The resulting Continuous Kernel Convolution (CKConv) allows us to model arbitrarily long sequences in a parallel manner, within a single operation, and without relying on any form of recurrence. We show that Continuous Kernel Convolutional Networks (CKCNNs) obtain state-of-the-art results in multiple datasets, e.g., permuted MNIST, and, thanks to their continuous nature, are able to handle non-uniformly sampled datasets and irregularly-sampled data natively. CKCNNs match or perform better than neural ODEs designed for these purposes in a faster and simpler manner.</p>","tags":["paper","convolutions","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Color%20Equivariant%20Convolutional%20Networks/","title":"Color Equivariant Convolutional Networks","text":"Properties authors Attila Lengyel, Ombretta Strafforello, Robert-Jan Bruintjes, Alexander Gielisse, Jan van Gemert <p>References: - Learning Partial Equivariances from Data</p>","tags":["paper"]},{"location":"100%20Reference%20notes/101%20Literature/Color%20Space%20Transformation%20Network/","title":"Color Space Transformation Network","text":"Properties authors Alexandros Karargyris year 2015 url https://arxiv.org/abs/1511.01064 <p>Abstract</p> <p>Deep networks have become very popular over the past few years. The main reason for this widespread use is their excellent ability to learn and predict knowledge in a very easy and efficient way. Convolutional neural networks and auto-encoders have become the normal in the area of imaging and computer vision achieving unprecedented accuracy levels in many applications. The most common strategy is to build and train networks with many layers by tuning their hyper-parameters. While this approach has proven to be a successful way to build robust deep learning schemes it suffers from high complexity. In this paper we introduce a module that learns color space transformations within a network. Given a large dataset of colored images the color space transformation module tries to learn color space transformations that increase overall classification accuracy. This module has shown to increase overall accuracy for the same network design and to achieve faster convergence. It is part of a broader family of image transformations (e.g. spatial transformer network).</p>","tags":["cnn","paper"]},{"location":"100%20Reference%20notes/101%20Literature/ConViT%20-%20Improving%20Vision%20Transformers%20with%20Soft%20Convolutional%20Inductive%20Biases/","title":"ConViT   Improving Vision Transformers with Soft Convolutional Inductive Biases","text":"Properties authors St\u00e9phane d'Ascoli, Hugo Touvron, Matthew L. Leavitt, Ari S. Morcos, Giulio Biroli, Levent Sagun <p>Abstract</p> <p>TODO:  - [ ] Read paper - [ ] Add main text summary</p> <p>From Early Convolutions Help Transformers See Better, where [9] is this paper:</p> <p>We did not observe evidence that the hard locality constraint in early layers hampers the representational capacity of the network, as might be feared [9]. [...] This perspective resonates with the findings of [9], who observe that early transformer blocks prefer to learn more local attention patterns than later blocks.</p> <p>This is contrary to How do vision transformers work?, as they claim that locality constraint is beneficial to ViTs. </p> <p>Haven't fully read this paper, so the above contradiction might be incorrect.</p>","tags":["vit","computer_vision","cnn","transformers","inductive_bias","paper"]},{"location":"100%20Reference%20notes/101%20Literature/DETRs%20Beat%20YOLOs%20on%20Real-time%20Object%20Detection/","title":"DETRs Beat YOLOs on Real time Object Detection","text":"Properties authors Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu, Jie Chen year 2023 url https://arxiv.org/abs/2304.08069v3 <p>Abstract</p> <p>The YOLO series has become the most popular framework for real-time object detection due to its reasonable trade-off between speed and accuracy. However, we observe that the speed and accuracy of YOLOs are negatively affected by the NMS. Recently, end-to-end Transformer-based detectors (DETRs) have provided an alternative to eliminating NMS. Nevertheless, the high computational cost limits their practicality and hinders them from fully exploiting the advantage of excluding NMS. In this paper, we propose the Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge that addresses the above dilemma. We build RT-DETR in two steps, drawing on the advanced DETR: first we focus on maintaining accuracy while improving speed, followed by maintaining speed while improving accuracy. Specifically, we design an efficient hybrid encoder to expeditiously process multi-scale features by decoupling intra-scale interaction and cross-scale fusion to improve speed. Then, we propose the uncertainty-minimal query selection to provide high-quality initial queries to the decoder, thereby improving accuracy. In addition, RT-DETR supports flexible speed tuning by adjusting the number of decoder layers to adapt to various scenarios without retraining. Our RT-DETR-R50 / R101 achieves 53.1% / 54.3% AP on COCO and 108 / 74 FPS on T4 GPU, outperforming previously advanced YOLOs in both speed and accuracy. We also develop scaled RT-DETRs that outperform the lighter YOLO detectors (S and M models). Furthermore, RT-DETR-R50 outperforms DINO-R50 by 2.2% AP in accuracy and about 21 times in FPS. After pre-training with Objects365, RT-DETR-R50 / R101 achieves 55.3% / 56.2% AP. The project page:\u00a0this https URL.</p>","tags":["paper","computer_vision","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/DETRs%20with%20Collaborative%20Hybrid%20Assignments%20Training/","title":"DETRs with Collaborative Hybrid Assignments Training","text":"Properties authors Zhuofan Zong, Guanglu Song, Yu Liu year 2023 url https://arxiv.org/abs/2211.12860v5 <p>Abstract</p> <p>In this paper, we provide the observation that too few queries assigned as positive samples in DETR with one-to-one set matching leads to sparse supervision on the encoder's output which considerably hurt the discriminative feature learning of the encoder and vice visa for attention learning in the decoder. To alleviate this, we present a novel collaborative hybrid assignments training scheme, namely\u00a0\ue22fo-DETR, to learn more efficient and effective DETR-based detectors from versatile label assignment manners. This new training scheme can easily enhance the encoder's learning ability in end-to-end detectors by training the multiple parallel auxiliary heads supervised by one-to-many label assignments such as ATSS and Faster RCNN. In addition, we conduct extra customized positive queries by extracting the positive coordinates from these auxiliary heads to improve the training efficiency of positive samples in the decoder. In inference, these auxiliary heads are discarded and thus our method introduces no additional parameters and computational cost to the original detector while requiring no hand-crafted non-maximum suppression (NMS). We conduct extensive experiments to evaluate the effectiveness of the proposed approach on DETR variants, including DAB-DETR, Deformable-DETR, and DINO-Deformable-DETR. The state-of-the-art DINO-Deformable-DETR with Swin-L can be improved from 58.5% to 59.5% AP on COCO val. Surprisingly, incorporated with ViT-L backbone, we achieve 66.0% AP on COCO test-dev and 67.9% AP on LVIS val, outperforming previous methods by clear margins with much fewer model sizes. Codes are available at \\url{this https URL}.</p>","tags":["paper","object_detection","computer_vision","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/DETRs%20with%20Collaborative%20Hybrid%20Assignments%20Training/#notes","title":"Notes","text":"<p>Beats EVA-02 - A Visual Representation for Neon Genesis on object detection.</p> <p>Weights for CO-DINO Swin-L (64.1 box AP on COCO val): https://github.com/Sense-X/Co-DETR?tab=readme-ov-file</p>","tags":["paper","object_detection","computer_vision","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/DINOv2%20-%20Learning%20Robust%20Visual%20Features%20without%20Supervision/","title":"DINOv2   Learning Robust Visual Features without Supervision","text":"Properties authors Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Rusell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv\u00e9 Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski year 2023 url https://arxiv.org/abs/2304.07193 <p>Abstract</p> <p>The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.</p>","tags":["paper","foundation_models","computer_vision","vit"]},{"location":"100%20Reference%20notes/101%20Literature/Deep%20Learning%20Book/","title":"Deep Learning Book","text":"Properties authors Ian Goodfellow, Yoshua Bengio, Aaron Courville year 2016 url https://www.deeplearningbook.org/","tags":["dl_theory","textbook"]},{"location":"100%20Reference%20notes/101%20Literature/DenseNets%20Reloaded%20-%20Paradigm%20Shift%20Beyond%20ResNets%20and%20ViTs/","title":"DenseNets Reloaded   Paradigm Shift Beyond ResNets and ViTs","text":"Properties authors Donghyun Kim, Byeongho Heo, Dongyoon Han year 2024 url https://arxiv.org/abs/2403.19588 <p>Abstract</p> <p>This paper revives Densely Connected Convolutional Networks (DenseNets) and reveals the underrated effectiveness over predominant ResNet-style architectures. We believe DenseNets' potential was overlooked due to untouched training methods and traditional design elements not fully revealing their capabilities. Our pilot study shows dense connections through concatenation are strong, demonstrating that DenseNets can be revitalized to compete with modern architectures. We methodically refine suboptimal components - architectural adjustments, block redesign, and improved training recipes towards widening DenseNets and boosting memory efficiency while keeping concatenation shortcuts. Our models, employing simple architectural elements, ultimately surpass Swin Transformer, ConvNeXt, and DeiT-III - key architectures in the residual learning lineage. Furthermore, our models exhibit near state-of-the-art performance on ImageNet-1K, competing with the very recent models and downstream tasks, ADE20k semantic segmentation, and COCO object detection/instance segmentation. Finally, we provide empirical analyses that uncover the merits of the concatenation over additive shortcuts, steering a renewed preference towards DenseNet-style designs. Our code is available at\u00a0this https URL.</p>","tags":["cnn","dl_theory","optimizability","paper"]},{"location":"100%20Reference%20notes/101%20Literature/Discovering%20Symmetry%20Breaking%20in%20Physical%20Systems%20with%20Relaxed%20Group%20Convolution/","title":"Discovering Symmetry Breaking in Physical Systems with Relaxed Group Convolution","text":"Properties authors Rui Wang, Elyssa Hofgard, Han Gao, Robin Walters, Tess E Smidt year 2024 url https://arxiv.org/abs/2310.02299 <p>Abstract</p> <p>Modeling symmetry breaking is essential for understanding the fundamental changes in the behaviors and properties of physical systems, from microscopic particle interactions to macroscopic phenomena like fluid dynamics and cosmic structures. Thus, identifying sources of asymmetry is an important tool for understanding physical systems. In this paper, we focus on learning asymmetries of data using relaxed group convolutions. We provide both theoretical and empirical evidence that this flexible convolution technique allows the model to maintain the highest level of equivariance that is consistent with data and discover the subtle symmetry-breaking factors in various physical systems. We employ various relaxed group convolution architectures to uncover various symmetry-breaking factors that are interpretable and physically meaningful in different physical systems, including the phase transition of crystal structure, the isotropy and homogeneity breaking in turbulent flow, and the time-reversal symmetry breaking in pendulum systems.</p> <p>Observations: - \"In the relaxed group convolution, the initial relaxed (equivariant) weights\u00a0{\ud835\udc64\ud835\udc59\u2062(\u210e)}\u00a0in each layer are set to be the same for all\u00a0\u210e, ensuring that the model exhibits equivariance prior to being trained. [...] we prove that these relaxed weights only deviate from being equal when the symmetries of the input and the output are lower than that of the model.\" (Related to Equivariance Initialization)</p>","tags":["equivariance","relaxed_equivariance","dl_theory","paper"]},{"location":"100%20Reference%20notes/101%20Literature/EVA-02%20-%20A%20Visual%20Representation%20for%20Neon%20Genesis/","title":"EVA 02   A Visual Representation for Neon Genesis","text":"Properties authors Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, Yue Cao year 2023 url https://arxiv.org/abs/2303.11331 <p>Abstract</p> <p>We launch EVA-02, a next-generation Transformer-based visual representation pre-trained to reconstruct strong and robust language-aligned vision features via masked image modeling. With an updated plain Transformer architecture as well as extensive pre-training from an open &amp; accessible giant CLIP vision encoder, EVA-02 demonstrates superior performance compared to prior state-of-the-art approaches across various representative vision tasks, while utilizing significantly fewer parameters and compute budgets. Notably, using exclusively publicly accessible training data, EVA-02 with only 304M parameters achieves a phenomenal 90.0 fine-tuning top-1 accuracy on ImageNet-1K val set. Additionally, our EVA-02-CLIP can reach up to 80.4 zero-shot top-1 on ImageNet-1K, outperforming the previous largest &amp; best open-sourced CLIP with only ~1/6 parameters and ~1/6 image-text training data. We offer four EVA-02 variants in various model sizes, ranging from 6M to 304M parameters, all with impressive performance. To facilitate open access and open research, we release the complete suite of EVA-02 to the community at\u00a0this https URL.</p>","tags":["paper","foundation_models","computer_vision","vit"]},{"location":"100%20Reference%20notes/101%20Literature/Early%20Convolutions%20Help%20Transformers%20See%20Better/","title":"Early Convolutions Help Transformers See Better","text":"Properties authors Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Doll\u00e1r, Ross Girshick <p>Hypothesis</p> <p>ViT's patchify convolution is contrary to standard early layers in CNNs. Maybe that's the cause?</p> <p>Main idea</p> <p>Replace patchify convolution with a small number of convolutional layers and drop one transformer block to make comparison fair.</p> <p></p> <p>Notes for myself: - Interesting experimentation regarding #optimizability , maybe take into account into hessian analysis</p>","tags":["cnn","transformers","vit","optimizability","paper"]},{"location":"100%20Reference%20notes/101%20Literature/Efficient%20Equivariant%20Transfer%20Learning%20from%20Pretrained%20Models/","title":"Efficient Equivariant Transfer Learning from Pretrained Models","text":"Properties authors Sourya Basu <p>Builds on top of Equi-Tuning - Group Equivariant Fine-Tuning of Pretrained Models and Equivariance with Learned Canonicalization Functions</p> <p>Hypothesis</p> <p>Pretrained models provide better quality features for certain transformations than others and simply averaging them is bad.</p> <p>Main idea</p> <p>Lambda-Equitune: Weighted average with learned weights, \\(\\lambda\\).</p> \\[ M_G^\\lambda(x) = \\frac{1}{\\sum_{g \\in G} \\lambda(gx)} \\sum_{g \\in G} \\lambda(gx) g^{-1} M(gx)  \\]","tags":["paper"]},{"location":"100%20Reference%20notes/101%20Literature/Efficient%20Modulation%20for%20Vision%20Networks/","title":"Efficient Modulation for Vision Networks","text":"Properties authors Xu Ma, Xiyang Dai, Jianwei Yang, Bin Xiao, Yinpeng Chen, Yun Fu, Lu Yuan year 2024 url https://arxiv.org/abs/2403.19963 <p>Abstract</p> <p>In this work, we present efficient modulation, a novel design for efficient vision networks. We revisit the modulation mechanism, which operates input through convolutional context modeling and feature projection layers, and fuses features via element-wise multiplication and an MLP block. We demonstrate that the modulation mechanism is particularly well suited for efficient networks and further tailor the modulation design by proposing the efficient modulation (EfficientMod) block, which is considered the essential building block for our networks. Benefiting from the prominent representational ability of modulation mechanism and the proposed efficient design, our network can accomplish better trade-offs between accuracy and efficiency and set new state-of-the-art performance in the zoo of efficient networks. When integrating EfficientMod with the vanilla self-attention block, we obtain the hybrid architecture which further improves the performance without loss of efficiency. We carry out comprehensive experiments to verify EfficientMod's performance. With fewer parameters, our EfficientMod-s performs 0.6 top-1 accuracy better than EfficientFormerV2-s2 and is 25% faster on GPU, and 2.9 better than MobileViTv2-1.0 at the same GPU latency. Additionally, our method presents a notable improvement in downstream tasks, outperforming EfficientFormerV2-s by 3.6 mIoU on the ADE20K benchmark. Code and checkpoints are available at\u00a0this https URL.</p>","tags":["efficient_dl","computer_vision"]},{"location":"100%20Reference%20notes/101%20Literature/EfficientViT-SAM%20-%20Accelerated%20Segment%20Anything%20Model%20Without%20Accuracy%20Loss/","title":"EfficientViT SAM   Accelerated Segment Anything Model Without Accuracy Loss","text":"Properties authors Zhuoyang Zhang, Han Cai, Song Han year 2024 url https://arxiv.org/abs/2402.05008 <p>Abstract</p> <p>We present EfficientViT-SAM, a new family of accelerated segment anything models. We retain SAM's lightweight prompt encoder and mask decoder while replacing the heavy image encoder with EfficientViT. For the training, we begin with the knowledge distillation from the SAM-ViT-H image encoder to EfficientViT. Subsequently, we conduct end-to-end training on the SA-1B dataset. Benefiting from EfficientViT's efficiency and capacity, EfficientViT-SAM delivers 48.9x measured TensorRT speedup on A100 GPU over SAM-ViT-H without sacrificing performance. Our code and pre-trained models are released at\u00a0this https URL.</p>","tags":["paper","efficient_dl","vit"]},{"location":"100%20Reference%20notes/101%20Literature/Emergent%20Equivariance%20in%20Deep%20Ensembles/","title":"Emergent Equivariance in Deep Ensembles","text":"Properties authors Jan E. Gerken, Pan Kessel year 2024 url https://arxiv.org/abs/2403.03103 <p>Abstract</p> <p>We demonstrate that deep ensembles are secretly equivariant models. More precisely, we show that deep ensembles become equivariant for all inputs and at all training times by simply using data augmentation. Crucially, equivariance holds off-manifold and for any architecture in the infinite width limit. The equivariance is emergent in the sense that predictions of individual ensemble members are not equivariant but their collective prediction is. Neural tangent kernel theory is used to derive this result and we verify our theoretical insights using detailed numerical experiments.</p>","tags":["equivariance","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Emerging%20Properties%20in%20Self-Supervised%20Vision%20Transformers/","title":"Emerging Properties in Self Supervised Vision Transformers","text":"Properties authors Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 Jegou, Julien Mairal, Piotr Bojanowski, Armand Joulin year 2021 url https://arxiv.org/abs/2104.14294 <p>Abstract</p> <p>In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.</p>","tags":["paper","foundation_models","computer_vision","vit"]},{"location":"100%20Reference%20notes/101%20Literature/End-to-End%20Object%20Detection%20with%20Transformers/","title":"End to End Object Detection with Transformers","text":"Properties authors Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko year 2020 url https://arxiv.org/abs/2005.12872 <p>Abstract</p> <p>We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at\u00a0this https URL.</p>","tags":["paper","computer_vision","object_detection","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/Equi-Tuning%20-%20Group%20Equivariant%20Fine-Tuning%20of%20Pretrained%20Models/","title":"Equi Tuning   Group Equivariant Fine Tuning of Pretrained Models","text":"Properties authors Sourya Basu <p>Main idea</p> <p>Given non-equivariant pre-trained model \\(M(x)\\), define equivariant model \\(M_G(x)\\), as the average of the inverted predictions for all group actions on input \\(x\\)</p> \\[  M_G(x) = \\frac{1}{|G|} \\sum_{g \\in G} g^{-1}  M(g x) \\] <p>Abstract</p>","tags":["dl2"]},{"location":"100%20Reference%20notes/101%20Literature/Equivariance%20with%20Learned%20Canonicalization%20Functions/","title":"Equivariance with Learned Canonicalization Functions","text":"Properties authors S\u00e9kou-Oumar Kaba, Arnab Kumar Mondal, Yan Zhang, Yoshua Bengio, Siamak Ravanbakhsh <p>Main idea</p> <p>We learn a canonicalization function \\(h\\) either by a neural network or an optimization procedure. $$ \\phi(x) = h'(x) f(h(x)^{-1} x) $$</p> <p>Abstract</p>","tags":["dl2"]},{"location":"100%20Reference%20notes/101%20Literature/Equivariance-aware%20architectural%20optimization%20of%20neural%20networks/","title":"Equivariance aware architectural optimization of neural networks","text":"Properties authors Kaitlin Maile, Dennis G. Wilson, Patrick Forr\u00e9 <p>References: - Learning Partial Equivariances from Data</p> <p>Abstract</p>","tags":["dl2"]},{"location":"100%20Reference%20notes/101%20Literature/Exact%20Conversion%20of%20In-Context%20Learning%20to%20Model%20Weights%20in%20Linearized-Attention%20Transformers/","title":"Exact Conversion of In Context Learning to Model Weights in Linearized Attention Transformers","text":"Properties authors Brian K Chen, Tianyang Hu, Hui Jin, Hwee Kuan Lee, Kenji Kawaguchi year 2024 url https://arxiv.org/abs/2406.02847 <p>Abstract</p> <p>In-Context Learning (ICL) has been a powerful emergent property of large language models that has attracted increasing attention in recent years. In contrast to regular gradient-based learning, ICL is highly interpretable and does not require parameter updates. In this paper, we show that, for linearized transformer networks, ICL can be made explicit and permanent through the inclusion of bias terms. We mathematically demonstrate the equivalence between a model with ICL demonstration prompts and the same model with the additional bias terms. Our algorithm (ICLCA) allows for exact conversion in an inexpensive manner. Existing methods are not exact and require expensive parameter updates. We demonstrate the efficacy of our approach through experiments that show the exact incorporation of ICL tokens into a linear transformer. We further suggest how our method can be adapted to achieve cheap approximate conversion of ICL tokens, even in regular transformer networks that are not linearized. Our experiments on GPT-2 show that, even though the conversion is only approximate, the model still gains valuable context from the included bias terms.</p>","tags":["paper","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/Exploiting%20Redundancy%20-%20Separable%20Group%20Convolutional%20Networks%20on%20Lie%20Groups/","title":"Exploiting Redundancy   Separable Group Convolutional Networks on Lie Groups","text":"Properties authors David M. Knigge, David W. Romero, Erik J. Bekkers <p>Abstract</p> <p>In this work, we investigate the properties of representations learned by regular G-CNNs, and show considerable parameter redundancy in group convolution kernels. This finding motivates further weight-tying by sharing convolution kernels over subgroups. To this end, we introduce convolution kernels that are separable over the subgroup and channel dimensions.</p> <p>Interesting because it reduces the total parameter count by separating group convolution kernels. This also has a regularisation effect.</p> <p>Citations: - Relaxing Equivariance Constraints with Non-stationary Continuous Filters</p>","tags":["dl2"]},{"location":"100%20Reference%20notes/101%20Literature/Exploring%20Plain%20Vision%20Transformer%20Backbones%20for%20Object%20Detection/","title":"Exploring Plain Vision Transformer Backbones for Object Detection","text":"Properties authors Yanghao Li, Hanzi Mao, Ross Girshick, Kaiming He year 2023 url https://arxiv.org/abs/2203.16527 <p>Abstract</p> <p>We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection. This design enables the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for fine-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is sufficient to build a simple feature pyramid from a single-scale feature map (without the common FPN design) and (ii) it is sufficient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain ViT backbones pre-trained as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the previous leading methods that were all based on hierarchical backbones, reaching up to 61.3 AP_box on the COCO dataset using only ImageNet-1K pre-training. We hope our study will draw attention to research on plain-backbone detectors. Code for ViTDet is available in Detectron2.</p>","tags":["paper","computer_vision","object_detection","transformers","vit"]},{"location":"100%20Reference%20notes/101%20Literature/Exploring%20Plain%20Vision%20Transformer%20Backbones%20for%20Object%20Detection/#notes","title":"Notes","text":"<ul> <li>It effectively adapts a pre-trained vision transformers as backbones and decoder heads by adding minimal layers in between to make them work</li> <li>Requires full fine-tuning</li> <li>Ranks #16 on https://paperswithcode.com/sota/object-detection-on-coco-minival, ~4 box map points lower than the first spot </li> </ul> <p>Code and weights at: https://github.com/facebookresearch/detectron2/tree/main/projects/ViTDet</p>","tags":["paper","computer_vision","object_detection","transformers","vit"]},{"location":"100%20Reference%20notes/101%20Literature/Fast%2C%20Expressive%20SE%28n%29%20Equivariant%20Networks%20through%20Weight-Sharing%20in%20Position-Orientation%20Space/","title":"Fast, Expressive SE(n) Equivariant Networks through Weight Sharing in Position Orientation Space","text":"Properties authors Erik J. Bekkers, Sharvaree Vadgama, Rob D. Hesselink, Putri A. van der Linden, David W. Romero <p>Abstract</p>","tags":["dl2"]},{"location":"100%20Reference%20notes/101%20Literature/FlexiViT%20-%20One%20Model%20for%20All%20Patch%20Sizes/","title":"FlexiViT   One Model for All Patch Sizes","text":"Properties authors Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon Kornblith, Xiaohua Zhai, Matthias Minderer, Michael Tschannen, Ibrahim Alabdulmoshin, Filip Pavetic year 2022 url https://arxiv.org/abs/2212.08013 <p>Abstract</p> <p>Vision Transformers convert images to sequences by slicing them into patches. The size of these patches controls a speed/accuracy tradeoff, with smaller patches leading to higher accuracy at greater computational cost, but changing the patch size typically requires retraining the model. In this paper, we demonstrate that simply randomizing the patch size at training time leads to a single set of weights that performs well across a wide range of patch sizes, making it possible to tailor the model to different compute budgets at deployment time. We extensively evaluate the resulting model, which we call FlexiViT, on a wide range of tasks, including classification, image-text retrieval, open-world detection, panoptic segmentation, and semantic segmentation, concluding that it usually matches, and sometimes outperforms, standard ViT models trained at a single patch size in an otherwise identical setup. Hence, FlexiViT training is a simple drop-in improvement for ViT that makes it easy to add compute-adaptive capabilities to most models relying on a ViT backbone architecture. Code and pre-trained models are available at\u00a0this https URL</p>","tags":["paper","foundation_models","computer_vision","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/FlexiViT%20-%20One%20Model%20for%20All%20Patch%20Sizes/#notes","title":"Notes","text":"<ul> <li> Read in depth, seems very promising</li> <li>Google already filed a patent for this: https://patents.google.com/patent/US20240169715A1/en</li> </ul>","tags":["paper","foundation_models","computer_vision","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/G-SGD%20-%20Optimizing%20ReLU%20Neural%20Networks%20in%20its%20Positively%20Scale-Invariant%20Space/","title":"G SGD   Optimizing ReLU Neural Networks in its Positively Scale Invariant Space","text":"Properties authors Qi Meng, Shuxin Zheng, Huishuai Zhang, Wei Chen, Zhi-Ming Ma, Tie-Yan Liu year 2018 url https://arxiv.org/abs/1802.03713 <p>Abstract</p> <p>It is well known that neural networks with rectified linear units (ReLU) activation functions are positively scale-invariant. Conventional algorithms like stochastic gradient descent optimize the neural networks in the vector space of weights, which is, however, not positively scale-invariant. This mismatch may lead to problems during the optimization process. Then, a natural question is: \\emph{can we construct a new vector space that is positively scale-invariant and sufficient to represent ReLU neural networks so as to better facilitate the optimization process }? In this paper, we provide our positive answer to this question. First, we conduct a formal study on the positive scaling operators which forms a transformation group, denoted as\u00a0\ue233. We show that the value of a path (i.e. the product of the weights along the path) in the neural network is invariant to positive scaling and prove that the value vector of all the paths is sufficient to represent the neural networks under mild conditions. Second, we show that one can identify some basis paths out of all the paths and prove that the linear span of their value vectors (denoted as\u00a0\ue233-space) is an invariant space with lower dimension under the positive scaling group. Finally, we design stochastic gradient descent algorithm in\u00a0\ue233-space (abbreviated as\u00a0\ue233-SGD) to optimize the value vector of the basis paths of neural networks with little extra cost by leveraging back-propagation. Our experiments show that\u00a0\ue233-SGD significantly outperforms the conventional SGD algorithm in optimizing ReLU networks on benchmark datasets.</p>","tags":["dl_theory","dl2"]},{"location":"100%20Reference%20notes/101%20Literature/Grokked%20Transformers%20are%20Implicit%20Reasoners%20-%20A%20Mechanistic%20Journey%20to%20the%20Edge%20of%20Generalization/","title":"Grokked Transformers are Implicit Reasoners   A Mechanistic Journey to the Edge of Generalization","text":"Properties authors Boshi Wang, Xiang Yue, Yu Su, Huan Sun year 2024 url https://arxiv.org/abs/2405.15071 <p>Abstract</p> <p>We study whether transformers can learn to implicitly reason over parametric knowledge, a skill that even the most capable language models struggle with. Focusing on two representative reasoning types, composition and comparison, we consistently find that transformers can learn implicit reasoning, but only through grokking, i.e., extended training far beyond overfitting. The levels of generalization also vary across reasoning types: when faced with out-of-distribution examples, transformers fail to systematically generalize for composition but succeed for comparison. We delve into the model's internals throughout training, conducting analytical experiments that reveal: 1) the mechanism behind grokking, such as the formation of the generalizing circuit and its relation to the relative efficiency of generalizing and memorizing circuits, and 2) the connection between systematicity and the configuration of the generalizing circuit. Our findings guide data and training setup to better induce implicit reasoning and suggest potential improvements to the transformer architecture, such as encouraging cross-layer knowledge sharing. Furthermore, we demonstrate that for a challenging reasoning task with a large search space, GPT-4-Turbo and Gemini-1.5-Pro based on non-parametric memory fail badly regardless of prompting styles or retrieval augmentation, while a fully grokked transformer can achieve near-perfect accuracy, showcasing the power of parametric memory for complex reasoning.</p>","tags":["paper","transformers","mechinterp"]},{"location":"100%20Reference%20notes/101%20Literature/Harmonics%20of%20Learning%20-%20Universal%20Fourier%20Features%20Emerge%20in%20Invariant%20Networks/","title":"Harmonics of Learning   Universal Fourier Features Emerge in Invariant Networks","text":"Properties authors Giovanni Luca Marchetti, Christopher Hillar, Danica Kragic, Sophia Sanborn year 2023 url https://arxiv.org/abs/2312.08550 <p>Abstract</p> <p>In this work, we formally prove that, under certain conditions, if a neural network is invariant to a finite group then its weights recover the Fourier transform on that group. This provides a mathematical explanation for the emergence of Fourier features -- a ubiquitous phenomenon in both biological and artificial learning systems. The results hold even for non-commutative groups, in which case the Fourier transform encodes all the irreducible unitary group representations. Our findings have consequences for the problem of symmetry discovery. Specifically, we demonstrate that the algebraic structure of an unknown group can be recovered from the weights of a network that is at least approximately invariant within certain bounds. Overall, this work contributes to a foundation for an algebraic learning theory of invariant neural network representations.</p> <p></p>","tags":["theory","equivariance","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/How%20do%20vision%20transformers%20work%3F/","title":"How do vision transformers work?","text":"Properties authors Namuk Park, Songkuk Kim year 2022 url https://arxiv.org/abs/2202.06709 <p>Abstract</p> <p>The success of multi-head self-attentions (MSAs) for computer vision is now indisputable. However, little is known about how MSAs work. We present fundamental explanations to help better understand the nature of MSAs. In particular, we demonstrate the following properties of MSAs and Vision Transformers (ViTs): (1) MSAs improve not only accuracy but also generalization by flattening the loss landscapes. Such improvement is primarily attributable to their data specificity, not long-range dependency. On the other hand, ViTs suffer from non-convex losses. Large datasets and loss landscape smoothing methods alleviate this problem; (2) MSAs and Convs exhibit opposite behaviors. For example, MSAs are low-pass filters, but Convs are high-pass filters. Therefore, MSAs and Convs are complementary; (3) Multi-stage neural networks behave like a series connection of small individual models. In addition, MSAs at the end of a stage play a key role in prediction. Based on these insights, we propose AlterNet, a model in which Conv blocks at the end of a stage are replaced with MSA blocks. AlterNet outperforms CNNs not only in large data regimes but also in small data regimes. The code is available at\u00a0this https URL.</p>","tags":["vit","computer_vision","cnn","optimizability"]},{"location":"100%20Reference%20notes/101%20Literature/How%20do%20vision%20transformers%20work%3F/#notes","title":"Notes","text":"","tags":["vit","computer_vision","cnn","optimizability"]},{"location":"100%20Reference%20notes/101%20Literature/How%20do%20vision%20transformers%20work%3F/#the-question-of-inductive-biases","title":"The question of inductive biases","text":"<p>Contrary to our expectations, experimental results show that the stronger the inductive bias, the lower both the test error and the training NLL. This indicates that ViT does not overfit training datasets. In addition, appropriate inductive biases, such as locality constraints for MSAs, helps NNs learn strong representations. We also observe these phenomena on CIFAR-10 and ImageNet as shown in Fig. C.1. Figure C.2 also supports that weak inductive biases disrupt NN training. In this experiment, extremely small patch sizes for the embedding hurt the predictive performance of ViT.</p> <p>Long range (global) attention is worse than local attention. MSA are good because they smooth loss landscape and are input dependent.</p> <p>What properties of MSAs do we need to improve optimization? We present various evidences to support that MSA is generalized spatial smoothing. It means that MSAs improve performance because their formulation\u2014Eq. (1)\u2014is an appropriate inductive bias. Their weak inductive bias disrupts NN training. In particular, a key feature of MSAs is their data specificity, not long-range dependency. As an extreme example, local MSAs with a 3 \u00d7 3 receptive field outperforms global MSA because they reduce unnecessary degrees of freedom. </p> <p>As far as my understanding goes, local MSA is not translation equivariant because it still is input dependent. So Local MSA has locality inductive bias but not translation equivariance. This is interesting, normal ConvNets do locality inductive bias by translation equivariance and it is not straight forward to remove their translation equivariance. Tracking at Input-dependent convolutions and Non-translationally equivariant convolutions.</p> <p>Locality inductive biases help with more stable training dynamics </p>","tags":["vit","computer_vision","cnn","optimizability"]},{"location":"100%20Reference%20notes/101%20Literature/How%20do%20vision%20transformers%20work%3F/#hessian-spectra","title":"Hessian Spectra","text":"<p> Legend:  ViT (red), CNN (blue) - ViT has small magnitude and negative values - CNN has large magnitude and positive values</p>","tags":["vit","computer_vision","cnn","optimizability"]},{"location":"100%20Reference%20notes/101%20Literature/Hydra%20-%20Bidirectional%20State%20Space%20Models%20Through%20Generalized%20Matrix%20Mixers/","title":"Hydra   Bidirectional State Space Models Through Generalized Matrix Mixers","text":"Properties authors Sukjun Hwang, Aakash Lahoti, Tri Dao, Albert Gu year 2024 url https://arxiv.org/abs/2407.09941 <p>Abstract</p> <p>A wide array of sequence models are built on a framework modeled after Transformers, comprising alternating sequence mixer and channel mixer layers. This paper studies a unifying matrix mixer view of sequence mixers that can be conceptualized as a linear map on the input sequence. This framework encompasses a broad range of well-known sequence models, including the self-attention of Transformers as well as recent strong alternatives such as structured state space models (SSMs), and allows understanding downstream characteristics such as efficiency and expressivity through properties of their structured matrix class. We identify a key axis of matrix parameterizations termed sequence alignment, which increases the flexibility and performance of matrix mixers, providing insights into the strong performance of Transformers and recent SSMs such as Mamba. Furthermore, the matrix mixer framework offers a systematic approach to developing sequence mixers with desired properties, allowing us to develop several new sub-quadratic sequence models. In particular, we propose a natural bidirectional extension of the Mamba model (Hydra), parameterized as a quasiseparable matrix mixer, which demonstrates superior performance over other sequence models including Transformers on non-causal tasks. As a drop-in replacement for attention layers, Hydra outperforms BERT by 0.8 points on the GLUE benchmark and ViT by 2% Top-1 accuracy on ImageNet.</p>","tags":["paper","sequence_models"]},{"location":"100%20Reference%20notes/101%20Literature/Improving%20Convergence%20and%20Generalization%20Using%20Parameter%20Symmetries/","title":"Improving Convergence and Generalization Using Parameter Symmetries","text":"Properties authors Bo Zhao, Robert M Gower, Robin Walters, Rose Yu year 2023 url https://arxiv.org/abs/2305.13404 <p>Abstract</p> <p>In overparametrized models, different values of the parameters may result in the same loss value. Parameter space symmetries are transformations that change the model parameters but leave the loss invariant. Teleportation applies such transformations to accelerate optimization. However, the exact mechanism behind this algorithm's success is not well understood. In this paper, we show that teleportation not only speeds up optimization in the short-term, but gives overall faster time to convergence. Additionally, we show that teleporting to minima with different curvatures improves generalization and provide insights on the connection between the curvature of the minima and generalization ability. Finally, we show that integrating teleportation into a wide range of optimization algorithms and optimization-based meta-learning improves convergence.</p>","tags":["equivariance","relaxed_equivariance","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/In%20Search%20of%20Projectively%20Equivariant%20Networks/","title":"In Search of Projectively Equivariant Networks","text":"Properties authors Georg Bokman, Axel Flinth, Fredrik Kahl year 2022 url https://arxiv.org/abs/2209.14719 <p>Abstract</p> <p>Equivariance of linear neural network layers is well studied. In this work, we relax the equivariance condition to only be true in a projective sense. We propose a way to construct a projectively equivariant neural network through building a standard equivariant network where the linear group representations acting on each intermediate feature space are\"multiplicatively modified lifts\"of projective group representations. By theoretically studying the relation of projectively and linearly equivariant linear layers, we show that our approach is the most general possible when building a network out of linear layers. The theory is showcased in two simple experiments.</p>","tags":["equivariance","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Knowledge%20Transfer%20from%20Vision%20Foundation%20Models%20for%20Efficient%20Training%20of%20Small%20Task-specific%20Models/","title":"Knowledge Transfer from Vision Foundation Models for Efficient Training of Small Task specific Models","text":"Properties authors Raviteja Vemulapalli, Hadi Pouransari, Fartash Faghri, Sachin Mehta, Mehrdad Farajtabar, Mohammad Rastegari, Oncel Tuzel year 2023 url https://arxiv.org/abs/2311.18237 <p>Abstract</p> <p>Vision Foundation Models (VFMs) pretrained on massive datasets exhibit impressive performance on various downstream tasks, especially with limited labeled target data. However, due to their high inference compute cost, these models cannot be deployed for many real-world applications. Motivated by this, we ask the following important question, \"How can we leverage the knowledge from a large VFM to train a small task-specific model for a new target task with limited labeled training data?\", and propose a simple task-oriented knowledge transfer approach as a highly effective solution to this problem. Our experimental results on five target tasks show that the proposed approach outperforms task-agnostic VFM distillation, web-scale CLIP pretraining, supervised ImageNet pretraining, and self-supervised DINO pretraining by up to 11.6%, 22.1%, 13.7%, and 29.8%, respectively. Furthermore, the proposed approach also demonstrates up to 9x, 4x and 15x reduction in pretraining compute cost when compared to task-agnostic VFM distillation, ImageNet pretraining and DINO pretraining, respectively, while outperforming them. We also show that the dataset used for transferring knowledge has a significant effect on the final target task performance, and introduce a retrieval-augmented knowledge transfer strategy that uses web-scale image retrieval to curate effective transfer sets.</p>","tags":["efficient_dl","paper","foundation_models"]},{"location":"100%20Reference%20notes/101%20Literature/LRP-QViT%20-%20Mixed-Precision%20Vision%20Transformer%20Quantization%20via%20Layer-wise%20Relevance%20Propagation/","title":"LRP QViT   Mixed Precision Vision Transformer Quantization via Layer wise Relevance Propagation","text":"Properties authors Navin Ranjan, Andreas Savakis year 2024 url https://arxiv.org/abs/2401.11243 <p>Abstract</p> <p>Vision transformers (ViTs) have demonstrated remarkable performance across various visual tasks. However, ViT models suffer from substantial computational and memory requirements, making it challenging to deploy them on resource-constrained platforms. Quantization is a popular approach for reducing model size, but most studies mainly focus on equal bit-width quantization for the entire network, resulting in sub-optimal solutions. While there are few works on mixed precision quantization (MPQ) for ViTs, they typically rely on search space-based methods or employ mixed precision arbitrarily. In this paper, we introduce LRP-QViT, an explainability-based method for assigning mixed-precision bit allocations to different layers based on their importance during classification. Specifically, to measure the contribution score of each layer in predicting the target class, we employ the Layer-wise Relevance Propagation (LRP) method. LRP assigns local relevance at the output layer and propagates it through all layers, distributing the relevance until it reaches the input layers. These relevance scores serve as indicators for computing the layer contribution score. Additionally, we have introduced a clipped channel-wise quantization aimed at eliminating outliers from post-LayerNorm activations to alleviate severe inter-channel variations. To validate and assess our approach, we employ LRP-QViT across ViT, DeiT, and Swin transformer models on various datasets. Our experimental findings demonstrate that both our fixed-bit and mixed-bit post-training quantization methods surpass existing models in the context of 4-bit and 6-bit quantization.</p>","tags":["paper","vit","computer_vision","peft","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/Learned%20Gridification%20for%20Efficient%20Point%20Cloud%20Processing/","title":"Learned Gridification for Efficient Point Cloud Processing","text":"Properties authors Putri A. van der Linden, Erik J. Bekkers, David W. Romero <p>Abstract</p>","tags":["dl2"]},{"location":"100%20Reference%20notes/101%20Literature/Learning%20Partial%20Equivariances%20from%20Data/","title":"Learning Partial Equivariances from Data","text":"Properties authors David W. Romero, Suhas Lohit year 2021 url https://arxiv.org/abs/2110.10211 <p>Monte Carlo Approximation of Group Convolutions</p> <p>We can approximate Group Convolutions on the expectation by uniformly sampling group actions \\(v_j\\). $$  (\\psi \\hat{*} f)(u_i) = \\sum_j \\psi (v_j^{-1} u_i)f(v_j) \\bar{\\mu}_{\\mathcal{G}} (v_j) $$</p> <p>Main idea</p> <ol> <li>Prioritize sampling of specific group elements during the group convolution by learning a probability distribution over them.</li> <li>1D continuous groups: use reparametrization trick on the Lie algebra of the group, which is uniform over a connected set of group elements but zero otherwise. \\(\\to\\) Partial Equivariance</li> <li>1D discrete groups: Bernoulli Distribution over all possible element combinations</li> </ol> <p>Citations: - Self-Supervised Detection of Perfect and Partial Input-Dependent Symmetries - Color Equivariant Convolutional Networks - Equivariance-aware architectural optimization of neural networks - Approximation-Generalization Trade-offs under (Approximate) Group Equivariance</p>","tags":["dl2","equivariance","partial_equivariance"]},{"location":"100%20Reference%20notes/101%20Literature/Learning%20both%20Weights%20and%20Connections%20for%20Efficient%20Neural%20Networks/","title":"Learning both Weights and Connections for Efficient Neural Networks","text":"Properties authors Song Han, Jeff Pool, John Tran, William J. Dally year 2015 url https://arxiv.org/abs/1506.02626 <p>Abstract</p> <p>Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.</p>","tags":["paper","efficient_dl","pruning","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Learning%20with%20Unmasked%20Tokens%20Drives%20Stronger%20Vision%20Learners/","title":"Learning with Unmasked Tokens Drives Stronger Vision Learners","text":"Properties authors Taekyung Kim, Sanghyuk Chun, Byeongho Heo, Dongyoon Han year 2024 url https://arxiv.org/abs/2310.13593 <p>Abstract</p> <p>Masked image modeling (MIM) has become a leading self-supervised learning strategy. MIMs such as Masked Autoencoder (MAE) learn strong representations by randomly masking input tokens for the encoder to process, with the decoder reconstructing the masked tokens to the input. However, MIM pre-trained encoders often exhibit a limited attention span, attributed to MIM's sole focus on regressing masked tokens only, which may impede the encoder's broader context learning. To tackle the limitation, we improve MIM by explicitly incorporating unmasked tokens into the training process. Specifically, our method enables the encoder to learn from broader context supervision, allowing unmasked tokens to experience broader contexts while the decoder reconstructs masked tokens. Thus, the encoded unmasked tokens are equipped with extensive contextual information, empowering masked tokens to leverage the enhanced unmasked tokens for MIM. As a result, our simple remedy trains more discriminative representations revealed by achieving 84.2% top-1 accuracy with ViT-B on ImageNet-1K with 0.6%p gain. We attribute the success to the enhanced pre-training method, as evidenced by the singular value spectrum and attention analyses. Finally, our models achieve significant performance gains at the downstream semantic segmentation and fine-grained visual classification tasks; and on diverse robust evaluation metrics. Code is available at\u00a0this https URL</p>","tags":["paper","foundation_models","computer_vision","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/Learning%20with%20Unmasked%20Tokens%20Drives%20Stronger%20Vision%20Learners/#notes","title":"Notes","text":"<p>Some notes regarding MIM as a good objective are on Masked Image Modelling.</p> <p>However, MIM strategies often encounter challenges, such as local dependency on attention to understand entire context of an image. For example, liu\u00a0et al.\u00a0[36]\u00a0revealed that MAE\u00a0[22], a state-of-the-art MIM method, exhibits shorter average attention distances. Furthermore, we observe that attention map patterns by MAE substantiate extremely local behavior (See Fig.\u00a01) indeed. In other words, the MAE-trained attention mechanism less integrates information across the entire image pixels and tends to focus on specific input regions. This is presumably attributed to MIM-pretraining, primarily dedicated to predicting low-level pixel details (e.g., color or texture) without a comprehensive understanding of less-regional information (e.g., the input structure or shape).</p> <p>This maybe should not really be an issue: How do vision transformers work? explicitly constraint ViTs to only use local attention and they improve performance. So maybe this is an advantage? See Are less inductive biases better or worse?.</p> <p></p>","tags":["paper","foundation_models","computer_vision","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/Llama%202%20-%20Open%20Foundation%20and%20Fine-Tuned%20Chat%20Models/","title":"Llama 2   Open Foundation and Fine Tuned Chat Models","text":"Properties authors Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom year 2023 url https://arxiv.org/abs/2307.09288 <p>Abstract</p> <p>In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.</p>","tags":["paper","foundation_models","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/LoRA%20-%20Low-Rank%20Adaptation%20of%20Large%20Language%20Models/","title":"LoRA   Low Rank Adaptation of Large Language Models","text":"Properties authors Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen year 2021 url https://arxiv.org/abs/2106.09685 <p>Abstract</p> <p>An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at\u00a0this https URL.</p>","tags":["paper","efficient_dl","peft"]},{"location":"100%20Reference%20notes/101%20Literature/Mamba%20-%20Linear-Time%20Sequence%20Modeling%20with%20Selective%20State%20Spaces/","title":"Mamba   Linear Time Sequence Modeling with Selective State Spaces","text":"Properties authors Albert Gu, Tri Dao year 2023 <p>Abstract</p> <p>Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\u00d7\u00a0higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.</p>","tags":["foundation_models","convolutions"]},{"location":"100%20Reference%20notes/101%20Literature/Memorization%20Through%20the%20Lens%20of%20Curvature%20of%20Loss%20Function%20Around%20Samples/","title":"Memorization Through the Lens of Curvature of Loss Function Around Samples","text":"Properties authors Isha Garg, Deepak Ravikumar, Kaushik Roy year 2024 url https://openreview.net/forum?id=WQbDS9RydY <p>Abstract</p> <p>Deep neural networks are over-parameterized and easily overfit to and memorize the datasets that they train on. In the extreme case, it has been shown that networks can memorize a randomly labeled dataset. In this paper, we propose using the curvature of the loss function around each training sample, averaged over training epochs, as a measure of memorization of a sample. We show that this curvature metric effectively captures memorization statistics, both qualitatively and quantitatively in popular image datasets. We provide quantitative validation of the proposed metric against memorization scores released by Feldman &amp; Zhang (2020). Further, experiments on mislabeled data detection show that corrupted samples are learned with high curvature and using curvature for identifying mislabelled examples outperforms existing approaches. Qualitatively, we find that high curvature samples correspond to long-tailed, mislabeled, or conflicting instances, indicating a likelihood of memorization. Notably, this analysis helps us find, to the best of our knowledge, a novel failure mode on the CIFAR100 and ImageNet datasets: that of duplicated images with differing labels.</p>","tags":["paper","dl_theory","llm"]},{"location":"100%20Reference%20notes/101%20Literature/Mixture%20of%20LoRa%20Experts/","title":"Mixture of LoRa Experts","text":"Properties authors Xun Wu, Shaohan Huang, Furu Wei year 2024 url https://arxiv.org/abs/2404.13628 <p>Abstract</p> <p>LoRA has gained widespread acceptance in the fine-tuning of large pre-trained models to cater to a diverse array of downstream tasks, showcasing notable effectiveness and efficiency, thereby solidifying its position as one of the most prevalent fine-tuning techniques. Due to the modular nature of LoRA's plug-and-play plugins, researchers have delved into the amalgamation of multiple LoRAs to empower models to excel across various downstream tasks. Nonetheless, extant approaches for LoRA fusion grapple with inherent challenges. Direct arithmetic merging may result in the loss of the original pre-trained model's generative capabilities or the distinct identity of LoRAs, thereby yielding suboptimal outcomes. On the other hand, Reference tuning-based fusion exhibits limitations concerning the requisite flexibility for the effective combination of multiple LoRAs. In response to these challenges, this paper introduces the Mixture of LoRA Experts (MoLE) approach, which harnesses hierarchical control and unfettered branch selection. The MoLE approach not only achieves superior LoRA fusion performance in comparison to direct arithmetic merging but also retains the crucial flexibility for combining LoRAs effectively. Extensive experimental evaluations conducted in both the Natural Language Processing (NLP) and Vision &amp; Language (V&amp;L) domains substantiate the efficacy of MoLE.</p>","tags":["paper","peft","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/MobileCLIP%20-%20Fast%20Image-Text%20Models%20through%20Multi-Modal%20Reinforced%20Training/","title":"MobileCLIP   Fast Image Text Models through Multi Modal Reinforced Training","text":"Properties authors Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, Oncel Tuzel year 2023 url https://arxiv.org/abs/2311.17049 <p>Abstract</p> <p>Contrastive pretraining of image-text foundation models, such as CLIP, demonstrated excellent zero-shot performance and improved robustness on a wide range of downstream tasks. However, these models utilize large transformer-based encoders with significant memory and latency overhead which pose challenges for deployment on mobile devices. In this work, we introduce MobileCLIP -- a new family of efficient image-text models optimized for runtime performance along with a novel and efficient training approach, namely multi-modal reinforced training. The proposed training approach leverages knowledge transfer from an image captioning model and an ensemble of strong CLIP encoders to improve the accuracy of efficient models. Our approach avoids train-time compute overhead by storing the additional knowledge in a reinforced dataset. MobileCLIP sets a new state-of-the-art latency-accuracy tradeoff for zero-shot classification and retrieval tasks on several datasets. Our MobileCLIP-S2 variant is 2.3\u00d7\u00a0faster while more accurate compared to previous best CLIP model based on ViT-B/16. We further demonstrate the effectiveness of our multi-modal reinforced training by training a CLIP model based on ViT-B/16 image backbone and achieving +2.9% average performance improvement on 38 evaluation benchmarks compared to the previous best. Moreover, we show that the proposed approach achieves 10\u00d7-1000\u00d7\u00a0improved learning efficiency when compared with non-reinforced CLIP training. Code and models are available at\u00a0this https URL\u00a0.</p>","tags":["paper","efficient_dl","efficient_vision","computer_vision","multimodal"]},{"location":"100%20Reference%20notes/101%20Literature/MobileViT%20-%20light-weight%2C%20general-purpose%2C%20and%20mobile-friendly%20vision%20transformer/","title":"MobileViT   light weight, general purpose, and mobile friendly vision transformer","text":"Properties authors Sachin Mehta, Mohammad Rastegari year 2022 url https://arxiv.org/abs/2110.02178 <p>Abstract</p> <p>Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of parameters.  </p> <p>Our source code is open-source and available at:\u00a0this https URL</p>","tags":["paper","efficient_dl","efficient_vision"]},{"location":"100%20Reference%20notes/101%20Literature/Model%20Compression%20in%20Practice%20-%20Lessons%20Learned%20from%20Practitioners%20Creating%20On-device%20Machine%20Learning%20Experiences/","title":"Model Compression in Practice   Lessons Learned from Practitioners Creating On device Machine Learning Experiences","text":"Properties authors Fred Hohman, Mary Beth Kery, Donghao Ren, Dominik Moritz year 2024 url https://arxiv.org/abs/2310.04621 <p>Abstract</p> <p>On-device machine learning (ML) promises to improve the privacy, responsiveness, and proliferation of new, intelligent user experiences by moving ML computation onto everyday personal devices. However, today's large ML models must be drastically compressed to run efficiently on-device, a hurtle that requires deep, yet currently niche expertise. To engage the broader human-centered ML community in on-device ML experiences, we present the results from an interview study with 30 experts at Apple that specialize in producing efficient models. We compile tacit knowledge that experts have developed through practical experience with model compression across different hardware platforms. Our findings offer pragmatic considerations missing from prior work, covering the design process, trade-offs, and technical strategies that go into creating efficient models. Finally, we distill design recommendations for tooling to help ease the difficulty of this work and bring on-device ML into to more widespread practice.</p>","tags":["paper","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/Model%20Compression%20in%20Practice%20-%20Lessons%20Learned%20from%20Practitioners%20Creating%20On-device%20Machine%20Learning%20Experiences/#notes","title":"Notes","text":"<p>Specific techniques on models weights help reduce size, but to get an efficient model comes from more careful design of the loss function, the system, which parts should and should not be modeled with ML. - [ ] How does the design of a loss function affect a model's efficiency? Note to myself to look into this in the future.</p> <p></p> <p>Although posttraining quantization is considered \u201ceasy\u201d [E9] as far as ML compression techniques go, practitioners emphasized that it still often takes complex code to implement and there are many algorithm variations A survey of quantization methods for efficient neural network inference to experiment with [T5]. For models that need high accuracy, post-training quantization may not be enough to hit budget without unacceptable accuracy degradation [E9, E4, E13, E5].</p> <ul> <li>Okay, so it's important to try a bunch of quantization techniques. Got it.</li> </ul> <p>\u201cIf you want to go to lower bit quantization, such as 4 or below, it\u2019s almost impossible to use post-training quantization because the difference in accuracy gets way too big. So for this level of compression you need to do training-aware compression.\u201d \u2014 E9</p> <ul> <li>Cool, I didn't know that training aware compression was such an important thing to consider, from an industry perspective, not just research.</li> </ul> <p>Although training-aware compression is considered the best form of optimization A survey of quantization methods for efficient neural network inference, a major drawback is that is must be included in initial model training: \u201cNot starting early with compression is a dead end,\u201d [E3].</p> <ul> <li> Why is that though? Why should compression-aware training happen from the start and not in the middle of training or even in finetuning? #rq</li> </ul> <p>[...] practitioners suggest estimating how much compression will be feasible with simple post-training quantization. To estimate quantization savings before training a model, first initialize the ML model architecture with random weights, then quantize, and test the model\u2019s speed and size on-device</p> <p>Strategy #6: Compression can degrade the accuracy of a model and change its behavior in unpredictable ways. It is essential to create a robust evaluation pipeline (e.g., defining metrics, curating test sets) before you start optimizing your model, so that you can reliably observe shifts in model error afterwards. To prevent degradation from a failed optimization, compare optimized models with varying amounts of compression to your original model, inspecting the metrics, subpopulation behaviors, and internals, such as weights and activations, to ensure they are within expected ranges.</p> <p>Okay, Robust evaluation pipeline is fundamental: Need to create unit tests, and for quantization specifically check that the distributions of weights (obviously) and activations (less obviously) are within the expected ranges. The latter might happen because of compounding degradation, this mean that errors in early layers caused by quantization might compound to later layers in unexpected ways.</p>","tags":["paper","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/Neural%20Mechanics%20-%20Symmetry%20and%20Broken%20Conservation%20Laws%20in%20Deep%20Learning%20Dynamics/","title":"Neural Mechanics   Symmetry and Broken Conservation Laws in Deep Learning Dynamics","text":"Properties authors Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel L.K. Yamins, Hidenori Tanaka year 2020 url https://arxiv.org/abs/2012.04728 <p>Abstract</p> <p>Understanding the dynamics of neural network parameters during training is one of the key challenges in building a theoretical foundation for deep learning. A central obstacle is that the motion of a network in high-dimensional parameter space undergoes discrete finite steps along complex stochastic gradients derived from real-world datasets. We circumvent this obstacle through a unifying theoretical framework based on intrinsic symmetries embedded in a network's architecture that are present for any dataset. We show that any such symmetry imposes stringent geometric constraints on gradients and Hessians, leading to an associated conservation law in the continuous-time limit of stochastic gradient descent (SGD), akin to Noether's theorem in physics. We further show that finite learning rates used in practice can actually break these symmetry induced conservation laws. We apply tools from finite difference methods to derive modified gradient flow, a differential equation that better approximates the numerical trajectory taken by SGD at finite learning rates. We combine modified gradient flow with our framework of symmetries to derive exact integral expressions for the dynamics of certain parameter combinations. We empirically validate our analytic expressions for learning dynamics on VGG-16 trained on Tiny ImageNet. Overall, by exploiting symmetry, our work demonstrates that we can analytically describe the learning dynamics of various parameter combinations at finite learning rates and batch sizes for state of the art architectures trained on any dataset.</p>","tags":["dl2","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/On%20Good%20Practices%20for%20Task-Specific%20Distillation%20of%20Large%20Pretrained%20Visual%20Models/","title":"On Good Practices for Task Specific Distillation of Large Pretrained Visual Models","text":"Properties authors Juliette Marrie, Michael Arbel, Julien Mairal, Diane Larlus year 2024 url https://arxiv.org/abs/2402.11305 <p>Abstract</p> <p>Large pretrained visual models exhibit remarkable generalization across diverse recognition tasks. Yet, real-world applications often demand compact models tailored to specific problems. Variants of knowledge distillation have been devised for such a purpose, enabling task-specific compact models (the students) to learn from a generic large pretrained one (the teacher). In this paper, we show that the excellent robustness and versatility of recent pretrained models challenge common practices established in the literature, calling for a new set of optimal guidelines for task-specific distillation. To address the lack of samples in downstream tasks, we also show that a variant of Mixup based on stable diffusion complements standard data augmentation. This strategy eliminates the need for engineered text prompts and improves distillation of generic models into streamlined specialized networks.</p>","tags":["paper","distillation","foundation_models","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/On%20Good%20Practices%20for%20Task-Specific%20Distillation%20of%20Large%20Pretrained%20Visual%20Models/#notes","title":"Notes","text":"","tags":["paper","distillation","foundation_models","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/On%20the%20Relationship%20between%20Self-Attention%20and%20Convolutional%20Layers/","title":"On the Relationship between Self Attention and Convolutional Layers","text":"Properties authors Jean-Baptiste Cordonnier, Andreas Loukas, Martin Jaggi year 2020 url https://arxiv.org/abs/1911.03584 <p>Abstract</p> <p>Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Stand-Alone Self-Attention in Vision Models showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that self-attention layers attend to pixel-grid patterns similarly to CNN layers, corroborating our analysis. </p>","tags":["transformers","convolutions","theory"]},{"location":"100%20Reference%20notes/101%20Literature/On%20the%20Relationship%20between%20Self-Attention%20and%20Convolutional%20Layers/#notes","title":"Notes","text":"<ul> <li> Note to self: fully read article, it looks fun \u23eb #personal </li> </ul>","tags":["transformers","convolutions","theory"]},{"location":"100%20Reference%20notes/101%20Literature/On%20the%20Symmetries%20of%20Deep%20Learning%20Models%20and%20their%20Internal%20Representations/","title":"On the Symmetries of Deep Learning Models and their Internal Representations","text":"Properties authors Charles Godfrey, Davis Brown, Tegan Emerson, Henry Kvnige year 2022 url https://arxiv.org/abs/2205.14258 <p>Abstract</p> <p>Symmetry is a fundamental tool in the exploration of a broad range of complex systems. In machine learning symmetry has been explored in both models and data. In this paper we seek to connect the symmetries arising from the architecture of a family of models with the symmetries of that family's internal representation of data. We do this by calculating a set of fundamental symmetry groups, which we call the intertwiner groups of the model. We connect intertwiner groups to a model's internal representations of data through a range of experiments that probe similarities between hidden states across models with the same architecture. Our work suggests that the symmetries of a network are propagated into the symmetries in that network's representation of data, providing us with a better understanding of how architecture affects the learning and prediction process. Finally, we speculate that for ReLU networks, the intertwiner groups may provide a justification for the common practice of concentrating model interpretability exploration on the activation basis in hidden layers rather than arbitrary linear combinations thereof.</p> <p>Notes: - The following papers study the effect of weight space symmetries on training dynamics:     - Neural Mechanics - Symmetry and Broken Conservation Laws in Deep Learning Dynamics     - Understanding symmetries in deep networks     - G-SGD - Optimizing ReLU Neural Networks in its Positively Scale-Invariant Space      - Deep Learning Book</p>","tags":["dl_theory","dl2"]},{"location":"100%20Reference%20notes/101%20Literature/OpenELM%20-%20An%20Efficient%20Language%20Model%20Family%20with%20Open-source%20Training%20and%20Inference%20Framework/","title":"OpenELM   An Efficient Language Model Family with Open source Training and Inference Framework","text":"Properties authors Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Dmitry Belenko, Peter Zatloukal, Mohammad Rastegari year 2024 url https://arxiv.org/abs/2404.14619 <p>Abstract</p> <p>The reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks. To this end, we release OpenELM, a state-of-the-art open language model. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy. For example, with a parameter budget of approximately one billion parameters, OpenELM exhibits a 2.36% improvement in accuracy compared to OLMo while requiring\u00a02\u00d7\u00a0fewer pre-training tokens. Diverging from prior practices that only provide model weights and inference code, and pre-train on private datasets, our release includes the complete framework for training and evaluation of the language model on publicly available datasets, including training logs, multiple checkpoints, and pre-training configurations. We also release code to convert models to MLX library for inference and fine-tuning on Apple devices. This comprehensive release aims to empower and strengthen the open research community, paving the way for future open research endeavors. Our source code along with pre-trained model weights and training recipes is available at \\url{this https URL}. Additionally, \\model models can be found on HuggingFace at: \\url{this https URL}.</p>","tags":["llm","foundation_models"]},{"location":"100%20Reference%20notes/101%20Literature/Optimal%20Brain%20Damage/","title":"Optimal Brain Damage","text":"Properties authors John Denker, Sara Solla, Yann LeCun year 1989 url https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html <p>Abstract</p> <p>We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.</p> <p>OBD Pruning Algorithm</p> <p>Use saliency measure based on Hessian (loss wrt parameters) to pick which parameters to prune. Finetune afterwards.</p>","tags":["paper","efficient_vision","efficient_dl","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Optimization%20Dynamics%20of%20Equivariant%20and%20Augmented%20Neural%20Networks/","title":"Optimization Dynamics of Equivariant and Augmented Neural Networks","text":"Properties authors Alex Flinth, Fredrik Ohlsson year 2023 url https://arxiv.org/abs/2303.13458 <p>Abstract</p> <p>We investigate the optimization of multilayer perceptrons on symmetric data. We compare the strategy of constraining the architecture to be equivariant to that of using augmentation. We show that, under natural assumptions on the loss and non-linearities, the sets of equivariant stationary points are identical for the two strategies, and that the set of equivariant layers is invariant under the gradient flow for augmented models. Finally, we show that stationary points may be unstable for augmented training although they are stable for the equivariant models.</p> <p>Main observations: 1. They show that if the augmented model is equivariantly initialized, it will remain equivariant during training (See Equivariance Initialization) 3. Compared to the equivariant approach, augmentation introduces no new equivariant stationary points, nor does it exclude existing ones. (See Multiple global minima) 4. The existence of a stable equivariant minimum is not guaranteed by augmentation. (See Multiple global minima)</p> <p>Regarding Equivariance Initialization in this work:</p> <p>We initialize \u03a6A with equivariant layers A0 \u2208 E by drawing matrices randomly from a standard Gaussian distribution, and then projecting them orthogonally onto E. We train the network on (finite) datasets D using gradient descent in three different ways. </p> <p>My intuition is that they do something like the isotropic convolution from Priors over Neural Network weights</p>","tags":["dl_theory","equivariance","optimization"]},{"location":"100%20Reference%20notes/101%20Literature/Parameter%20Efficient%20Fine-tuning%20of%20Self-supervised%20ViTs%20without%20Catastrophic%20Forgetting/","title":"Parameter Efficient Fine tuning of Self supervised ViTs without Catastrophic Forgetting","text":"Properties authors Reza Akbarian Bafghi, Nidhin Harilal, Claire Monteleoni, Maziar Raissi year 2024 url https://arxiv.org/abs/2404.17245 <p>Abstract</p> <p>Artificial neural networks often suffer from catastrophic forgetting, where learning new concepts leads to a complete loss of previously acquired knowledge. We observe that this issue is particularly magnified in vision transformers (ViTs), where post-pre-training and fine-tuning on new tasks can significantly degrade the model's original general abilities. For instance, a DINO ViT-Base/16 pre-trained on ImageNet-1k loses over 70% accuracy on ImageNet-1k after just 10 iterations of fine-tuning on CIFAR-100. Overcoming this stability-plasticity dilemma is crucial for enabling ViTs to continuously learn and adapt to new domains while preserving their initial knowledge. In this work, we study two new parameter-efficient fine-tuning strategies: (1)~Block Expansion, and (2) Low-rank adaptation (LoRA). Our experiments reveal that using either Block Expansion or LoRA on self-supervised pre-trained ViTs surpass fully fine-tuned ViTs in new domains while offering significantly greater parameter efficiency. Notably, we find that Block Expansion experiences only a minimal performance drop in the pre-training domain, thereby effectively mitigating catastrophic forgetting in pre-trained ViTs.</p>","tags":["paper"]},{"location":"100%20Reference%20notes/101%20Literature/Parameter%20Efficient%20Fine-tuning%20of%20Self-supervised%20ViTs%20without%20Catastrophic%20Forgetting/#paper-results","title":"Paper Results","text":"Model N. params CIFAR-100 IN-1K Mean Standard Fine-tuning All 85.9 M 88.13 25.24 56.69 Top-3 21.3 M 84.56 74.15 79.36 Linear 76.9 K 80.57 76.11 78.34 LoRA \ud835\udc5f=4 301 K 87.91 66.82 77.37 \ud835\udc5f=8 448 K 88.27 65.99 77.13 \ud835\udc5f=16 743 K 87.84 65.06 76.45 Block Expansion \ud835\udc5d=1 7.2 M 82.72 75.75 79.24 \ud835\udc5d=2 14.3 M 86.70 75.54 81.12 \ud835\udc5d=3 21.3 M 88.58 74.61 81.60 \ud835\udc5d=4 28.4 M 89.09 72.28 80.69","tags":["paper"]},{"location":"100%20Reference%20notes/101%20Literature/Parameter%20Efficient%20Fine-tuning%20of%20Self-supervised%20ViTs%20without%20Catastrophic%20Forgetting/#observations","title":"Observations","text":"<ul> <li>Linear only fine-tuning does pretty well, kinda surprising.</li> <li>It's kind of suprising that LoRa Adapter do bad, but does it matter? What is the purpose of making LoRa resistant to catastrophic forgetting if the whole point of it is to be able to hot-swap modules depending on the task?</li> <li>Also worthy to point out that Block Expansion requires training parameters in the order of millions while LoRa only requires thousands. </li> </ul>","tags":["paper"]},{"location":"100%20Reference%20notes/101%20Literature/Parameter-Efficient%20Fine-Tuning%20for%20Pre-Trained%20Vision%20Models%20-%20A%20Survey/","title":"Parameter Efficient Fine Tuning for Pre Trained Vision Models   A Survey","text":"Properties authors Yi Xin, Siqi Luo, Haodi Zhou, Junlong Du, Xiaohong Liu, Yue Fan, Qing Li, Yuntao Du year 2024 url https://arxiv.org/abs/2402.02242 <p>Abstract</p> <p>Large-scale pre-trained vision models (PVMs) have shown great potential for adaptability across various downstream vision tasks. However, with state-of-the-art PVMs growing to billions or even trillions of parameters, the standard full fine-tuning paradigm is becoming unsustainable due to high computational and storage demands. In response, researchers are exploring parameter-efficient fine-tuning (PEFT), which seeks to exceed the performance of full fine-tuning with minimal parameter modifications. This survey provides a comprehensive overview and future directions for visual PEFT, offering a systematic review of the latest advancements. First, we provide a formal definition of PEFT and discuss model pre-training methods. We then categorize existing methods into three categories: addition-based, partial-based, and unified-based. Finally, we introduce the commonly used datasets and applications and suggest potential future research challenges. A comprehensive collection of resources is available at\u00a0this https URL.</p> <p></p>","tags":["paper","efficient_dl","efficient_vision","transformers","peft"]},{"location":"100%20Reference%20notes/101%20Literature/Progress%20measures%20for%20grokking%20via%20mechanistic%20interpretability/","title":"Progress measures for grokking via mechanistic interpretability","text":"Properties authors Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, Jacob Steinhardt year 2023 url https://arxiv.org/abs/2301.05217 <p>Abstract</p> <p>Neural networks often exhibit emergent behavior, where qualitatively new capabilities arise from scaling up the amount of parameters, training data, or training steps. One approach to understanding emergence is to find continuous \\textit{progress measures} that underlie the seemingly discontinuous qualitative changes. We argue that progress measures can be found via mechanistic interpretability: reverse-engineering learned behaviors into their individual components. As a case study, we investigate the recently-discovered phenomenon of ``grokking'' exhibited by small transformers trained on modular addition tasks. We fully reverse engineer the algorithm learned by these networks, which uses discrete Fourier transforms and trigonometric identities to convert addition to rotation about a circle. We confirm the algorithm by analyzing the activations and weights and by performing ablations in Fourier space. Based on this understanding, we define progress measures that allow us to study the dynamics of training and split training into three continuous phases: memorization, circuit formation, and cleanup. Our results show that grokking, rather than being a sudden shift, arises from the gradual amplification of structured mechanisms encoded in the weights, followed by the later removal of memorizing components.</p> <p>Related - Grokking</p>","tags":["paper","interpretability","mechinterp"]},{"location":"100%20Reference%20notes/101%20Literature/Provably%20Strict%20Generalisation%20Benefit%20for%20Equivariant%20Models/","title":"Provably Strict Generalisation Benefit for Equivariant Models","text":"Properties authors Bryn Elesedy, Sheheryar Zaidi year 2021 url https://arxiv.org/abs/2102.10333 <p>Abstract</p> <p>It is widely believed that engineering a model to be invariant/equivariant improves generalisation. Despite the growing popularity of this approach, a precise characterisation of the generalisation benefit is lacking. By considering the simplest case of linear models, this paper provides the first provably non-zero improvement in generalisation for invariant/equivariant models when the target distribution is invariant/equivariant with respect to a compact group. Moreover, our work reveals an interesting relationship between generalisation, the number of training examples and properties of the group action. Our results rest on an observation of the structure of function spaces under averaging operators which, along with its consequences for feature averaging, may be of independent interest.</p>","tags":["dl_theory","equivariance"]},{"location":"100%20Reference%20notes/101%20Literature/ProxylessNAS%20-%20Direct%20Neural%20Architecture%20Search%20on%20Target%20Task%20and%20Hardware/","title":"ProxylessNAS   Direct Neural Architecture Search on Target Task and Hardware","text":"Properties authors Han Cai, Ligeng Zhu, Song Han year 2019 url https://arxiv.org/abs/1812.00332 <p>Abstract</p> <p>Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g.\u00a0104\u00a0GPU hours) makes it difficult to \\emph{directly} search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize~\\emph{proxy} tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present \\emph{ProxylessNAS} that can \\emph{directly} learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08\\% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7\u00a0fewer parameters. On ImageNet, our model achieves 3.1\\% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7\u00a0faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.</p>","tags":["paper","efficient_dl","nas"]},{"location":"100%20Reference%20notes/101%20Literature/ProxylessNAS%20-%20Direct%20Neural%20Architecture%20Search%20on%20Target%20Task%20and%20Hardware/#notes","title":"Notes","text":"<ul> <li>To avoid measuring performance on the target device, they learn a latency model.<ol> <li>They take multiple measurements of a device with different architectures.</li> <li>They train a model to predict the latency given the architecture.</li> </ol> </li> </ul>","tags":["paper","efficient_dl","nas"]},{"location":"100%20Reference%20notes/101%20Literature/R-MAE%20-%20Regions%20Meet%20Masked%20Autoencoders/","title":"R MAE   Regions Meet Masked Autoencoders","text":"Properties authors Duy-Kien Nguyen, Vaibhav Aggarwal, Yanghao Li, Martin R. Oswald, Alexander Kirillov, Cees G. M. Snoek, Xinlei Chen year 2023 url https://arxiv.org/abs/2306.05411 <p>Abstract</p> <p>In this work, we explore regions as a potential visual analogue of words for self-supervised image representation learning. Inspired by Masked Autoencoding (MAE), a generative pre-training baseline, we propose masked region autoencoding to learn from groups of pixels or regions. Specifically, we design an architecture which efficiently addresses the one-to-many mapping between images and regions, while being highly effective especially with high-quality regions. When integrated with MAE, our approach (R-MAE) demonstrates consistent improvements across various pre-training datasets and downstream detection and segmentation benchmarks, with negligible computational overheads. Beyond the quantitative evaluation, our analysis indicates the models pre-trained with masked region autoencoding unlock the potential for interactive segmentation. The code is provided at\u00a0this https URL.</p>","tags":["paper","foundation_models"]},{"location":"100%20Reference%20notes/101%20Literature/R-MAE%20-%20Regions%20Meet%20Masked%20Autoencoders/#note","title":"Note","text":"<ul> <li> Note to self: Read in depth</li> </ul>","tags":["paper","foundation_models"]},{"location":"100%20Reference%20notes/101%20Literature/Refusal%20in%20Language%20Models%20Is%20Mediated%20by%20a%20Single%20Direction/","title":"Refusal in Language Models Is Mediated by a Single Direction","text":"Properties authors Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Rimsky, Wes Gurnee, Neel Nanda year 2024 url https://arxiv.org/abs/2406.11717 <p>Abstract</p> <p>Conversational large language models are fine-tuned for both instruction-following and safety, resulting in models that obey benign requests but refuse harmful ones. While this refusal behavior is widespread across chat models, its underlying mechanisms remain poorly understood. In this work, we show that refusal is mediated by a one-dimensional subspace, across 13 popular open-source chat models up to 72B parameters in size. Specifically, for each model, we find a single direction such that erasing this direction from the model's residual stream activations prevents it from refusing harmful instructions, while adding this direction elicits refusal on even harmless instructions. Leveraging this insight, we propose a novel white-box jailbreak method that surgically disables refusal with minimal effect on other capabilities. Finally, we mechanistically analyze how adversarial suffixes suppress propagation of the refusal-mediating direction. Our findings underscore the brittleness of current safety fine-tuning methods. More broadly, our work showcases how an understanding of model internals can be leveraged to develop practical methods for controlling model behavior.</p>","tags":["paper","transformers","mechinterp","interpretability"]},{"location":"100%20Reference%20notes/101%20Literature/Relaxed%20Octahedral%20Group%20Convolution%20for%20Learning%20Symmetry%20Breaking%20in%203D%20Physical%20Systems/","title":"Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in 3D Physical Systems","text":"Properties authors Rui Wang, Robin Walters, Tess E Smidt year 2023 url https://arxiv.org/abs/2310.02299 <p>Abstract</p> <p>Deep equivariant models use symmetries to improve sample efficiency and generalization. However, the assumption of perfect symmetry in many of these models can sometimes be restrictive, especially when the data does not perfectly align with such symmetries. Thus, we introduce relaxed octahedral group convolution for modeling 3D physical systems in this paper. This flexible convolution technique provably allows the model to both maintain the highest level of equivariance that is consistent with data and discover the subtle symmetry-breaking factors in the physical systems. Empirical results validate that our approach can not only provide insights into the symmetry-breaking factors in phase transitions but also achieves superior performance in fluid super-resolution tasks.</p>","tags":["relaxed_equivariance","equivariance"]},{"location":"100%20Reference%20notes/101%20Literature/Relaxing%20Equivariance%20Constraints%20with%20Non-stationary%20Continuous%20Filters/","title":"Relaxing Equivariance Constraints with Non stationary Continuous Filters","text":"Properties authors David W. Romero <p>Abstract</p>","tags":["dl2","equivariance","partial_equivariance"]},{"location":"100%20Reference%20notes/101%20Literature/Retrospective%20-%20EIE%20-%20Efficient%20Inference%20Engine%20onSparse%20and%20Compressed%20Neural%20Network/","title":"Retrospective   EIE   Efficient Inference Engine onSparse and Compressed Neural Network","text":"Properties authors Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A. Horowitz, William J. Dally year 2023 url https://arxiv.org/abs/2306.09552 <p>Abstract</p> <p>EIE proposed to accelerate pruned and compressed neural networks, exploiting weight sparsity, activation sparsity, and 4-bit weight-sharing in neural network accelerators. Since published in ISCA'16, it opened a new design space to accelerate pruned and sparse neural networks and spawned many algorithm-hardware co-designs for model compression and acceleration, both in academia and commercial AI chips. In retrospect, we review the background of this project, summarize the pros and cons, and discuss new opportunities where pruning, sparsity, and low precision can accelerate emerging deep learning workloads.</p>","tags":["paper","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/Revealing%20the%20Utilized%20Rank%20of%20Subspaces%20of%20Learning%20in%20Neural%20Networks/","title":"Revealing the Utilized Rank of Subspaces of Learning in Neural Networks","text":"Properties authors Isha Garg, Christian Koguchi, Eshan Verma, Daniel Ulbricht year 2024 url https://arxiv.org/abs/2407.04797 <p>Abstract</p> <p>In this work, we study how well the learned weights of a neural network utilize the space available to them. This notion is related to capacity, but additionally incorporates the interaction of the network architecture with the dataset. Most learned weights appear to be full rank, and are therefore not amenable to low rank decomposition. This deceptively implies that the weights are utilizing the entire space available to them. We propose a simple data-driven transformation that projects the weights onto the subspace where the data and the weight interact. This preserves the functional mapping of the layer and reveals its low rank structure. In our findings, we conclude that most models utilize a fraction of the available space. For instance, for ViTB-16 and ViTL-16 trained on ImageNet, the mean layer utilization is 35% and 20% respectively. Our transformation results in reducing the parameters to 50% and 25% respectively, while resulting in less than 0.2% accuracy drop after fine-tuning. We also show that self-supervised pre-training drives this utilization up to 70%, justifying its suitability for downstream tasks.</p>","tags":["paper","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Rewrite%20the%20Stars/","title":"Rewrite the Stars","text":"Properties authors Xu Ma, Xiyang Dai, Yue Bai, Yizhou Wang, Yun Fu year 2024 url https://arxiv.org/abs/2403.19967 <p>Abstract</p> <p>Recent studies have drawn attention to the untapped potential of the \"star operation\" (element-wise multiplication) in network design. While intuitive explanations abound, the foundational rationale behind its application remains largely unexplored. Our study attempts to reveal the star operation's ability to map inputs into high-dimensional, non-linear feature spaces -- akin to kernel tricks -- without widening the network. We further introduce StarNet, a simple yet powerful prototype, demonstrating impressive performance and low latency under compact network structure and efficient budget. Like stars in the sky, the star operation appears unremarkable but holds a vast universe of potential. Our work encourages further exploration across tasks, with codes available at\u00a0this https URL.</p>","tags":["dl_theory","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/SAM-CLIP%20-%20Merging%20Vision%20Foundation%20Models%20towards%20Semantic%20and%20Spatial%20Understanding/","title":"SAM CLIP   Merging Vision Foundation Models towards Semantic and Spatial Understanding","text":"Properties authors Haoxiang Wang, Fartash Faghri, Raviteja Vemulapalli, Mehrdad Farajtabar, Sachin Mehta, Mohammad Rastegari, Oncel Tuzel, Hadi Pouransari, Pavan Kumar Anasosalu Vasu year 2024 url https://arxiv.org/abs/2310.15308 <p>Abstract</p> <p>The landscape of publicly available vision foundation models (VFMs), such as CLIP and Segment Anything Model (SAM), is expanding rapidly. VFMs are endowed with distinct capabilities stemming from their pre-training objectives. For instance, CLIP excels in semantic understanding, while SAM specializes in spatial understanding for segmentation. In this work, we introduce a simple recipe to efficiently merge VFMs into a unified model that absorbs their expertise. Our method integrates techniques of multi-task learning, continual learning, and distillation. Further, it demands significantly less computational cost compared to traditional multi-task training from scratch, and it only needs a small fraction of the pre-training datasets that were initially used to train individual models. By applying our method to SAM and CLIP, we obtain SAM-CLIP: a unified model that combines the capabilities of SAM and CLIP into a single vision transformer. Compared with deploying SAM and CLIP independently, our merged model, SAM-CLIP, reduces storage and compute costs for inference, making it well-suited for edge device applications. We show that SAM-CLIP not only retains the foundational strengths of SAM and CLIP, but also introduces synergistic functionalities, notably in zero-shot semantic segmentation, where SAM-CLIP establishes new state-of-the-art results on 5 benchmarks. It outperforms previous models that are specifically designed for this task by a large margin, including +6.8% and +5.9% mean IoU improvement on Pascal-VOC and COCO-Stuff datasets, respectively.</p>","tags":["paper","efficient_dl","efficient_vision"]},{"location":"100%20Reference%20notes/101%20Literature/Scaling%20%28Down%29%20CLIP%20-%20A%20Comprehensive%20Analysis%20of%20Data%2C%20Architecture%2C%20and%20Training%20Strategies/","title":"Scaling (Down) CLIP   A Comprehensive Analysis of Data, Architecture, and Training Strategies","text":"Properties authors Zichao Li, Cihang Xie, Ekin Dogus Cubuk year 2024 url https://arxiv.org/abs/2404.08197 <p>Abstract</p> <p>This paper investigates the performance of the Contrastive Language-Image Pre-training (CLIP) when scaled down to limited computation budgets. We explore CLIP along three dimensions: data, architecture, and training strategies. With regards to data, we demonstrate the significance of high-quality training data and show that a smaller dataset of high-quality data can outperform a larger dataset with lower quality. We also examine how model performance varies with different dataset sizes, suggesting that smaller ViT models are better suited for smaller datasets, while larger models perform better on larger datasets with fixed compute. Additionally, we provide guidance on when to choose a CNN-based architecture or a ViT-based architecture for CLIP training. We compare four CLIP training strategies - SLIP, FLIP, CLIP, and CLIP+Data Augmentation - and show that the choice of training strategy depends on the available compute resource. Our analysis reveals that CLIP+Data Augmentation can achieve comparable performance to CLIP using only half of the training data. This work provides practical insights into how to effectively train and deploy CLIP models, making them more accessible and affordable for practical use in various applications.</p>","tags":["efficient_dl","vit","cnn"]},{"location":"100%20Reference%20notes/101%20Literature/Segment%20Anything/","title":"Segment Anything","text":"Properties authors Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, Ross Girshick year 2023 url https://arxiv.org/abs/2304.02643 <p>Abstract</p> <p>We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at\u00a0this https URL\u00a0to foster research into foundation models for computer vision.</p>","tags":["paper","segmentation","computer_vision","foundation_models"]},{"location":"100%20Reference%20notes/101%20Literature/Self-Supervised%20Detection%20of%20Perfect%20and%20Partial%20Input-Dependent%20Symmetries/","title":"Self Supervised Detection of Perfect and Partial Input Dependent Symmetries","text":"Properties authors David W. Romero, Alonso Urbano","tags":["dl2","equivariance","partial_equivariance","inductive_bias"]},{"location":"100%20Reference%20notes/101%20Literature/SimPLR%20-%20A%20Simple%20and%20Plain%20Transformer%20for%20Scaling-Efficient%20Object%20Detection%20and%20Segmentation/","title":"SimPLR   A Simple and Plain Transformer for Scaling Efficient Object Detection and Segmentation","text":"Properties authors Duy-Kien Nguyen, Martin R. Oswald, Cees G. M. Snoek year 2024 url https://arxiv.org/abs/2310.05920 <p>Abstract</p> <p>The ability to detect objects in images at varying scales has played a pivotal role in the design of modern object detectors. Despite considerable progress in removing hand-crafted components and simplifying the architecture with transformers, multi-scale feature maps and/or pyramid design remain a key factor for their empirical success. In this paper, we show that this reliance on either feature pyramids or an hierarchical backbone is unnecessary and a transformer-based detector with scale-aware attention enables the plain detector 'SimPLR' whose backbone and detection head are both non-hierarchical and operate on single-scale features. We find through our experiments that SimPLR with scale-aware attention is plain and simple, yet competitive with multi-scale vision transformer alternatives. Compared to the multi-scale and single-scale state-of-the-art, our model scales much better with bigger capacity (self-supervised) models and more pre-training data, allowing us to report a consistently better accuracy and faster runtime for object detection, instance segmentation as well as panoptic segmentation. Code will be released.</p>","tags":["paper","object_detection","computer_vision","vit"]},{"location":"100%20Reference%20notes/101%20Literature/SimPLR%20-%20A%20Simple%20and%20Plain%20Transformer%20for%20Scaling-Efficient%20Object%20Detection%20and%20Segmentation/#notes","title":"Notes","text":"<p>\u201cDespite enabling plain-backbone detectors, feature pyramids are still an important factor in ViTDet to detect objects at various scales\u201d (Nguyen et al., 2024, p. 4)</p> <p>Not really an issue as far as I understand, but in the spirit of less inductive biases it makes sense. Feature pyramids intuitively hardcode scale information.</p> <p>\u201cMost recently, Lin et al. [35] introduce the transformer-based detector, PlainDETR, which also removes the multi-scale input. However, it still relies on multi-scale features to generate the object proposals for its decoder.\u201d (Nguyen et al., 2024, p. 4)</p> <p>Don't quite understand this, does this still allow arbitrary vits? - [ ] Read PlainDETR \ud83d\udd3d </p>","tags":["paper","object_detection","computer_vision","vit"]},{"location":"100%20Reference%20notes/101%20Literature/Simultaneous%20linear%20connectivity%20of%20neural%20networks%20modulo%20permutation/","title":"Simultaneous linear connectivity of neural networks modulo permutation","text":"Properties authors Ekansh Sharma, Devin Kwok, Tom Denton, Daniel M. Roy, David Rolnick, Gintare Karolina Dziugaite year 2024 url https://arxiv.org/abs/2404.06498 <p>Abstract</p> <p>Neural networks typically exhibit permutation symmetries which contribute to the non-convexity of the networks' loss landscapes, since linearly interpolating between two permuted versions of a trained network tends to encounter a high loss barrier. Recent work has argued that permutation symmetries are the only sources of non-convexity, meaning there are essentially no such barriers between trained networks if they are permuted appropriately. In this work, we refine these arguments into three distinct claims of increasing strength. We show that existing evidence only supports \"weak linear connectivity\"-that for each pair of networks belonging to a set of SGD solutions, there exist (multiple) permutations that linearly connect it with the other networks. In contrast, the claim \"strong linear connectivity\"-that for each network, there exists one permutation that simultaneously connects it with the other networks-is both intuitively and practically more desirable. This stronger claim would imply that the loss landscape is convex after accounting for permutation, and enable linear interpolation between three or more independently trained models without increased loss. In this work, we introduce an intermediate claim-that for certain sequences of networks, there exists one permutation that simultaneously aligns matching pairs of networks from these sequences. Specifically, we discover that a single permutation aligns sequences of iteratively trained as well as iteratively pruned networks, meaning that two networks exhibit low loss barriers at each step of their optimization and sparsification trajectories respectively. Finally, we provide the first evidence that strong linear connectivity may be possible under certain conditions, by showing that barriers decrease with increasing network width when interpolating among three networks.</p>","tags":["dl_theory","linear_connectivity","network_permutation_symmetries"]},{"location":"100%20Reference%20notes/101%20Literature/Stand-Alone%20Self-Attention%20in%20Vision%20Models/","title":"Stand Alone Self Attention in Vision Models","text":"Properties authors Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, Jonathon Shlens year 2019 <p>Abstract</p> <p>Convolutions are a fundamental building block of modern computer vision systems. Recent approaches have argued for going beyond convolutions in order to capture long-range dependencies. These efforts focus on augmenting convolutional models with content-based interactions, such as self-attention and non-local means, to achieve gains on a number of vision tasks. The natural question that arises is whether attention can be a stand-alone primitive for vision models instead of serving as just an augmentation on top of convolutions. In developing and testing a pure self-attention vision model, we verify that self-attention can indeed be an effective stand-alone layer. A simple procedure of replacing all instances of spatial convolutions with a form of self-attention applied to ResNet model produces a fully self-attentional model that outperforms the baseline on ImageNet classification with 12% fewer FLOPS and 29% fewer parameters. On COCO object detection, a pure self-attention model matches the mAP of a baseline RetinaNet while having 39% fewer FLOPS and 34% fewer parameters. Detailed ablation studies demonstrate that self-attention is especially impactful when used in later layers. These results establish that stand-alone self-attention is an important addition to the vision practitioner's toolbox.</p>","tags":["vit","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/Surgical%20Fine-Tuning%20Improves%20Adaptation%20to%20Distribution%20Shifts/","title":"Surgical Fine Tuning Improves Adaptation to Distribution Shifts","text":"Properties authors Yoonho Lee, Annie S. Chen, Fahim Tajwar, Huaxiu Yao, Percy Liang, Chelsea Finn, Ananya Kumar year 2022 url https://arxiv.org/abs/2210.11466 <p>Abstract</p> <p>A common approach to transfer learning under distribution shift is to fine-tune the last few layers of a pre-trained model, preserving learned features while also adapting to the new task. This paper shows that in such settings, selectively fine-tuning a subset of layers (which we term surgical fine-tuning) matches or outperforms commonly used fine-tuning approaches. Moreover, the type of distribution shift influences which subset is more effective to tune: for example, for image corruptions, fine-tuning only the first few layers works best. We validate our findings systematically across seven real-world data tasks spanning three types of distribution shifts. Theoretically, we prove that for two-layer neural networks in an idealized setting, first-layer tuning can outperform fine-tuning all layers. Intuitively, fine-tuning more parameters on a small target dataset can cause information learned during pre-training to be forgotten, and the relevant information depends on the type of shift.</p> <p>Notes: - Paper mentions that it depends on what kind of distribution shift the choice of layers (subset of parameters) to finetune. - They provide an automatic procedure to select those layers that beats full finetuning but is suboptimal when compared to expert/surgical finetuning. Suggest future work in this regard.</p>","tags":["paper","peft","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/Surgical-DINO%20-%20Adapter%20Learning%20of%20Foundation%20Models%20for%20Depth%20Estimation%20in%20Endoscopic%20Surgery/","title":"Surgical DINO   Adapter Learning of Foundation Models for Depth Estimation in Endoscopic Surgery","text":"Properties authors Beilei Cui, Mobarakol Islam, Long Bai, Hongliang Ren year 2024 url https://arxiv.org/abs/2401.06013 <p>Abstract</p> <p>Purpose: Depth estimation in robotic surgery is vital in 3D reconstruction, surgical navigation and augmented reality visualization. Although the foundation model exhibits outstanding performance in many vision tasks, including depth estimation (e.g., DINOv2), recent works observed its limitations in medical and surgical domain-specific applications. This work presents a low-ranked adaptation (LoRA) of the foundation model for surgical depth estimation. Methods: We design a foundation model-based depth estimation method, referred to as Surgical-DINO, a low-rank adaptation of the DINOv2 for depth estimation in endoscopic surgery. We build LoRA layers and integrate them into DINO to adapt with surgery-specific domain knowledge instead of conventional fine-tuning. During training, we freeze the DINO image encoder, which shows excellent visual representation capacity, and only optimize the LoRA layers and depth decoder to integrate features from the surgical scene. Results: Our model is extensively validated on a MICCAI challenge dataset of SCARED, which is collected from da Vinci Xi endoscope surgery. We empirically show that Surgical-DINO significantly outperforms all the state-of-the-art models in endoscopic depth estimation tasks. The analysis with ablation studies has shown evidence of the remarkable effect of our LoRA layers and adaptation. Conclusion: Surgical-DINO shed some light on the successful adaptation of the foundation models into the surgical domain for depth estimation. There is clear evidence in the results that zero-shot prediction on pre-trained weights in computer vision datasets or naive fine-tuning is not sufficient to use the foundation model in the surgical domain directly. Code is available at\u00a0this https URL.</p> <p>References: - LoRA - Low-Rank Adaptation of Large Language Models</p> <p>Keywords: - LoRa Adapter</p>","tags":["paper","efficient_dl","efficient_vision","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/Symmetries%20in%20Overparametrized%20Neural%20Networks%20-%20A%20Mean-Field%20View/","title":"Symmetries in Overparametrized Neural Networks   A Mean Field View","text":"Properties authors Javier Maass Martinez, Joaquin Fontbona year 2024 url https://arxiv.org/abs/2405.19995 <p>Abstract</p> <p>We develop a Mean-Field (MF) view of the learning dynamics of overparametrized Artificial Neural Networks (NN) under data symmetric in law wrt the action of a general compact group\u00a0G. We consider for this a class of generalized shallow NNs given by an ensemble of\u00a0N\u00a0multi-layer units, jointly trained using stochastic gradient descent (SGD) and possibly symmetry-leveraging (SL) techniques, such as Data Augmentation (DA), Feature Averaging (FA) or Equivariant Architectures (EA). We introduce the notions of weakly and strongly invariant laws (WI and SI) on the parameter space of each single unit, corresponding, respectively, to\u00a0G-invariant distributions, and to distributions supported on parameters fixed by the group action (which encode EA). This allows us to define symmetric models compatible with taking\u00a0N\u2192\u221e\u00a0and give an interpretation of the asymptotic dynamics of DA, FA and EA in terms of Wasserstein Gradient Flows describing their MF limits. When activations respect the group action, we show that, for symmetric data, DA, FA and freely-trained models obey the exact same MF dynamic, which stays in the space of WI laws and minimizes therein the population risk. We also give a counterexample to the general attainability of an optimum over SI laws. Despite this, quite remarkably, we show that the set of SI laws is also preserved by the MF dynamics even when freely trained. This sharply contrasts the finite-N\u00a0setting, in which EAs are generally not preserved by unconstrained SGD. We illustrate the validity of our findings as\u00a0N\u00a0gets larger in a teacher-student experimental setting, training a student NN to learn from a WI, SI or arbitrary teacher model through various SL schemes. We last deduce a data-driven heuristic to discover the largest subspace of parameters supporting SI distributions for a problem, that could be used for designing EA with minimal generalization error.</p>","tags":["dl_theory","equivariance"]},{"location":"100%20Reference%20notes/101%20Literature/Talaria%20-%20Interactively%20Optimizing%20Machine%20Learning%20Models%20for%20Efficient%20Inference/","title":"Talaria   Interactively Optimizing Machine Learning Models for Efficient Inference","text":"Properties authors Fred Hohman, Chaoqun Wang, Jinmook Lee, Jochen G\u00f6rtler, Dominik Moritz, Jeffrey P Bigham, Zhile Ren, Cecile Foret, Qi Shan, Ziaoyi Zhang year 2024 url https://arxiv.org/abs/2404.03085 <p>Abstract</p> <p>On-device machine learning (ML) moves computation from the cloud to personal devices, protecting user privacy and enabling intelligent user experiences. However, fitting models on devices with limited resources presents a major technical challenge: practitioners need to optimize models and balance hardware metrics such as model size, latency, and power. To help practitioners create efficient ML models, we designed and developed Talaria: a model visualization and optimization system. Talaria enables practitioners to compile models to hardware, interactively visualize model statistics, and simulate optimizations to test the impact on inference metrics. Since its internal deployment two years ago, we have evaluated Talaria using three methodologies: (1) a log analysis highlighting its growth of 800+ practitioners submitting 3,600+ models; (2) a usability survey with 26 users assessing the utility of 20 Talaria features; and (3) a qualitative interview with the 7 most active users about their experience using Talaria.</p>","tags":["efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/The%20Empirical%20Impact%20of%20Neural%20Parameter%20Symmetries%2C%20or%20Lack%20Thereof/","title":"The Empirical Impact of Neural Parameter Symmetries, or Lack Thereof","text":"Properties authors Derek Lim, Moe Putterman, Robin Walters, Haggai Maron, Stefanie Jegelka year 2024 url https://arxiv.org/abs/2405.20231 <p>Abstract</p> <p>Many algorithms and observed phenomena in deep learning appear to be affected by parameter symmetries -- transformations of neural network parameters that do not change the underlying neural network function. These include linear mode connectivity, model merging, Bayesian neural network inference, metanetworks, and several other characteristics of optimization or loss-landscapes. However, theoretical analysis of the relationship between parameter space symmetries and these phenomena is difficult. In this work, we empirically investigate the impact of neural parameter symmetries by introducing new neural network architectures that have reduced parameter space symmetries. We develop two methods, with some provable guarantees, of modifying standard neural networks to reduce parameter space symmetries. With these new methods, we conduct a comprehensive experimental study consisting of multiple tasks aimed at assessing the effect of removing parameter symmetries. Our experiments reveal several interesting observations on the empirical impact of parameter symmetries; for instance, we observe linear mode connectivity between our networks without alignment of weight spaces, and we find that our networks allow for faster and more effective Bayesian neural network training.</p>","tags":["equivariance","relaxed_equivariance","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/The%20Lie%20derivative%20for%20measuring%20learned%20equivariance/","title":"The Lie derivative for measuring learned equivariance","text":"Properties authors Nate Gruver, Marc Finzi, Micah Goldblum, Andrew Gordon Wilson year 2022 url https://arxiv.org/abs/2210.02984 <p>Abstract</p> <p>The Lie derivative is introduced, a method for measuring equivariance with strong mathematical foundations and minimal hyperparameters that finds that transformers can be more equivariant than convolutional neural networks after training, and that as models get larger and more accurate they tend to display more equivariance, regardless of architecture.</p>","tags":["equivariance"]},{"location":"100%20Reference%20notes/101%20Literature/The%20Lie%20derivative%20for%20measuring%20learned%20equivariance/#notes","title":"Notes","text":"","tags":["equivariance"]},{"location":"100%20Reference%20notes/101%20Literature/The%20Unreasonable%20Ineffectiveness%20of%20the%20Deeper%20Layers/","title":"The Unreasonable Ineffectiveness of the Deeper Layers","text":"Properties authors Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, Daniel A. Roberts year 2024 url https://arxiv.org/abs/2403.17887 <p>Abstract</p> <p>We empirically study a simple layer-pruning strategy for popular families of open-weight pretrained LLMs, finding minimal degradation of performance on different question-answering benchmarks until after a large fraction (up to half) of the layers are removed. To prune these models, we identify the optimal block of layers to prune by considering similarity across layers; then, to \"heal\" the damage, we perform a small amount of finetuning. In particular, we use parameter-efficient finetuning (PEFT) methods, specifically quantization and Low Rank Adapters (QLoRA), such that each of our experiments can be performed on a single A100 GPU. From a practical perspective, these results suggest that layer pruning methods can complement other PEFT strategies to further reduce computational resources of finetuning on the one hand, and can improve the memory and latency of inference on the other hand. From a scientific perspective, the robustness of these LLMs to the deletion of layers implies either that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge.</p>","tags":["transformers","efficient_dl","pruning","quantization"]},{"location":"100%20Reference%20notes/101%20Literature/TiC-CLIP%20-%20Continual%20Training%20of%20CLIP%20models/","title":"TiC CLIP   Continual Training of CLIP models","text":"Properties authors Saurabh Garg, Mehrdad Farajtabar, Hadi Pouransari, Raviteja Vemulapalli, Sachin Mehta, Oncel Tuzel, Vaishaal Shankar, Fartash Faghri year 2024 url https://arxiv.org/abs/2310.16226 <p>Abstract</p> <p>Keeping large foundation models up to date on latest data is inherently expensive. To avoid the prohibitive costs of constantly retraining, it is imperative to continually train these models. This problem is exacerbated by the lack of any large scale continual learning benchmarks or baselines. We introduce the first set of web-scale Time-Continual (TiC) benchmarks for training vision-language models: TiC-DataComp, TiC-YFCC, and TiC-Redcaps. TiC-DataComp, our largest dataset, contains over 12.7B timestamped image-text pairs spanning 9 years (2014-2022). We first use our benchmarks to curate various dynamic evaluations to measure temporal robustness of existing models. We show OpenAI's CLIP (trained on data up to 2020) loses\u00a0\u22488%\u00a0zero-shot accuracy on our curated retrieval task from 2021-2022 compared with more recently trained models in OpenCLIP repository. We then study how to efficiently train models on time-continuous data. We demonstrate that a simple rehearsal-based approach that continues training from the last checkpoint and replays old data reduces compute by\u00a02.5\u00d7\u00a0when compared to the standard practice of retraining from scratch. Code is available at\u00a0this https URL.</p>","tags":["paper","continual_learning","multimodal"]},{"location":"100%20Reference%20notes/101%20Literature/Training%20quantized%20nets%20-%20A%20deeper%20understanding/","title":"Training quantized nets   A deeper understanding","text":"Properties authors Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, Tom Goldstein year 2017 url https://arxiv.org/abs/1706.02379 <p>Abstract</p> <p>Currently, deep neural networks are deployed on low-power portable devices by first training a full-precision model using powerful hardware, and then deriving a corresponding low-precision model for efficient inference on such systems. However, training models directly with coarsely quantized weights is a key step towards learning on embedded platforms that have limited computing resources, memory capacity, and power consumption. Numerous recent publications have studied methods for training quantized networks, but these studies have mostly been empirical. In this work, we investigate training methods for quantized neural networks from a theoretical viewpoint. We first explore accuracy guarantees for training methods under convexity assumptions. We then look at the behavior of these algorithms for non-convex problems, and show that training algorithms that exploit high-precision representations have an important greedy search phase that purely quantized training methods lack, which explains the difficulty of training using low-precision arithmetic.</p>","tags":["paper","quantization"]},{"location":"100%20Reference%20notes/101%20Literature/Training%20quantized%20nets%20-%20A%20deeper%20understanding/#notes","title":"Notes","text":"<ul> <li> Read paper</li> </ul>","tags":["paper","quantization"]},{"location":"100%20Reference%20notes/101%20Literature/Understanding%20Deep%20Learning%20-%20Chapter%2010/","title":"Understanding Deep Learning   Chapter 10","text":"Properties authors Simon J.D. Prince year 2023 url https://udlbook.github.io/udlbook/","tags":["textbook","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Understanding%20Deep%20Learning%20-%20Chapter%2020/","title":"Understanding Deep Learning   Chapter 20","text":"Properties authors Simon J.D. Prince year 2023 url https://udlbook.github.io/udlbook/","tags":["textbook","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Understanding%20Deep%20Learning%20-%20Chapter%2020/#chapter-20-why-does-deep-learning-work","title":"Chapter 20: Why does deep learning work?","text":"<p>Contents</p> <ul> <li>20.1 The case against deep learning </li> <li>20.2 Factors that influence fitting performance </li> <li>20.3 Properties of loss functions</li> <li>20.4 Factors that determine generalization</li> <li>20.5 Do we need so many parameters? </li> <li>20.6 Do networks have to be deep?</li> <li>20.7 Summary</li> </ul>","tags":["textbook","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Understanding%20symmetries%20in%20deep%20networks/","title":"Understanding symmetries in deep networks","text":"Properties authors Vijay Badrinarayanan, Bamdev Mishra, Roberto Cipolla year 2015 url https://arxiv.org/abs/1511.01029 <p>Abstract</p> <p>Recent works have highlighted scale invariance or symmetry present in the weight space of a typical deep network and the adverse effect it has on the Euclidean gradient based stochastic gradient descent optimization. In this work, we show that a commonly used deep network, which uses convolution, batch normalization, reLU, max-pooling, and sub-sampling pipeline, possess more complex forms of symmetry arising from scaling-based reparameterization of the network weights. We propose to tackle the issue of the weight space symmetry by constraining the filters to lie on the unit-norm manifold. Consequently, training the network boils down to using stochastic gradient descent updates on the unit-norm manifold. Our empirical evidence based on the MNIST dataset shows that the proposed updates improve the test performance beyond what is achieved with batch normalization and without sacrificing the computational efficiency of the weight updates.</p> <p>,</p>","tags":["dl_theory","dl2"]},{"location":"100%20Reference%20notes/101%20Literature/Using%20Degeneracy%20in%20the%20Loss%20Landscape%20for%20Mechanistic%20Interpretability/","title":"Using Degeneracy in the Loss Landscape for Mechanistic Interpretability","text":"Properties authors Lucius Bushnaq, Jake Mendel, Stefan Heimersheim, Dan Braun, Nicholas Goldowsky-Dill, Kaarel H\u00e4nni, Cindy Wu, Marius Hobbhahn year 2024 url https://arxiv.org/abs/2405.10927 <p>Abstract</p> <p>Mechanistic Interpretability aims to reverse engineer the algorithms implemented by neural networks by studying their weights and activations. An obstacle to reverse engineering neural networks is that many of the parameters inside a network are not involved in the computation being implemented by the network. These degenerate parameters may obfuscate internal structure. Singular learning theory teaches us that neural network parameterizations are biased towards being more degenerate, and parameterizations with more degeneracy are likely to generalize further. We identify 3 ways that network parameters can be degenerate: linear dependence between activations in a layer; linear dependence between gradients passed back to a layer; ReLUs which fire on the same subset of datapoints. We also present a heuristic argument that modular networks are likely to be more degenerate, and we develop a metric for identifying modules in a network that is based on this argument. We propose that if we can represent a neural network in a way that is invariant to reparameterizations that exploit the degeneracies, then this representation is likely to be more interpretable, and we provide some evidence that such a representation is likely to have sparser interactions. We introduce the Interaction Basis, a tractable technique to obtain a representation that is invariant to degeneracies from linear dependence of activations or Jacobians.</p>","tags":["paper","dl_theory","mechinterp","optimization"]},{"location":"100%20Reference%20notes/101%20Literature/ViDT%20-%20An%20Efficient%20and%20Effective%20Fully%20Transformer-based%20Object%20Detector/","title":"ViDT   An Efficient and Effective Fully Transformer based Object Detector","text":"Properties authors Hwanjun Song, Deqing Sun, Sanghyuk Chun, Varun Jampani, Dongyoon Han, Byeongho Heo, Wonjae Kim, Ming-Hsuan Yang year 2021 url https://arxiv.org/abs/2110.03921 <p>Abstract</p> <p>Transformers are transforming the landscape of computer vision, especially for recognition tasks. Detection transformers are the first fully end-to-end learning systems for object detection, while vision transformers are the first fully transformer-based architecture for image classification. In this paper, we integrate Vision and Detection Transformers (ViDT) to build an effective and efficient object detector. ViDT introduces a reconfigured attention module to extend the recent Swin Transformer to be a standalone object detector, followed by a computationally efficient transformer decoder that exploits multi-scale features and auxiliary techniques essential to boost the detection performance without much increase in computational load. Extensive evaluation results on the Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP and latency trade-off among existing fully transformer-based object detectors, and achieves 49.2AP owing to its high scalability for large models. We will release the code and trained models at\u00a0this https URL</p>","tags":["paper","object_detection","vit","computer_vision"]},{"location":"100%20Reference%20notes/101%20Literature/Vision%20Mamba%20-%20Efficient%20Visual%20Representation%20Learning%20with%20Bidirectional%20State%20Space%20Model/","title":"Vision Mamba   Efficient Visual Representation Learning with Bidirectional State Space Model","text":"Properties authors Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, Xinggang Wang year 2024 url https://arxiv.org/abs/2401.09417 <p>Abstract</p> <p>Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation &amp; memory efficiency. For example, Vim is 2.8\u00d7\u00a0faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248\u00d71248. The results demonstrate that Vim is capable of overcoming the computation &amp; memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models. Code is available at\u00a0this https URL.</p>","tags":["transformers","mamba","ssm","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/Vision%20Transformers%20Need%20Registers/","title":"Vision Transformers Need Registers","text":"Properties authors Timoth\u00e9e Darcet, Maxime Oquab, Julien Mairal, Piotr Bojanowski year 2023 url https://arxiv.org/pdf/2309.16588 <p>Abstract</p> <p>Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing.</p>","tags":["paper","vit","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Vision%20Transformers%20Need%20Registers/#note","title":"Note","text":"<ul> <li>note to myself:<ul> <li> Read paper in depth #personal \ud83d\udd3c </li> </ul> </li> </ul>","tags":["paper","vit","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/What%20Do%20Self-Supervised%20Vision%20Transformers%20Learn%3F/","title":"What Do Self Supervised Vision Transformers Learn?","text":"Properties authors Namuk Park, Wonjae Kim, Byeongho Heo, Taekyung Kim, Sangdoo Yun year 2023 url https://arxiv.org/abs/2305.00729 <p>Abstract</p> <p>We present a comparative study on how and why contrastive learning (CL) and masked image modeling (MIM) differ in their representations and in their performance of downstream tasks. In particular, we demonstrate that self-supervised Vision Transformers (ViTs) have the following properties: (1) CL trains self-attentions to capture longer-range global patterns than MIM, such as the shape of an object, especially in the later layers of the ViT architecture. This CL property helps ViTs linearly separate images in their representation spaces. However, it also makes the self-attentions collapse into homogeneity for all query tokens and heads. Such homogeneity of self-attention reduces the diversity of representations, worsening scalability and dense prediction performance. (2) CL utilizes the low-frequency signals of the representations, but MIM utilizes high-frequencies. Since low- and high-frequency information respectively represent shapes and textures, CL is more shape-oriented and MIM more texture-oriented. (3) CL plays a crucial role in the later layers, while MIM mainly focuses on the early layers. Upon these analyses, we find that CL and MIM can complement each other and observe that even the simplest harmonization can help leverage the advantages of both methods. The code is available at\u00a0this https URL.</p>","tags":["paper","dl_theory","vit","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/What%20Do%20Self-Supervised%20Vision%20Transformers%20Learn%3F/#notes","title":"Notes","text":"<p>Another certified banger\u2122 by Naver AI Lab. Also check How do vision transformers work? (the link might not be working because of the interrogation symbol on the name, will fix later).</p> <ul> <li> Add annotations from Zotero \ud83d\udd3d</li> </ul>","tags":["paper","dl_theory","vit","transformers"]},{"location":"100%20Reference%20notes/102%20Authors/Albert%20Gu/","title":"Albert Gu","text":"Properties affiliation Carnegie Mellon University"},{"location":"100%20Reference%20notes/102%20Authors/Alex%20Flinth/","title":"Alex Flinth","text":"Properties affiliation Umea University"},{"location":"100%20Reference%20notes/102%20Authors/Alexander%20Kirillov/","title":"Alexander Kirillov","text":"Properties affiliation OpenAI, FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Alexey%20Dosovitskiy/","title":"Alexey Dosovitskiy","text":"Properties affiliation Google"},{"location":"100%20Reference%20notes/102%20Authors/Ananya%20Kumar/","title":"Ananya Kumar","text":"Properties affiliation Stanford"},{"location":"100%20Reference%20notes/102%20Authors/Andreas%20Loukas/","title":"Andreas Loukas","text":"Properties affiliation EPFL"},{"location":"100%20Reference%20notes/102%20Authors/Andreas%20Savakis/","title":"Andreas Savakis","text":"Properties affiliation Rochester Institute of Technology"},{"location":"100%20Reference%20notes/102%20Authors/Angela%20Fan/","title":"Angela Fan","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Annie%20S.%20Chen/","title":"Annie S. Chen","text":"Properties affiliation Stanford"},{"location":"100%20Reference%20notes/102%20Authors/Antonio%20Orvieto/","title":"Antonio Orvieto","text":"Properties affiliation Max Planck Institute for Intelligent Systems"},{"location":"100%20Reference%20notes/102%20Authors/Ardavan%20Pedram/","title":"Ardavan Pedram","text":"Properties affiliation Stanford, Samsung"},{"location":"100%20Reference%20notes/102%20Authors/Armand%20Joulin/","title":"Armand Joulin","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Attila%20Lengyel/","title":"Attila Lengyel","text":"Properties affiliation TU Delft"},{"location":"100%20Reference%20notes/102%20Authors/Boshi%20Wang/","title":"Boshi Wang","text":"Properties affiliation The Ohio State University"},{"location":"100%20Reference%20notes/102%20Authors/Byeongho%20Heo/","title":"Byeongho Heo","text":"Properties affiliation Naver AI Lab"},{"location":"100%20Reference%20notes/102%20Authors/Caglar%20Gulcehre/","title":"Caglar Gulcehre","text":"Properties affiliation CLAIRE, EPFL"},{"location":"100%20Reference%20notes/102%20Authors/Carmen%20Amo%20Alonso/","title":"Carmen Amo Alonso","text":"Properties affiliation ETH Zurich"},{"location":"100%20Reference%20notes/102%20Authors/Cees%20G.%20M.%20Snoek/","title":"Cees G. M. Snoek","text":"Properties affiliation University of Amsterdam"},{"location":"100%20Reference%20notes/102%20Authors/Chelsea%20Finn/","title":"Chelsea Finn","text":"Properties affiliation Stanford"},{"location":"100%20Reference%20notes/102%20Authors/Chong%20Wang/","title":"Chong Wang","text":"Properties affiliation Apple, Princeton University"},{"location":"100%20Reference%20notes/102%20Authors/Christopher%20Olah/","title":"Christopher Olah","text":"Properties affiliation Anthropic"},{"location":"100%20Reference%20notes/102%20Authors/Daniel%20M.%20Roy/","title":"Daniel M. Roy","text":"Properties affiliation Vector Institute"},{"location":"100%20Reference%20notes/102%20Authors/Daniel%20Ulbricht/","title":"Daniel Ulbricht","text":"Properties affiliation Apple"},{"location":"100%20Reference%20notes/102%20Authors/David%20M.%20Knigge/","title":"David M. Knigge","text":"Properties affiliation University of Amsterdam"},{"location":"100%20Reference%20notes/102%20Authors/David%20W.%20Romero/","title":"David W. Romero","text":"Properties affiliation Vrije Universiteit Amsterdam"},{"location":"100%20Reference%20notes/102%20Authors/Diane%20Larlus/","title":"Diane Larlus","text":"Properties affiliation Naver Labs Europe"},{"location":"100%20Reference%20notes/102%20Authors/Donghyun%20Kim/","title":"Donghyun Kim","text":"Properties affiliation Naver Cloud AI"},{"location":"100%20Reference%20notes/102%20Authors/Dongyoon%20Han/","title":"Dongyoon Han","text":"Properties affiliation Naver AI Lab"},{"location":"100%20Reference%20notes/102%20Authors/Duy-Kien%20Nguyen/","title":"Duy Kien Nguyen","text":"Properties affiliation University of Amsterdam"},{"location":"100%20Reference%20notes/102%20Authors/Edward%20J.%20Hu/","title":"Edward J. Hu","text":"Properties affiliation Microsoft"},{"location":"100%20Reference%20notes/102%20Authors/Eric%20Mintun/","title":"Eric Mintun","text":"Properties affiliation FAIR, UC Santa Barbara"},{"location":"100%20Reference%20notes/102%20Authors/Erik%20J.%20Bekkers/","title":"Erik J. Bekkers","text":"Properties affiliation University of Amsterdam"},{"location":"100%20Reference%20notes/102%20Authors/Eshan%20Verma/","title":"Eshan Verma","text":"Properties affiliation Apple"},{"location":"100%20Reference%20notes/102%20Authors/Fahim%20Tajwar/","title":"Fahim Tajwar","text":"Properties affiliation Stanford"},{"location":"100%20Reference%20notes/102%20Authors/Fartash%20Faghri/","title":"Fartash Faghri","text":"Properties affiliation Apple"},{"location":"100%20Reference%20notes/102%20Authors/Francisco%20Massa/","title":"Francisco Massa","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Fred%20Hohman/","title":"Fred Hohman","text":"Properties affiliation Apple"},{"location":"100%20Reference%20notes/102%20Authors/Furu%20Wei/","title":"Furu Wei","text":"Properties affiliation Microsoft"},{"location":"100%20Reference%20notes/102%20Authors/Gabriel%20Synnaeve/","title":"Gabriel Synnaeve","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Gintare%20Karolina%20Dziugaite/","title":"Gintare Karolina Dziugaite","text":"Properties affiliation Google DeepMind"},{"location":"100%20Reference%20notes/102%20Authors/Hadi%20Pouransari/","title":"Hadi Pouransari","text":"Properties affiliation Apple"},{"location":"100%20Reference%20notes/102%20Authors/Han%20Cai/","title":"Han Cai","text":"Properties affiliation MIT, Shanghai Jiao Tong University"},{"location":"100%20Reference%20notes/102%20Authors/Hanzi%20Mao/","title":"Hanzi Mao","text":"Properties affiliation FAIR, NVIDIA"},{"location":"100%20Reference%20notes/102%20Authors/Haoxiang%20Wang/","title":"Haoxiang Wang","text":"Properties affiliation Apple, University of Illinois at Urbana-Champaign"},{"location":"100%20Reference%20notes/102%20Authors/Herv%C3%A9%20Jegou/","title":"Herv\u00e9 Jegou","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Huaxiu%20Yao/","title":"Huaxiu Yao","text":"Properties affiliation Stanford"},{"location":"100%20Reference%20notes/102%20Authors/Hugo%20Touvron/","title":"Hugo Touvron","text":"Properties affiliation FAIR, Sorbonne University"},{"location":"100%20Reference%20notes/102%20Authors/Huizi%20Mao/","title":"Huizi Mao","text":"Properties affiliation NVIDIA"},{"location":"100%20Reference%20notes/102%20Authors/Isha%20Garg/","title":"Isha Garg","text":"Properties affiliation Purdue University, Apple"},{"location":"100%20Reference%20notes/102%20Authors/Ishan%20Misra/","title":"Ishan Misra","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Jan%20E.%20Gerken/","title":"Jan E. Gerken","text":"Properties affiliation Chalmers University of Technology"},{"location":"100%20Reference%20notes/102%20Authors/Javier%20Maass%20Martinez/","title":"Javier Maass Martinez","text":"Properties affiliation University of Chile"},{"location":"100%20Reference%20notes/102%20Authors/Jean-Baptiste%20Cordonnier/","title":"Jean Baptiste Cordonnier","text":"Properties affiliation EPFL"},{"location":"100%20Reference%20notes/102%20Authors/Jeff%20Pool/","title":"Jeff Pool","text":"Properties affiliation NVIDIA"},{"location":"100%20Reference%20notes/102%20Authors/Jing%20Pu/","title":"Jing Pu","text":"Properties affiliation Google, Stanford"},{"location":"100%20Reference%20notes/102%20Authors/Joaquin%20Fontbona/","title":"Joaquin Fontbona","text":"Properties affiliation University of Chile"},{"location":"100%20Reference%20notes/102%20Authors/John%20Denker/","title":"John Denker","text":"Properties affiliation Nokia Bell Labs"},{"location":"100%20Reference%20notes/102%20Authors/John%20Tran/","title":"John Tran","text":"Properties affiliation NVIDIA"},{"location":"100%20Reference%20notes/102%20Authors/Julien%20Mairal/","title":"Julien Mairal","text":"Properties affiliation INRIA"},{"location":"100%20Reference%20notes/102%20Authors/Juliette%20Marrie/","title":"Juliette Marrie","text":"Properties affiliation Naver Labs Europe, INRIA"},{"location":"100%20Reference%20notes/102%20Authors/Kaiming%20He/","title":"Kaiming He","text":"Properties affiliation FAIR, MIT"},{"location":"100%20Reference%20notes/102%20Authors/Kamyar%20Azizzadenesheli/","title":"Kamyar Azizzadenesheli","text":"Properties affiliation NVIDIA, Purdue University"},{"location":"100%20Reference%20notes/102%20Authors/Kaushik%20Roy/","title":"Kaushik Roy","text":"Properties affiliation Purdue University"},{"location":"100%20Reference%20notes/102%20Authors/Lawrence%20Chan/","title":"Lawrence Chan","text":"Properties affiliation UC Berkeley"},{"location":"100%20Reference%20notes/102%20Authors/Lucius%20Bushnaq/","title":"Lucius Bushnaq","text":"Properties affiliation Apollo Research"},{"location":"100%20Reference%20notes/102%20Authors/Maciej%20Wo%C5%82czyk/","title":"Maciej Wo\u0142czyk","text":"Properties affiliation IDEAS NCBR"},{"location":"100%20Reference%20notes/102%20Authors/Mahmoud%20Assran/","title":"Mahmoud Assran","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Marc%20Finzi/","title":"Marc Finzi","text":"Properties affiliation New York University"},{"location":"100%20Reference%20notes/102%20Authors/Mark%20A.%20Horowitz/","title":"Mark A. Horowitz","text":"Properties affiliation Stanford"},{"location":"100%20Reference%20notes/102%20Authors/Martin%20Jaggi/","title":"Martin Jaggi","text":"Properties affiliation EPFL"},{"location":"100%20Reference%20notes/102%20Authors/Martin%20R.%20Oswald/","title":"Martin R. Oswald","text":"Properties affiliation University of Amsterdam"},{"location":"100%20Reference%20notes/102%20Authors/Mathilde%20Caron/","title":"Mathilde Caron","text":"Properties affiliation FAIR, INRIA"},{"location":"100%20Reference%20notes/102%20Authors/Maxime%20Oquab/","title":"Maxime Oquab","text":"Properties affiliation FAIR, INRIA"},{"location":"100%20Reference%20notes/102%20Authors/Mehrdad%20Farajtabar/","title":"Mehrdad Farajtabar","text":"Properties affiliation Apple"},{"location":"100%20Reference%20notes/102%20Authors/Michael%20Arbel/","title":"Michael Arbel","text":"Properties affiliation INRIA"},{"location":"100%20Reference%20notes/102%20Authors/Mohammad%20Rastegari/","title":"Mohammad Rastegari","text":"Properties affiliation Apple"},{"location":"100%20Reference%20notes/102%20Authors/Namuk%20Park/","title":"Namuk Park","text":"Properties affiliation Naver AI Lab, Prescient Design, Genentech"},{"location":"100%20Reference%20notes/102%20Authors/Navin%20Ranjan/","title":"Navin Ranjan","text":"Properties affiliation Rochester Institute of Technology"},{"location":"100%20Reference%20notes/102%20Authors/Neel%20Nanda/","title":"Neel Nanda","text":"Properties affiliation Google DeepMind, Anthropic"},{"location":"100%20Reference%20notes/102%20Authors/Nicolas%20Carion/","title":"Nicolas Carion","text":"Properties affiliation New York University"},{"location":"100%20Reference%20notes/102%20Authors/Nicolas%20Usunier/","title":"Nicolas Usunier","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Oncel%20Tuzel/","title":"Oncel Tuzel","text":"Properties affiliation Apple"},{"location":"100%20Reference%20notes/102%20Authors/Patrick%20Forr%C3%A9/","title":"Patrick Forr\u00e9","text":"Properties affiliation University of Amsterdam"},{"location":"100%20Reference%20notes/102%20Authors/Pavan%20Kumar%20Anasosalu%20Vasu/","title":"Pavan Kumar Anasosalu Vasu","text":"Properties affiliation Apple"},{"location":"100%20Reference%20notes/102%20Authors/Percy%20Liang/","title":"Percy Liang","text":"Properties affiliation Stanford"},{"location":"100%20Reference%20notes/102%20Authors/Piotr%20Bojanowski/","title":"Piotr Bojanowski","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Raviteja%20Vemulapalli/","title":"Raviteja Vemulapalli","text":"Properties affiliation Apple"},{"location":"100%20Reference%20notes/102%20Authors/Razvan%20Pascanu/","title":"Razvan Pascanu","text":"Properties affiliation Google DeepMind"},{"location":"100%20Reference%20notes/102%20Authors/Robin%20Walters/","title":"Robin Walters","text":"Properties affiliation Northeastern University"},{"location":"100%20Reference%20notes/102%20Authors/Rose%20Yu/","title":"Rose Yu","text":"Properties affiliation UC San Diego"},{"location":"100%20Reference%20notes/102%20Authors/Ross%20Girshick/","title":"Ross Girshick","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Rui%20Wang/","title":"Rui Wang","text":"Properties affiliation MIT, UC San Diego"},{"location":"100%20Reference%20notes/102%20Authors/Ruoming%20Pang/","title":"Ruoming Pang","text":"Properties affiliation Apple, Princeton University"},{"location":"100%20Reference%20notes/102%20Authors/Sachin%20Mehta/","title":"Sachin Mehta","text":"Properties affiliation Apple"},{"location":"100%20Reference%20notes/102%20Authors/Sangdoo%20Yun/","title":"Sangdoo Yun","text":"Properties affiliation Naver AI Lab"},{"location":"100%20Reference%20notes/102%20Authors/Sanghyuk%20Chun/","title":"Sanghyuk Chun","text":"Properties affiliation Naver AI Lab"},{"location":"100%20Reference%20notes/102%20Authors/Sara%20Solla/","title":"Sara Solla","text":"Properties affiliation Northwestern University"},{"location":"100%20Reference%20notes/102%20Authors/Sergey%20Zagoruyko/","title":"Sergey Zagoruyko","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Shaohan%20Huang/","title":"Shaohan Huang","text":"Properties affiliation Microsoft"},{"location":"100%20Reference%20notes/102%20Authors/Simon%20J.D.%20Prince/","title":"Simon J.D. Prince","text":"Properties affiliation University of Bath"},{"location":"100%20Reference%20notes/102%20Authors/Skander%20Moalla/","title":"Skander Moalla","text":"Properties affiliation CLAIRE, EPFL"},{"location":"100%20Reference%20notes/102%20Authors/Soham%20De/","title":"Soham De","text":"Properties affiliation Google DeepMind, University of Maryland"},{"location":"100%20Reference%20notes/102%20Authors/Song%20Han/","title":"Song Han","text":"Properties affiliation MIT"},{"location":"100%20Reference%20notes/102%20Authors/Songkuk%20Kim/","title":"Songkuk Kim","text":"Properties affiliation Yonsei University"},{"location":"100%20Reference%20notes/102%20Authors/Sourya%20Basu/","title":"Sourya Basu","text":"Properties affiliation University of Illinois at Urbana-Champaign, IBM Research"},{"location":"100%20Reference%20notes/102%20Authors/St%C3%A9phane%20d%27Ascoli/","title":"St\u00e9phane d'Ascoli","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Sukjun%20Hwang/","title":"Sukjun Hwang","text":"Properties affiliation Carnegie Mellon University"},{"location":"100%20Reference%20notes/102%20Authors/Taekyung%20Kim/","title":"Taekyung Kim","text":"Properties affiliation Naver AI Lab"},{"location":"100%20Reference%20notes/102%20Authors/Tete%20Xiao/","title":"Tete Xiao","text":"Properties affiliation FAIR <p>Associations: FAIR, UC Berkeley</p>"},{"location":"100%20Reference%20notes/102%20Authors/Tom%20Gunter/","title":"Tom Gunter","text":"Properties affiliation Apple, University of Oxford"},{"location":"100%20Reference%20notes/102%20Authors/Tom%20Lieberum/","title":"Tom Lieberum","text":"Properties affiliation University of Amsterdam, Google DeepMind"},{"location":"100%20Reference%20notes/102%20Authors/Vaibhav%20Aggarwal/","title":"Vaibhav Aggarwal","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/William%20J.%20Dally/","title":"William J. Dally","text":"Properties affiliation Stanford, NVIDIA"},{"location":"100%20Reference%20notes/102%20Authors/Wonjae%20Kim/","title":"Wonjae Kim","text":"Properties affiliation Naver AI Lab"},{"location":"100%20Reference%20notes/102%20Authors/Xiang%20Yue/","title":"Xiang Yue","text":"Properties affiliation Carnegie Mellon University"},{"location":"100%20Reference%20notes/102%20Authors/Xingyu%20Liu/","title":"Xingyu Liu","text":"Properties affiliation Carnegie Mellon University"},{"location":"100%20Reference%20notes/102%20Authors/Xinlei%20Chen/","title":"Xinlei Chen","text":"Properties affiliation FAIR, Zhejiang University, Carnegie Mellon University, Zhejiang University"},{"location":"100%20Reference%20notes/102%20Authors/Xiuying%20Wei/","title":"Xiuying Wei","text":"Properties affiliation EPFL, CLAIRE"},{"location":"100%20Reference%20notes/102%20Authors/Xu%20Ma/","title":"Xu Ma","text":"Properties affiliation Northeastern University"},{"location":"100%20Reference%20notes/102%20Authors/Xun%20Wu/","title":"Xun Wu","text":"Properties affiliation Microsoft, Tsinghua University"},{"location":"100%20Reference%20notes/102%20Authors/Yanghao%20Li/","title":"Yanghao Li","text":"Properties affiliation FAIR, Apple"},{"location":"100%20Reference%20notes/102%20Authors/Yann%20LeCun/","title":"Yann LeCun","text":"Properties affiliation FAIR, New York University"},{"location":"100%20Reference%20notes/102%20Authors/Yelong%20Shen/","title":"Yelong Shen","text":"Properties affiliation Microsoft"},{"location":"100%20Reference%20notes/102%20Authors/Yoonho%20Lee/","title":"Yoonho Lee","text":"Properties affiliation Stanford"},{"location":"100%20Reference%20notes/102%20Authors/Zeyuan%20Allen-Zhu/","title":"Zeyuan Allen Zhu","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Zhuoyang%20Zhang/","title":"Zhuoyang Zhang","text":"Properties affiliation NVIDIA, Tsinghua University"},{"location":"100%20Reference%20notes/102%20Authors/Ziaoyi%20Zhang/","title":"Ziaoyi Zhang","text":"Properties affiliation Apple"},{"location":"100%20Reference%20notes/102%20Authors/Zirui%20Wang/","title":"Zirui Wang","text":"Properties affiliation Apple, Google, Carnegie Mellon University"},{"location":"100%20Reference%20notes/103%20Affiliations/CLAIRE/","title":"CLAIRE","text":""},{"location":"100%20Reference%20notes/103%20Affiliations/CLAIRE/#three-essential-pilars-of-the-lab","title":"Three essential pilars of the lab","text":"<p>Efficient deep learning algorithms</p> <ul> <li>Efficient RL</li> <li>Sample efficient learning algorithms</li> <li>Model Recycling</li> <li>Efficient sequence models</li> </ul> <p>Robust, safe and responsible algorithms</p> <ul> <li>RLHF/Alignment</li> <li>Uncertainty aware/Bayesian algorithms</li> <li>Offline RL</li> <li>Active learning/Human in the loop algorithms</li> <li>Better evaluations</li> </ul> <p>Improving reasoning: Moving from system 1 to system 2 level thinking</p> <ul> <li>Improving reasoning</li> <li>Creativity</li> <li>Deliberation</li> <li>Causality</li> <li>Imagination</li> <li>Planning</li> </ul>"},{"location":"100%20Reference%20notes/103%20Affiliations/CLAIRE/#notes","title":"Notes","text":"<ul> <li>omg, this is amazing</li> <li> Note to self: Look at CLAIRE's research \u23eb </li> </ul>"},{"location":"100%20Reference%20notes/103%20Affiliations/Naver%20Labs%20Europe/","title":"Naver Labs Europe","text":"<p>Related to Naver AI Lab</p>"},{"location":"100%20Reference%20notes/104%20Other/EPFL-CS439%20-%20Optimization%20for%20Machine%20Learning/","title":"EPFL CS439   Optimization for Machine Learning","text":"Properties authors Martin Jaggi, Nicolas Flammarion year 2024 url https://github.com/epfml/OptML_course/tree/master <p>Abstract</p> <p>This course teaches an overview of modern mathematical optimization methods, for applications in machine learning and data science. In particular, scalability of algorithms to large datasets will be discussed in theory and in implementation.</p> <p>Topics</p> <p>Convexity, Gradient Methods, Proximal algorithms, Subgradient Methods, Stochastic and Online Variants of mentioned methods, Coordinate Descent, Frank-Wolfe, Accelerated Methods, Primal-Dual context and certificates, Lagrange and Fenchel Duality, Second-Order Methods including Quasi-Newton Methods, Derivative-Free Optimization.</p>","tags":["course","optimization"]},{"location":"100%20Reference%20notes/104%20Other/Introducing%20Apple%E2%80%99s%20On-Device%20and%20Server%20Foundation%20Models/","title":"Introducing Apple\u2019s On Device and Server Foundation Models","text":"Properties year 2024 url https://machinelearning.apple.com/research/introducing-apple-foundation-models","tags":["efficient_dl"]},{"location":"100%20Reference%20notes/104%20Other/Introducing%20Apple%E2%80%99s%20On-Device%20and%20Server%20Foundation%20Models/#pre-training","title":"## Pre-Training","text":"<p>Our foundation models are trained on\u00a0Apple's AXLearn framework, an open-source project we released in 2023. It builds on top of JAX and XLA, and allows us to train the models with high efficiency and scalability on various training hardware and cloud platforms, including TPUs and both cloud and on-premise GPUs. We used a combination of data parallelism, tensor parallelism, sequence parallelism, and Fully Sharded Data Parallel (FSDP) to scale training along multiple dimensions such as data, model, and sequence length.</p>","tags":["efficient_dl"]},{"location":"100%20Reference%20notes/104%20Other/Introducing%20Apple%E2%80%99s%20On-Device%20and%20Server%20Foundation%20Models/#optimization","title":"Optimization","text":"<p>In addition to ensuring our generative models are highly capable, we have used a range of innovative techniques to optimize them on-device and on our private cloud for speed and efficiency. We have applied an extensive set of optimizations for both first token and extended token inference performance.</p> <p>Both the on-device and server models use grouped-query-attention. We use shared input and output vocab embedding tables to reduce memory requirements and inference cost. These shared embedding tensors are mapped without duplications. The on-device model uses a vocab size of 49K, while the server model uses a vocab size of 100K, which includes additional language and technical tokens.</p> <p>For on-device inference, we use low-Bit Palettization, a critical optimization technique that achieves the necessary memory, power, and performance requirements. To maintain model quality, we developed a new framework using LoRA adapters that incorporates a mixed 2-bit and 4-bit configuration strategy \u2014 averaging 3.5 bits-per-weight \u2014 to achieve the same accuracy as the uncompressed models.</p> <p>Additionally, we use an interactive model latency and power analysis tool,\u00a0Talaria, to better guide the bit rate selection for each operation. We also utilize activation quantization and embedding quantization, and have developed an approach to enable efficient Key-Value (KV) cache update on our neural engines.</p> <p>References: Talaria - Interactively Optimizing Machine Learning Models for Efficient Inference Notes: - Might be useful to look at KV Cache hardware-dependency</p> <p>With this set of optimizations, on iPhone 15 Pro we are able to reach time-to-first-token latency of about 0.6 millisecond per prompt token, and a generation rate of 30 tokens per second. Notably, this performance is attained before employing token speculation techniques, from which we see further enhancement on the token generation rate.</p>","tags":["efficient_dl"]},{"location":"100%20Reference%20notes/104%20Other/Introducing%20Apple%E2%80%99s%20On-Device%20and%20Server%20Foundation%20Models/#model-adaptation","title":"Model Adaptation","text":"<p>Our foundation models are fine-tuned for users\u2019 everyday activities, and can dynamically specialize themselves on-the-fly for the task at hand. We utilize adapters, small neural network modules that can be plugged into various layers of the pre-trained model, to fine-tune our models for specific tasks. For our models we adapt the attention matrices, the attention projection matrix, and the fully connected layers in the point-wise feedforward networks for a suitable set of the decoding layers of the transformer architecture.</p> <p>Notes: - How do you adapt the attention matrices? Is it like a bias? `A[i][j] += lora[i][j] - Attention projection matrix I suppose referes to the projection matrices \\(W_Q, W_K, W_V\\)</p> <p>By fine-tuning only the adapter layers, the original parameters of the base pre-trained model remain unchanged, preserving the general knowledge of the model while tailoring the adapter layers to support specific tasks.</p> <p>We represent the values of the adapter parameters using 16 bits, and for the ~3 billion parameter on-device model, the parameters for a rank 16 adapter typically require 10s of megabytes. The adapter models can be dynamically loaded, temporarily cached in memory, and swapped \u2014 giving our foundation model the ability to specialize itself on the fly for the task at hand while efficiently managing memory and guaranteeing the operating system's responsiveness.</p> <p>To facilitate the training of the adapters, we created an efficient infrastructure that allows us to rapidly retrain, test, and deploy adapters when either the base model or the training data gets updated. The adapter parameters are initialized using\u00a0the accuracy-recovery adapter introduced in the Optimization section.</p>","tags":["efficient_dl"]},{"location":"100%20Reference%20notes/104%20Other/Introduction%20to%20Quantization%20on%20PyTorch/","title":"Introduction to Quantization on PyTorch","text":"Properties authors Raghuraman Krishnamoorthi, James Reed, Min Ni, Chris Gottbrath, Seth Weidman year 2020 url https://pytorch.org/blog/introduction-to-quantization-on-pytorch/","tags":["website","efficient_dl","quantization"]},{"location":"100%20Reference%20notes/104%20Other/Introduction%20to%20Quantization%20on%20PyTorch/#notes","title":"Notes","text":"<p>Quantization aware training is typically only used in CNN models when post training static or dynamic quantization doesn\u2019t yield sufficient accuracy. This can occur with models that are highly optimized to achieve small size (such as Mobilenet).</p> <p>Currently, operator coverage is limited and may restrict the choices listed in the table below: The table below provides a guideline.</p> Model Type Preferred scheme Why LSTM/RNN Dynamic Quantization Throughput dominated by compute/memory bandwidth for weights BERT/Transformer Dynamic Quantization Throughput dominated by compute/memory bandwidth for weights CNN Static Quantization Throughput limited by memory bandwidth for activations CNN Quantization Aware Training In the case where accuracy can't be achieved with static quantization <p>Does the Transformer row apply also for vision transformers? Since the number of tokens is quite large.</p> Model Float Latency (ms) Quantized Latency (ms) Inference Performance Gain Device Notes BERT 581 313 1.8x Xeon-D2191 (1.6GHz) Batch size = 1, Maximum sequence length= 128, Single thread, x86-64, Dynamic quantization Resnet-50 214 103 2x Xeon-D2191 (1.6GHz) Single thread, x86-64, Static quantization Mobilenet-v2 97 17 5.7x Samsung S9 Static quantization, Floating point numbers are based on Caffe2 run-time and are not optimized <p>So I should expect something around ~2x latency improvement with dynamic quantization</p>","tags":["website","efficient_dl","quantization"]},{"location":"100%20Reference%20notes/104%20Other/MIT-65940%20-%20TinyML%20and%20Efficient%20Deep%20Learning%20Computing/","title":"MIT 65940   TinyML and Efficient Deep Learning Computing","text":"Properties authors Song Han year 2023 url https://hanlab.mit.edu/courses/2023-fall-65940","tags":["course"]},{"location":"100%20Reference%20notes/104%20Other/Optimizing%20Vision%20Transformer%20Model%20for%20Deployment/","title":"Optimizing Vision Transformer Model for Deployment","text":"Properties authors Jeff Tang, Geeta Chauhan year 2021 url https://pytorch.org/tutorials/beginner/vt_tutorial.html","tags":["website"]},{"location":"100%20Reference%20notes/104%20Other/Quantized%20Transfer%20Learning%20for%20Computer%20Vision%20Tutorial/","title":"Quantized Transfer Learning for Computer Vision Tutorial","text":"Properties authors Zafar Takhirov url https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html","tags":["website"]},{"location":"100%20Reference%20notes/104%20Other/TinyML%20and%20Efficient%20Deep%20Learning%20Computing%20-%20Lecture%2012/","title":"TinyML and Efficient Deep Learning Computing   Lecture 12","text":"Properties authors Song Han year 2023 url https://www.dropbox.com/scl/fi/spgvr9owflz6s1lt5po17/lec12.pdf?rlkey=cwqpteopgvsdgnxd8xtcniadr&amp;e=2&amp;dl=0","tags":["lecture"]},{"location":"100%20Reference%20notes/104%20Other/TinyML%20and%20Efficient%20Deep%20Learning%20Computing%20-%20Lecture%203/","title":"TinyML and Efficient Deep Learning Computing   Lecture 3","text":"Properties authors Song Han year 2023 url https://www.dropbox.com/scl/fi/2oxmtvoeccyuw47yfambb/lec03.pdf?rlkey=3ykm0g21ibsoqn7xnw43v7aaw&amp;e=1&amp;dl=0","tags":["lecture"]},{"location":"100%20Reference%20notes/104%20Other/TinyML%20and%20Efficient%20Deep%20Learning%20Computing%20-%20Lecture%205/","title":"TinyML and Efficient Deep Learning Computing   Lecture 5","text":"Properties authors Song Han year 2023 url https://www.dropbox.com/scl/fi/eos92o2fgys6gk0gizogl/lec05.pdf?rlkey=2hohvi8jcvjw3f8m8vugfa2mz&amp;e=1&amp;dl=0 <p>Content: 1. Reviews numeric datatypes (floating point, etc) 2. Learns basic concept of quantization 3. Introduces three types of common neural network quantization:     - K-Means-based Quantization     - Linear Quantization     - [[Binary and Ternary Quantization|Binary and Ternary Quantization]] (will be covered on Lecture 6)</p>","tags":["lecture"]},{"location":"100%20Reference%20notes/104%20Other/TinyML%20and%20Efficient%20Deep%20Learning%20Computing%20-%20Lecture%206/","title":"TinyML and Efficient Deep Learning Computing   Lecture 6","text":"Properties authors Song Han year 2023 url https://www.dropbox.com/scl/fi/1mo0umu0qtq7uxap2l5m3/lec06.pdf?rlkey=bdl2mgusgajddjuvjxb0fot36&amp;e=2&amp;dl=0 <p>Content: 1. Quantization Granularity     1. Per tensor quantization: same quantization parameters for the entire matrix     2. Per channel quantization: sometimes each channels have considerably different weight distributions, have different quantization parameters per channel/row     3. Group quantization: similar idea 2. Dynamic Range Clipping     - To quantize activations, we must keep track of activations statistics     - Use KL divergence to measure information loss     - Allocating dynamic range to outliers hurts representation ability (see below image)     -  3. Rounding</p> <p>Quantization Aware Training - To minimize the loss of accuracy, especially aggressive quantization with 4 bits and lower bit width, neural network will be trained/fine-tuned with quantized weights and activations. - Usually, fine-tuning a pre-trained floating point model provides better accuracy than training from scratch.</p>","tags":["lecture"]},{"location":"100%20Reference%20notes/104%20Other/TinyML%20and%20Efficient%20Deep%20Learning%20Computing/","title":"TinyML and Efficient Deep Learning Computing","text":"Properties authors Song Han year 2023 url https://hanlab.mit.edu/courses/2023-fall-65940","tags":["course"]},{"location":"100%20Reference%20notes/104%20Other/Tweet%20-%20Stable%20Diffusion%20XL%20on%20iPhone%20with%20Core%20ML%21/","title":"Tweet   Stable Diffusion XL on iPhone with Core ML!","text":"Properties authors Atila Orhon year 2023 url https://x.com/atiorh/status/1707402410870862002 <p>We compressed the diffusion model using our Mixed-Bit Palettization technique (described in https://huggingface.co/blog/stable-diffusion-xl-coreml\u2026) which yields an average of 4.04-bits (5.2GB -&gt; 1.3GB) while maintaining higher accuracy than linear 8-bit quantization. Compressed model runs faster too</p> <p>Notes - 4 times smaller memory footprint - Better than linear 8-bit quantization - Faster inference time</p>","tags":["efficient_dl"]}]}