{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>{{ blog_content }}</p>"},{"location":"000%20Zettelkasten/2D%20Convolutions/","title":"2D Convolutions","text":"<p>Fully comprehensive resource with animations: Conv2d</p>","tags":["cnn"]},{"location":"000%20Zettelkasten/Ahead-of-Time%20%28AOT%29%20Compilation/","title":"Ahead of Time (AOT) Compilation","text":"<p>Generally: Compilation that occurs before the program is executed.</p> <p>Specifically to ML (PyTorch):  - When a model is AOT compiled (using <code>torch.jit.script</code>(or trace) or <code>torch.export</code>), the entire program is translated from python into an intermediate representation that is independent of it. That is, you don't need a python interpreter to run that IR. - Note: torchscript is AOT in the sense that it requires to capture the whole graph before runtime but it performs further optimizations just-in-time.</p>","tags":["compilers","pytorch","optimization"]},{"location":"000%20Zettelkasten/Are%20less%20inductive%20biases%20better%20or%20worse%3F/","title":"Are less inductive biases better or worse?","text":"<p>There's a general consensus that less inductive biases are better, intuitively because it helps optimization by allowing for more hardware-friendly architectures, etc.</p> <p>First, An image is worth 16x16 words - Transformers for image recognition at scale shows that ViTs, with minimal inductive biases, outperform ConvNets. ViTs have: - No translational equivariance baked in - No locality inductive bias enforced     - Although positional encodings exist and fixed sinusoidal encodings can be used, they are mostly learned and randomly/zero initialized. They show that Vision Transformers scale better than ConvNets and Mixed Architectures (Convolutional stems + Transformer).</p> <p>A ConvNet for the 2020s proves that ResNets are outdated and improves the network with recent advances to match ViTs performance. </p> <p>The Lie derivative for measuring learned equivariance shows surprising result: ViTs exhibit more translational equivariance after training than ConvNets, as measured per their Lie Derivative.</p> <p>An Image is Worth More Than 16x16 Patches - Exploring Transformers on Individual Pixels tackles the toy question of dropping the convolutional stem that does the patchification in ViTs, with the intention of further reducing inductive biases. They prove that the resulting model (although too computationally intensive to be used in practice), competes with ViTs.</p> <p>How do vision transformers work? argues that the benefit of Vision Transformers is not that they have less inductive biases, but that the their operations are input dependent (see Input-dependent convolutions) and that Self Attention acts as a smoothing mechanism (that helps with better training dynamics on the large data regimes). They ablate this decision by constraining ViTs attention to be local, outperforming ViTs with global attention both in small and large data regimes. This is a strong indication that locality constraints are useful.  </p> <p>Learning with Unmasked Tokens Drives Stronger Vision Learners implicitly counter-argues How do vision transformers work? by noticing that MIM-trained ViTs exhibit localized attention maps and \"fixing\" it. Their approach outperforms other MIM-trained ViTs, so locality as good inductive bias is not definitely answered.</p> <p>Deep Learning is Not So Mysterious or Different argues for soft inductive bias instead of hard inductive biases (like conv2d's translation equivariance or equivariant models in general).</p> <p>Searching for Efficient Linear Layers over a Continuous Space of Structured Matrices shows scaling laws that indicate that increasing parameter sharing leads to worse scaling.</p> <p>Yi Ma often argues that low dimensionality should be the only inductive bias we need (ref).</p>","tags":["dl_theory","question"]},{"location":"000%20Zettelkasten/Are%20less%20inductive%20biases%20better%20or%20worse%3F/#vits-vs-dense-prediction-tasks","title":"ViTs vs Dense prediction tasks","text":"<p>A ConvNet for the 2020s mentions that ViTs struggle on dense prediction tasks and they require hierarchical architectural choices (Swin Transformer) to do well. These choices re-introduce inductive biases.</p> <p>However, there's recent promising work that is (I think) successfully dropping these constraints: - Exploring Plain Vision Transformer Backbones for Object Detection - SimPLR - A Simple and Plain Transformer for Scaling-Efficient Object Detection and Segmentation</p>","tags":["dl_theory","question"]},{"location":"000%20Zettelkasten/Bit%20Palettization/","title":"Bit Palettization","text":"<p>Seems to be similar to K-Means-based Quantization.</p> <p>[...] we use 6-bit palettization, a type of quantization that compresses model weights from a 16-bit floating-point representation to just 6 bits per parameter. The name \u201cpalettization\u201d refers to a technique similar to the one used in computer graphics to work with a limited set of colors: the color table (or \u201cpalette\u201d) contains a fixed number of colors, and the colors in the image are replaced with the indexes of the closest colors available in the palette. This immediately provides the benefit of drastically reducing storage size, and thus reducing download time and on-device disk use.</p> <p> References: - https://huggingface.co/blog/stable-diffusion-xl-coreml#what-is-mixed-bit-palettization - https://huggingface.co/blog/fast-diffusers-coreml</p> <p>Notes: - Multiplying by this weight matrix intuitively should be slower, it would be interesting to see what is the tradeoff speed vs memory. This tweet Tweet - Stable Diffusion XL on iPhone with Core ML! suggests that it runs faster than the non-quantized alternative.</p>","tags":["efficient_dl","transformers"]},{"location":"000%20Zettelkasten/Block%20Expansion/","title":"Block Expansion","text":"<p>Key idea: - Introduce extra transformer block that is initialized to be the identity function and train that. </p> <p>From Parameter Efficient Fine-tuning of Self-supervised ViTs without Catastrophic Forgetting</p> <p>We introduce the concept of Block Expansion for fine-tuning pre-trained ViTs, building upon an idea that was recently proposed for language models\u00a0[27]\u00a0but has yet to be explored in vision. This technique is used to augment the capacity of a model without altering its initial output. In a ViT model comprised of sequential transformer blocks\u00a0(\\(\\phi_0,\\phi_1,\u2026,\\phi_N\\)), Block Expansion adds an identity block\u00a0(\\(\\phi_{id}\\))\u00a0after a set of transformer blocks such that\u00a0\\(\\phi_{id}(x)=x\\), meaning it returns the input as its output, ensuring the model\u2019s output remains unchanged immediately after expansion. To expand a model from\u00a0\ud835\udc41\u00a0to\u00a0\ud835\udc41\u2032\u00a0blocks, the original blocks are first grouped into sets containing\u00a0\ud835\udc40\u00a0blocks each. Within each set, an identity copy of the topmost block is created and placed on top, effectively increasing the model\u2019s depth without initially changing its behavior. In each newly expanded block, two linear layers are zero-initialized to enable identity mapping, as shown in Figure\u00a01\u00a0(c). These newly added blocks are only fine-tuned with the new data while the remaining blocks are frozen.</p>","tags":["efficient_dl","transformers"]},{"location":"000%20Zettelkasten/Convergence%20rate%20and%20Hessian%20spectra/","title":"Convergence rate and Hessian spectra","text":"<ul> <li>Remember: If a Hessian matrix is positive definite everywhere, then the function is convex =&gt; bad neg eigenvalues</li> <li>Large eigenvalues of the  Metrics for flatness Some metrics, such as the maximum Hessian eigenvalue, measure the worstcase loss increase under an adversarial perturbation to the weights [10, 16], while other proposed metrics, such as the Hessian trace, measure the expected loss increase under random perturbations to the weights.</li> </ul>","tags":["optimizability"]},{"location":"000%20Zettelkasten/Depthwise%20separable%20convolutions/","title":"Depthwise separable convolutions","text":"<p>Splits the computation into two steps:\u00a0depthwise convolution\u00a0applies a single convolutional filter per each input channel and\u00a0pointwise convolution\u00a0is used to create a linear combination of the output of the depthwise convolution.</p> <p>Related ideas are often used to reduce the size/complexity of convolutional layers. It reduces expressivity of convolutions but its less parameters. For example Exploiting Redundancy - Separable Group Convolutional Networks on Lie Groups</p> <p>Also used in (ConvNext) A ConvNet for the 2020s</p> <p></p>","tags":["cnn"]},{"location":"000%20Zettelkasten/Do%20Vision%20Foundation%20models%20exist%3F/","title":"Do Vision Foundation models exist?","text":"","tags":["question","foundation_models","computer_vision"]},{"location":"000%20Zettelkasten/Do%20Vision%20Foundation%20models%20exist%3F/#object-detection","title":"Object detection","text":"<p>Research using DINOv2 as a backbone for object detection:</p> <p>DINOv2 \u274c - Poor Object Detection Performance with DINOv2 Backbone and Faster R-CNN Head on Cityscapes Dataset     - Using mask rcnn head but still relevant, maybe dinov2 is not a good object detection backbone?</p> <p>DINOv2 \u2705</p> <p>\"NVIDIA has also released a foundational model called NV-Dinov2, which is available through the NVIDIA AI Enterprise program. NV-Dinov2 is a visual foundational model trained on an NVIDIA proprietary large scale dataset.\" NV-DINOv2 - NVIDIA provides CLIP VIT and DINO VIT backbones for object detection and segmentation (closed source)     - This signals that it is not only possible but actually useful in production (the tao toolkit specifically markets to providing enterprise-ready vision transformers)     - However it also very specifically states the inferior performance of vits compared with specifically trained dense-prediction networks:         &gt; \"To mitigate the inferior performance of a standard vision transformer (ViT) on dense prediction tasks, TAO supports the\u00a0ViT-Adapter_\u00a0architecture. This allows a powerful ViT that has learned rich semantic representations from a large corpus of data to achieve comparable performance to vision-specific transformers on dense prediction tasks.\"</p> <ul> <li> <p>Exploring Plain Vision Transformer Backbones for Object Detection</p> <ul> <li>VitDET with DINO backbone gh issue<ul> <li>There's some caveats but they are fixable</li> </ul> </li> </ul> </li> <li> <p>SimPLR - A Simple and Plain Transformer for Scaling-Efficient Object Detection and Segmentation</p> <ul> <li>Improves over ViTDet</li> </ul> </li> </ul>","tags":["question","foundation_models","computer_vision"]},{"location":"000%20Zettelkasten/Dynamic%20Resource%20Allocation%20in%20Vision/","title":"Dynamic Resource Allocation in Vision","text":"<p>At the embedding level - 1D tokens (truncating patch suffix (at the sequence dim))     - Principal Components Enable A New Language of Images     - FlexTok - Resampling Images into 1D Token Sequences of Flexible Length - 2D tokens (truncating patch suffixes (at the embed dim))     - Matryoshka Representation Learning     - Franca - Nested Matryoshka Clustering for Scalable Visual Representation Learning</p> <p>At the network level (embed_dim, depth, etc) - EA-ViT - Efficient Adaptation for Elastic Vision Transformer - MatFormer - Nested Transformer for Elastic Inference - HydraViT - Stacking Heads for a Scalable ViT - Slicing Vision Transformer for Flexible Inference</p> <p>At the network level we also have patch pruning and merging methods</p>","tags":["efficient_dl","efficient_vision"]},{"location":"000%20Zettelkasten/Equivariance%20Initialization/","title":"Equivariance Initialization","text":"<p>Related: - Priors over Neural Network weights</p>","tags":["dl_theory"]},{"location":"000%20Zettelkasten/Global%20Precedence%20Effect/","title":"Global Precedence Effect","text":"<p>From: https://www.neurobs.com/manager/content/docs/psychlab101_experiments/Global%20Precedence/description.html</p> <p>How do we process visual information? When we look at someone we know, we could recognize them by identifying individual parts of their face in isolation: the mouth, nose, ears, eyes, and son. But we could also recognize the configuration all the parts: how close their eyes are to their nose, the shape of the face and hairline, the height of their ears, and so on. Navon (1977) was interested in whether we process visual information piece-by-piece or in a more \"global\" holistic sense. To study that, he devised what are now called Navon hierarchical figures. Here's an example of a \"Navon letter\" in which a larger stimulus (a letter) is comprised of smaller stimuli (different letters): { width=\"100\" } The global precedence effect isn't universal. There are situations or tasks where the bias can be reduced, eliminated, or even reversed. For example, some recent research suggests the effect may be culturally specific. Navon's experiments were performed on a largely white, college-age population; since then investigators have found\u00a0stronger\u00a0global precedence effects in East Asian populations than were reported in Navon's study (McKone et al., 2010). Similar cultural differences in visual processing between North American and East Asian cultures have been demonstrated before (e.g., Kitayama, Duffy, Kawamura, &amp; Larsen, 2003), and one theory is that the difference occurs because North American (specifically US) cultures are more individualistic, and East Asian cultures are more collectivist. Presumably because of this cultural difference, the visual system is tuned more to local (i.e., individual) features in the US and more to global (i.e., collective) features in East Asian cultures. Other researchers recently tested an African tribe and found that they demonstrated a\u00a0local\u00a0precedence effect (Davidoff, Fontenenau, &amp; Fagot, 2008). </p> <p>From Principal Components Enable A New Language of Images:</p> <p>Human perception\u00a0of visual stimuli has been shown to follow the global precedence effect\u00a0[32], where the global information of the scene is processed before the local information. In\u00a0[14], controlled experiments of presentation time on human perception of visual scene have further confirmed with the global precedence effect, where less information (presentation time) is needed to access the non-semantic, sensory-related information of the scene compared to the semantically meaningful, object- or scene-related information. Similar results have been reported in\u00a0[3], where sensory attributes are more likely to be processed when the scene is blurred. Moreover,\u00a0[33]\u00a0have suggested that reliable structural information can be quickly extracted based on coarse spatial scale information. These results suggest that human perception of visual stimuli is hierarchical, where the global information of the scene is processed before the local information. </p>","tags":["cognitive-science","psychology"]},{"location":"000%20Zettelkasten/Group%20Axioms/","title":"Group Axioms","text":"<p>A group is a non-empty set \\(G\\) together with a binary operation on \\(G\\) (\\(\\cdot\\)), that fulfills the following axioms: 1. Associativity: For all \\(a, b, c \\in G\\), one has \\((a \\cdot b) \\cdot c = a \\cdot (b \\cdot c)\\) 2. Identity element: There exists an element \\(e\\in G\\) such that, for every \\(a \\in G\\), \\(e \\cdot a = a\\) and \\(a \\cdot e = a\\) 3. Inverse element: For each \\(a\\in G\\), there exists a unique element \\(b\\in G\\) such that \\(a \\cdot b = e\\) and \\(b \\cdot a = e\\), where \\(e\\) is the identity element. The inverse of \\(a\\) is denoted as \\(a^{-1}\\)</p>","tags":["math"]},{"location":"000%20Zettelkasten/Group%20direct%20product/","title":"Group direct product","text":"<p>Given groups \\(G\\) (with operation *) and \\(H\\) (with operation \\(\\Delta\\)), the direct product \\(G \\times H\\) is defined as follows: 1. The underlying set is the Cartesian product, \\(G \\times H\\). That is, the ordered pairs \\((g, h)\\), where \\(g \\in G\\) and \\(h \\in H\\).  2. The binary operation on \\(G \\times H\\) is defined component-wise.</p> \\[  (g_1, h_1) \\cdot (g_2, h_2) = (g_1 * g_2, h_1 \\Delta h_2) \\] <p>The resulting algebraic object satisfies the Group Axioms.</p>","tags":["math"]},{"location":"000%20Zettelkasten/Hardware-specific%20structured%20pruning/","title":"Hardware specific structured pruning","text":"<p>Key Idea</p> <p>Some GPU architectures can take advantage of specific sparsity patterns.</p> <p>According to this the training procedure would look as follows:</p> <p>NVIDIA has developed a simple and universal recipe for sparsifying deep neural networks for inference\u00a0using this 2:4 structured sparsity pattern. The network is first trained using dense weights, then fine-grained structured pruning is applied, and finally the remaining non-zero weights are fine-tuned with additional training steps. This method results in virtually no loss in inferencing accuracy based on evaluation across dozens of networks spanning vision, object detection, segmentation, natural language modeling, and translation.</p> <p> References: - TinyML and Efficient Deep Learning Computing - Lecture 3 - https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/ - https://developer.nvidia.com/blog/structured-sparsity-in-the-nvidia-ampere-architecture-and-applications-in-search-engines/</p>","tags":["efficient_dl","hardware_aware_dl"]},{"location":"000%20Zettelkasten/Input-dependent%20convolutions/","title":"Input dependent convolutions","text":"<ul> <li>How do vision transformers work? states that the key advantage of Self Attention over Convolutions is not the long range dependencies (global attention) but rather its data specificity (aka input dependency)</li> <li>This is related to Mamba - Linear-Time Sequence Modeling with Selective State Spaces's insight :<ul> <li>\"We identify a key limitation of prior models: the ability to efficiently select data in an input-dependent manner (i.e. focus on or ignore particular inputs).\"</li> </ul> </li> </ul> <p>There most likely is work on input-dependent convolutions: - [ ] CKConv - Continuous Kernel Convolution For Sequential Data is probably related, but haven't read it in full.  Check this. - [ ] Review literature on input-dependent convolutions</p>","tags":["cnn","theory"]},{"location":"000%20Zettelkasten/K-Means-based%20Quantization/","title":"K Means based Quantization","text":"<p>Perform clustering on weights, and replace weights with cluster <code>int</code> index matrix (to which cluster each weight entry belongs to) and a list of <code>float</code> centroids.</p> <p>Storing integers consumes less memory while you can keep fully precision on the float centroids (although you lose precision because it does not necessarily correspond to an actual value in the previous weight matrix).</p> <p>Resources: - https://scikit-learn.org/stable/auto_examples/cluster/plot_color_quantization.html - TinyML and Efficient Deep Learning Computing - Lecture 5 </p>","tags":["efficient_dl"]},{"location":"000%20Zettelkasten/KV%20Cache/","title":"KV Cache","text":"<p> From: TinyML and Efficient Deep Learning Computing - Lecture 12</p>","tags":["efficient_dl","transformers"]},{"location":"000%20Zettelkasten/Linear%20Quantization/","title":"Linear Quantization","text":"<p>Visualization </p> <p>Then, for each layer in your network (linear, conv, etc), you represent the matrices involved like the previous formulation, do some arithmetic to see what you can precompute and zero-out and voil\u00e1</p>","tags":["efficient_dl"]},{"location":"000%20Zettelkasten/LoRa%20Adapter/","title":"LoRa Adapter","text":"<p> Image source: https://medium.com/@bnjmn_marie/lora-load-and-merge-your-adapters-with-care-3204119f0426</p>","tags":["efficient_dl","transformers"]},{"location":"000%20Zettelkasten/Masked%20Image%20Modelling/","title":"Masked Image Modelling","text":"<p>It seems like MIM objectives are becoming a strong learning objective for vision foundation models. Right now it seems to be the closest answer to: Do Vision Foundation models exist?</p> <p>However, intuitively it seems a bit like a weak signal, as it focuses on individual patches/pixels, without much consideration to semantic information. This is echoed on Learning with Unmasked Tokens Drives Stronger Vision Learners:</p> <p>However, MIM strategies often encounter challenges, such as local dependency on attention to understand entire context of an image. For example, liu\u00a0et al.\u00a0[36]\u00a0revealed that MAE\u00a0[22], a state-of-the-art MIM method, exhibits shorter average attention distances. Furthermore, we observe that attention map patterns by MAE substantiate extremely local behavior (See Fig.\u00a01) indeed. In other words, the MAE-trained attention mechanism less integrates information across the entire image pixels and tends to focus on specific input regions. This is presumably attributed to MIM-pretraining, primarily dedicated to predicting low-level pixel details (e.g., color or texture) without a comprehensive understanding of less-regional information (e.g., the input structure or shape).</p> <p>Related papers: - Learning with Unmasked Tokens Drives Stronger Vision Learners - DINOv2 - Learning Robust Visual Features without Supervision - Learning with Unmasked Tokens Drives Stronger Vision Learners - What Do Self-Supervised Vision Transformers Learn? \ud83d\udea8</p>","tags":["foundation_models","computer_vision"]},{"location":"000%20Zettelkasten/Masked%20Image%20Modelling/#register-tokens","title":"Register tokens?","text":"<p>Vision Transformers Need Registers observe that there are no high-norm artifacts that would justify adding registers and claims this is because the model only ises local information.</p>","tags":["foundation_models","computer_vision"]},{"location":"000%20Zettelkasten/Maximal%20pruning%20and%20functional%20recovery/","title":"Maximal pruning and functional recovery","text":"<p>Key Idea</p> <p>You can iteratively prune and finetune the network weights and still maintain performance up to some pruning ratio.</p> <p> Reference: - TinyML and Efficient Deep Learning Computing - Lecture 3 - Learning both Weights and Connections for Efficient Neural Networks</p>","tags":["dl_theory","efficient_dl"]},{"location":"000%20Zettelkasten/Mean%20Attention%20Distance/","title":"Mean Attention Distance","text":"<p>Introduced in An image is worth 16x16 words - Transformers for image recognition at scale.</p> <p>From What Do Self-Supervised Vision Transformers Learn?</p> <p>\u201cAttention distance is defined as the average distance between the query tokens and key tokens considering their self-attention weights. Therefore, it conceptually corresponds to the size of the receptive fields in CNNs.\u201d (Park et al., 2023, p. 3)</p> <p>Key Observation</p> <p>Can be used to measure what is the how much local or global information is a transformer using. See What Do Self-Supervised Vision Transformers Learn?.</p>","tags":["dl_theory","transformers"]},{"location":"000%20Zettelkasten/Multiple%20global%20minima/","title":"Multiple global minima","text":"<p>We expect loss functions for deep networks to have a large family of equivalent global minima.</p> <ul> <li>Fully connected networks: permutation of the hidden units</li> <li>Convolutional networks: permuting the channels and convolution kernels appropriately.</li> <li>...</li> </ul> <p>The above modifications all produce the same output for every input. However, the global minimum only depends on the output at the training data points. </p> <p>In overparameterized networks, there will also be families of solutions that behave identically at the data points but differently between them. All of these are also global minima.</p> <p>References: - Understanding Deep Learning - Chapter 20 (20.3.1)</p>","tags":["optimizability","dl_theory"]},{"location":"000%20Zettelkasten/Neural%20Network%20Quantization/","title":"Neural Network Quantization","text":"<p>Related: - HuggingFace Docs - A survey of quantization methods for efficient neural network inference - A recent (2024) work by Han et al: AWQ - Activation-aware Weight Quantization for LLM Compression and Acceleration</p>","tags":["quantization","efficient_dl"]},{"location":"000%20Zettelkasten/Non-translationally%20equivariant%20convolutions/","title":"Non translationally equivariant convolutions","text":"<p>I'm not sure if this makes sense at all, just tracking paper ideas lmao</p> <p>See: - Input-dependent convolutions - How do vision transformers work?</p>","tags":["cnn","convolutions","equivariance","partial_equivariance"]},{"location":"000%20Zettelkasten/Positive%20Logic%20Programs/","title":"Positive Logic Programs","text":"","tags":["knowledge_representation"]},{"location":"000%20Zettelkasten/Positive%20Logic%20Programs/#positive-logic-programs","title":"Positive logic programs","text":"<p>Two components:  1. Facts:  <code>a.</code> 2. Rules:  <code>a :- b, c, d</code> , which is the same as <code>b \u2227 c \u2227 d \u2192 a</code></p> <p>This is a positive logic program: <pre><code>rainy(amsterdam).\nrainy(vienna).\nwet(X) :- rainy(X). # eq: \u2200x. (Rainy(x) \u2192 Wet(x))\n</code></pre></p>","tags":["knowledge_representation"]},{"location":"000%20Zettelkasten/Positive%20Logic%20Programs/#database-semantics","title":"Database semantics","text":"<p>Assumptions 1. Domain closure: The objects mentioned are the only objects. 2. Unique-names assumption: Two variables can't refer to the same object 3. Closed-world assumption: Whatever we don't know is false</p> What does the database semantics allow us to do? <ol> <li>We can specify a relation by the set of inputs that are true</li> <li>We can specify objects simply by the terms that point to them</li> <li>We don't have to explicitly define what function symbols mean</li> </ol> <p>Thus, an interpretation is a set that defines which atoms are true. The remainder are false.</p>","tags":["knowledge_representation"]},{"location":"000%20Zettelkasten/Positive%20Logic%20Programs/#models","title":"Models","text":"What is a model? <p>A model is an interpretation which makes all rules of a program true.</p> <p>However, we're not interested in all models, we want the highest expressivity at the lowest information.</p> What is the definition of a minimal model? <p>A model is minimal if no strict subset exist that is also a model.</p> How do you construct a minimal model? <p>Start with facts and add new literals that are on the lhs of a rule where all body is in M. <pre><code>M = {f for f in facts}\nwhile True:\n    for head, body in rules:\n        if all(l in M for l in body):\n            M.add(l)\n</code></pre></p> What is the definition of a supported model? <p>A model is supported if all its atoms are supported. An atom of a model is supported if it appears as a head where the body is true.</p> What properties does minimal models and supported models have for positive logic programs? <p>For positive logic programs:</p> <ul> <li>Minimal models are unique</li> <li>A minimal model is also a supported model (but not necessarily viceversa)</li> </ul>","tags":["knowledge_representation"]},{"location":"000%20Zettelkasten/Positive%20Logic%20Programs/#normal-logic-programs","title":"Normal logic programs","text":"<p>Now we allow negation.</p> <pre><code>a :- b_1, ..., b_n, not c_1, ..., not c_m.\n</code></pre> Do properties of minimal models for PL still hold for NL? Why? <p>No, negation removes allows for non-uniqueness of minimal models.</p>","tags":["knowledge_representation"]},{"location":"000%20Zettelkasten/Priors%20over%20Neural%20Network%20weights/","title":"Priors over Neural Network weights","text":"<p>From Understanding Deep Learning - Chapter 10, 1d convolutions can be represented as weight matrices from a MLP with a specific prior where the diagonals are the same (d). </p> <p>Rotationally equivariant convolutions can be implemented by isotropic filters (a prior on the conv2d weight): </p>","tags":["dl_theory","equivariance"]},{"location":"000%20Zettelkasten/PyTorch%20Functionalization/","title":"PyTorch Functionalization","text":"<p>Given a program/function of PyTorch operators, functionalization will return a new function, that: 1. Has the same semantics as the old function 2. Has no mutations in it</p> <p>Functionalization operates at the level of our ATen API.</p> <p>More info on PyTorch - Functionalization in PyTorch - Everything you need to know</p>","tags":["pytorch","compilers"]},{"location":"000%20Zettelkasten/PyTorch%20Quantization%20for%20TensorRT/","title":"PyTorch Quantization for TensorRT","text":"<p>There seems to be quite a few possible ways to do this: - PyTorch Eager Mode Quantization TensorRT Acceleration , seems a bit cumbersome:     1. torchao quantization      2. ONNX conversion     3. Graph Surgery (changing some ops in the onnx graph)     4. tensorrt conversion - Not sure if it works, but would be ideal     1. torch.export     2. torchao quantization     3. tensorrt conversion - Less ideal would be:     1. torchao quantization     2. torch.export     3. tensorrt conversion     - I've already sort of tried this using the vgg ptq example from tensorrt, but torch.export complained that it couldn't translate the quantized operations</p>","tags":["quantization","efficient_dl"]},{"location":"000%20Zettelkasten/Rate%20Distortion%20and%20Spectral%20Analysis%20on%20Representations/","title":"Rate Distortion and Spectral Analysis on Representations","text":"<p>This note gathers papers that use concepts from information theory and spectral theory for deep learning.</p>","tags":["dl_theory","information_theory","spectral"]},{"location":"000%20Zettelkasten/Rate%20Distortion%20and%20Spectral%20Analysis%20on%20Representations/#hierarchical-tokenization-for-images-also-relates-to-global-precedence-effect","title":"Hierarchical Tokenization for images (also relates to Global Precedence Effect)","text":"<ul> <li>FlexTok - Resampling Images into 1D Token Sequences of Flexible Length. FlexTok converts an image into a variable-length, ordered 1-D token sequence that preserves hierarchical semantics and allows bitrate-adaptive reconstruction.</li> <li>Principal Components Enable A New Language of Images. Embeds a provable PCA-like basis into visual tokens, yielding structured and interpretable image representations that boost downstream performance.</li> </ul>","tags":["dl_theory","information_theory","spectral"]},{"location":"000%20Zettelkasten/Rate%20Distortion%20and%20Spectral%20Analysis%20on%20Representations/#other-non-linear-tokenizations","title":"Other non-linear tokenizations","text":"<ul> <li>Byte Latent Transformer - Patches Scale Better Than Tokens #entropy \u2014 BLT demonstrates that scaling vision models with raw byte-level patches outperforms fixed-token approaches at equal compute. </li> </ul>","tags":["dl_theory","information_theory","spectral"]},{"location":"000%20Zettelkasten/Rate%20Distortion%20and%20Spectral%20Analysis%20on%20Representations/#coding-rate","title":"Coding Rate","text":"<ul> <li>[[White-Box Transformers via Sparse Rate Reduction - Compression Is All There Is|White-Box Transformers via Sparse Rate Reduction - Compression Is All There Is]] \u2014 Frames representation learning as sparse rate reduction toward mixtures of low-dimensional Gaussians, yielding transparent, theoretically grounded transformer layers. </li> <li>Simplifying DINO via Coding Rate Regularization \u2014 Shows that adding a coding-rate loss term stabilizes and simplifies DINO, removing most heuristics while improving robustness and accuracy.<ul> <li>Both use coding rate, which is differential #entropy under a Gaussian source and serves as an upper bound on true differential entropy for real-valued vectors.</li> </ul> </li> </ul>","tags":["dl_theory","information_theory","spectral"]},{"location":"000%20Zettelkasten/Rate%20Distortion%20and%20Spectral%20Analysis%20on%20Representations/#other","title":"Other","text":"<ul> <li>https://sander.ai/2024/09/02/spectral-autoregression.html \u2014 Argues that diffusion models implement approximate autoregression in the frequency domain, unifying diffusion and autoregressive viewpoints. </li> <li>Rethinking Lossy Compression - The Rate-Distortion-Perception Tradeoff \u2014 Establishes a fundamental three-way trade-off showing that enforcing high perceptual quality raises the achievable rate-distortion curve.</li> <li>Rate\u2013Distortion\u2013Perception Trade-Off in Information Theory, Generative Models, and Intelligent Communications \u2014 Extends the RDP framework, highlighting how generative models can achieve perceptually optimized communication at additional rate cost. </li> <li>Average entropy of Gaussian mixtures \u2014 Provides an analytic series expansion for the differential entropy of Gaussian mixtures, supplying tighter bounds useful for coding-rate objectives. </li> <li>Matryoshka Representation Learning \u2014 Introduces nested \u201cdoll\u201d embeddings that flexibly trade compute for accuracy, adapting a single representation to diverse downstream tasks. </li> <li>Learning Continually by Spectral Regularization \u2014 Maintains network plasticity in continual learning by constraining each layer\u2019s largest singular value near one, preserving gradient diversity. </li> <li>Towards Understanding the Spectral Bias of Deep Learning \u2014 Provides a theoretical explanation linking NTK eigenvalues to faster learning of low-frequency functions, illuminating spectral bias. </li> </ul> <p>PS: Personally curated list. (1-sentence summaries by o3 :p). </p>","tags":["dl_theory","information_theory","spectral"]},{"location":"000%20Zettelkasten/Representation%20%28Group%20Theory%29/","title":"Representation (Group Theory)","text":"<p>Property required:</p> \\[ p(g)p(h) = p(g \\cdot h) \\] <p>A representation of a group action can be a linear operator like:</p> \\[ p(\\theta) = [sin(\\theta) ...] \\]","tags":["math","group_theory"]},{"location":"000%20Zettelkasten/Residual%20stream/","title":"Residual stream","text":"<p>\"A transformer\u00a0starts with a token embedding, followed by a series of \u201cresidual blocks\u201d, and finally a token unembedding. Each residual block consists of an attention layer, followed by an MLP layer. Both the attention and MLP layers each \u201cread\u201d their input from the residual stream (by performing a linear projection), and then \u201cwrite\u201d their result to the residual stream by adding a linear projection back in.\u00a0Each attention layer consists of multiple heads, which operate in parallel.\" A Mathematical Framework for Transformer Circuits</p> <p></p>","tags":["mechinterp","transformers"]},{"location":"000%20Zettelkasten/SK%20Centering/","title":"SK Centering","text":"<p>In DINOv2, the original DINO \u201csoftmax\u2011centering\u201d\u2014where the teacher\u2019s logits are calibrated by subtracting an exponential\u2011moving\u2011average (EMA) \u201ccenter\u201d vector before softmax\u2014is replaced by an iterative batch\u2011level normalization using the Sinkhorn\u2013Knopp (SK) algorithm from SwAV. This SK \u201ccentering\u201d enforces a doubly\u2011stochastic assignment matrix (uniform marginals over both samples and prototypes), thereby centering the distribution to a uniform prior in an optimal\u2011transport sense rather than by EMA subtraction</p>","tags":["contrastive_learning"]},{"location":"000%20Zettelkasten/Should%20we%20separate%20semantics%20and%20pose%3F/","title":"Should we separate semantics and pose?","text":"<p>There's been work in the past to attempt to separate semantics and pose information from representations. Generally speaking there is a bunch of literature on equivariant neural networks and [[Equivariant SSL|Equivariant SSL]] but usually restrict themselves to specific transformations, etc.</p> <p>One somewhat recent paper that attempts to find a more general way to encode equivariant representations is SIE where they split the representation in an invariant part and an equivariant part. The invariant part is supposed to encode semantics while the equivariant part encodes pose information (in particular rotations in this paper if I recall). They find that such a separation is a bit noisy (sometimes you need the semantics to understand the pose and viceversa) but that might be implementation dependent? </p> <p>Overall, it loosely seems that works that try to separate semantics and pose tend to be a bit brittle and not super general. Although of course the idea is appealing.</p> <p>Somewhat related, there has been very recent work (2025) that has explicitly tried to remove positional information from their representations in order to keep \"purer\" semantic representations. For example:  Franca trains a linear classifier on top of patch representations that predicts the absolute position and then orthogonalizes the patch representations with respect to the classifier weights. This is supposed to remove positional information from the patch representations. Into the Rabbit Hull does the exact same thing but moreso to analyze if locality in patch features stems from the positional encodings (tldr: if they remove the pos, it still yields locally connected embs). </p> <p>So in a way there is a case for separating semantics and pose, or more so remove the pose altogether and keep semantics (or at least absolute position). This might make sense in semantic segmentation tasks of image models, but what about video models which can leverage 3d information? </p>","tags":["computer_vision","dl_theory","representation_learning","inductive_bias"]},{"location":"100%20Reference%20notes/101%20Literature/4M%20-%20Massively%20Multimodal%20Masked%20Modeling/","title":"4M   Massively Multimodal Masked Modeling","text":"Properties authors David Mizrahi, Roman Bachmann, O\u011fuzhan Fatih Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, Amir Zamir year 2023 url http://arxiv.org/abs/2312.06647 <p>Abstract</p> <p>Current machine learning models for vision are often highly specialized and limited to a single modality and task. In contrast, recent large language models exhibit a wide range of capabilities, hinting at a possibility for similarly versatile models in computer vision. In this paper, we take a step in this direction and propose a multimodal training scheme called 4M. It consists of training a single unified Transformer encoder-decoder using a masked modeling objective across a wide range of input/output modalities \u2013 including text, images, geometric, and semantic modalities, as well as neural network feature maps. 4M achieves scalability by unifying the representation space of all modalities through mapping them into discrete tokens and performing multimodal masked modeling on a small randomized subset of tokens. 4M leads to models that exhibit several key capabilities: (1) they can perform a diverse set of vision tasks out of the box, (2) they excel when fine-tuned for unseen downstream tasks or new input modalities, and (3) they can function as a generative model that can be conditioned on arbitrary modalities, enabling a wide variety of expressive multimodal editing capabilities with remarkable flexibility.</p>","tags":["paper","multimodal"]},{"location":"100%20Reference%20notes/101%20Literature/4M%20-%20Massively%20Multimodal%20Masked%20Modeling/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","multimodal"]},{"location":"100%20Reference%20notes/101%20Literature/A%20Brief%20Review%20of%20Hypernetworks%20in%20Deep%20Learning/","title":"A Brief Review of Hypernetworks in Deep Learning","text":"Properties authors Vinod Kumar Chahuan, Jiandong Zhou, Ping Lu, Soheila Molaei, David A. Clifton year 2023 url https://arxiv.org/abs/2306.06955 <p>Abstract</p> <p>Hypernetworks, or hypernets in short, are neural networks that generate weights for another neural network, known as the target network. They have emerged as a powerful deep learning technique that allows for greater flexibility, adaptability, dynamism, faster training, information sharing, and model compression etc. Hypernets have shown promising results in a variety of deep learning problems, including continual learning, causal inference, transfer learning, weight pruning, uncertainty quantification, zero-shot learning, natural language processing, and reinforcement learning etc. Despite their success across different problem settings, currently, there is no review available to inform the researchers about the developments and to help in utilizing hypernets. To fill this gap, we review the progress in hypernets. We present an illustrative example to train deep neural networks using hypernets and propose categorizing hypernets based on five design criteria as inputs, outputs, variability of inputs and outputs, and architecture of hypernets. We also review applications of hypernets across different deep learning problem settings, followed by a discussion of general scenarios where hypernets can be effectively employed. Finally, we discuss the challenges and future directions that remain under-explored in the field of hypernets. We believe that hypernetworks have the potential to revolutionize the field of deep learning. They offer a new way to design and train neural networks, and they have the potential to improve the performance of deep learning models on a variety of tasks. Through this review, we aim to inspire further advancements in deep learning through hypernetworks.</p>","tags":["paper","hypernetworks"]},{"location":"100%20Reference%20notes/101%20Literature/A%20ConvNet%20for%20the%202020s/","title":"A ConvNet for the 2020s","text":"Properties authors Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie year 2022 url https://arxiv.org/abs/2201.03545 <p>Abstract</p> <p>The \"Roaring 20s\" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually \"modernize\" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.</p>","tags":["cnn","foundation_models","computer_vision","dl_theory","paper"]},{"location":"100%20Reference%20notes/101%20Literature/A%20ConvNet%20for%20the%202020s/#notes","title":"Notes","text":"<p>Authors modernize ConvNets with SOTA architectural choices and training recipes to achieve SOTA ViT performance on dense prediction tasks (Object Detection, etc). { width=\"500\" }</p> <p>Important limitation, scaling laws for ConvNext are not proved to be as good as ViTs, although they also mention that they are promising:</p> <p>These findings are encouraging but not yet completely convincing \u2014 our exploration thus far has been limited to a small scale, but vision Transformers\u2019 scaling behavior is what truly distinguishes them.</p> <p>Table 1. Classification accuracy on ImageNet-1K. Similar to Transformers, ConvNeXt also shows promising scaling behavior with higher-capacity models and a larger (pre-training) dataset.</p> <ul> <li> What are the follow ups for this paper regarding scaling laws of modern convnets when compared to vits?</li> </ul> <p>One of the main motivations of this paper is that ViTs were not very good at dense prediction tasks such as object detection:</p> <p>A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks.</p>","tags":["cnn","foundation_models","computer_vision","dl_theory","paper"]},{"location":"100%20Reference%20notes/101%20Literature/A%20Cookbook%20of%20Self-Supervised%20Learning/","title":"A Cookbook of Self Supervised Learning","text":"Properties authors Randall Balestriero, Mark Ibrahim, Vlad Sobal, Ari Morcos, Shashank Shekhar, Tom Goldstein, Florian Bordes, Adrien Bardes, Gregoire Mialon, Yuandong Tian, Avi Schwarzschild, Andrew Gordon Wilson, Jonas Geiping, Quentin Garrido, Pierre Fernandez, Amir Bar, Hamed Pirsiavash, Yann LeCun, Micah Goldblum year 2023 url http://arxiv.org/abs/2304.12210 <p>Abstract</p> <p>Self-supervised learning, dubbed the dark matter of intelligence, is a promising path to advance machine learning. Yet, much like cooking, training SSL methods is a delicate art with a high barrier to entry. While many components are familiar, successfully training a SSL method involves a dizzying set of choices from the pretext tasks to training hyper-parameters. Our goal is to lower the barrier to entry into SSL research by laying the foundations and latest SSL recipes in the style of a cookbook. We hope to empower the curious researcher to navigate the terrain of methods, understand the role of the various knobs, and gain the know-how required to explore how delicious SSL can be.</p>","tags":["paper","ssl","dense_ssl","dl_theory","representation_learning"]},{"location":"100%20Reference%20notes/101%20Literature/A%20Cookbook%20of%20Self-Supervised%20Learning/#notes","title":"Notes","text":"<p>Zotero Link</p> <p>\u201c3. Learning spatial context: This category of methods trains a model to understand the relative positions and orientations of objects within a scene. RotNet [Gidaris et al., 2018] masks the direction of gravity by applying a random rotation and then asks the model to predict the rotation. Doersch et al. [2015] is one of the first SSL methods that simply predicts the relative location of two randomly sampled patches in an image. This strategy was superseded by \u201cjigsaw\u201d methods [Pathak et al., 2016, Noroozi et al., 2018] that break an image into an array of disjoint patches and predict the relative location of each.\u201d (Balestriero et al., 2023, p. 5)</p> <p>\u201cSeveral works have aimed at understanding how BYOL and SimSiam avoid collapse such as Tian et al. [2021] or Halvagal et al. [2022], where they found that the asymmetry between the two branches is the key, as well the training dynamics which regularize the variance of the embeddings implicitly\u201d (Balestriero et al., 2023, p. 12)</p> <p>\u201cThe SSL canonical correlation analysis family originates with the Canonical Correlation Framework (CCA) [Hotelling, 1992]. The high-level goal of CCA is to infer the relationship between two variables by analyzing their cross-covariance matrices.\u201d (Balestriero et al., 2023, p. 13)</p> <p>\u201cVICReg, the most recent among these methods, balances three objectives based on co-variance matrices of representations from two views: variance, invariance, covariance shown in Figure 6. Regularizing the variance along each dimension of the representation prevents collapse, the invariance ensures two views are encoded similarly, and the co-variance encourages different dimensions of the representation to capture different features.\u201d (Balestriero et al., 2023, p. 14)</p> <p>\u201cJing et al. [2022] study the role of linear projectors in contrastive learning. More precisely, it is argued that the projector prevents dimensional collapse in the representation space and that it only needs to be diagonal and low-rank to do so. Although the proposed method without a projector outperforms SimCLR with a one layer linear projector, for 2- and 3-MLP projectors, performance remains out of reach. Cosentino et al. [2022] study the interplay of the projector and data augmentations when the augmentations are Lie group transformations, and, as Mialon et al. [2022], provide an explanation on the effect of width and depth of the projector.\u201d (Balestriero et al., 2023, p. 18)</p> <p>\u201cIt is worth noting that perfect invariance is not achieved thanks to the projector Guillotine Regularization - Why removing layers is needed to improve generalization in Self-Supervised Learning, which helps improve performance on tasks which are not entirely invariant.\u201d (Balestriero et al., 2023, p. 21)</p> <p>\u201cGuillotine Regularization - Why removing layers is needed to improve generalization in Self-Supervised Learning show that adding a projector is not only useful for SSL but is also highly beneficial in a supervised training setting when there is a misalignment between the training and downstream tasks (which was also demonstrated by Sariyildiz et al. [2022])\u201d (Balestriero et al., 2023, p. 23)</p> <p>\u201cIn some cases, enforcing invariance over two very different views might be a very strong constraint that could harm the performance, like when the content of the two views is different.\u201d (Balestriero et al., 2023, p. 24)</p> <p>\u201cSimilarly to how large batch sizes were seen as a requirement for contrastive methods, a large output dimension of the projector was seen as a requirement for covariance based methods.\u201d (Balestriero et al., 2023, p. 24)</p> <p>This is also not the case for SimDINO, it succesfully removes that last projection.</p> <p>\u201cWhile VICReg stays more sensitive to the output dimension of the projector than SimCLR, it is significantly more robust than originally thought and very large output dimensions are not a requirement\u201d (Balestriero et al., 2023, p. 24)</p> <p>\u201cFurther, Zhao et al. [2021] argue that self-supervised learners also lack localization information because the models are able to use all parts of the image, both foreground and background, to make their predictions.\u201d (Balestriero et al., 2023, p. 43)</p> <p>\u201cInterestingly, older pretext tasks such as jigsaw or colorization, which predate the recent SSL craze sparked by MoCo and SimCLR, can also achieve competitive performance compared to supervised learning backbones when the pretext task is made \u201chard\u201d enough Scaling and Benchmarking Self-Supervised Visual Representation Learning\u201d (Balestriero et al., 2023, p. 43)</p> <p>\u201cMore recently, VICRegL - Self-Supervised Learning of Local Visual Features applies a similar principle by combining geometric and learned matching, with a non-contrastive criterion\u201d (Balestriero et al., 2023, p. 44)</p>","tags":["paper","ssl","dense_ssl","dl_theory","representation_learning"]},{"location":"100%20Reference%20notes/101%20Literature/A%20Hierarchy%20of%20Graph%20Neural%20Networks%20Based%20on%20Learnable%20Local%20Features/","title":"A Hierarchy of Graph Neural Networks Based on Learnable Local Features","text":"Properties authors Michael Linghzhi Li, Meng Dong, Jiawei Zhou, Alexander M. Rush year 2019 url https://arxiv.org/abs/1911.05256 <p>Abstract</p> <p>Graph neural networks (GNNs) are a powerful tool to learn representations on graphs by iteratively aggregating features from node neighbourhoods. Many variant models have been proposed, but there is limited understanding on both how to compare different architectures and how to construct GNNs systematically. Here, we propose a hierarchy of GNNs based on their aggregation regions. We derive theoretical results about the discriminative power and feature representation capabilities of each class. Then, we show how this framework can be utilized to systematically construct arbitrarily powerful GNNs. As an example, we construct a simple architecture that exceeds the expressiveness of the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theory on both synthetic and real-world benchmarks, and demonstrate our example's theoretical power translates to strong results on node classification, graph classification, and graph regression tasks.</p> <p>Interesting insight: - \u201cUsing this hierarchy, we can derive theoretical results which provide insight into GNNs. For example, we show that no matter how many layers are added, networks which only aggregate over immediate neighbors cannot learn the number of triangles in a node\u2019s neighbourhood\u201d (Li et al., 2019, p. 1) </p> <p>HOWEVER: - you can bypass this by encoding geometric information like position and orientation, see Fast, Expressive SE(n) Equivariant Networks through Weight-Sharing in Position-Orientation Space slides</p> <p>Michael Lingzhi Li,\u00a0Meng Dong,\u00a0Jiawei Zhou,\u00a0Alexander M. Rush</p>","tags":["gcn","graphs","gnn","paper"]},{"location":"100%20Reference%20notes/101%20Literature/A%20Mathematical%20Framework%20for%20Transformer%20Circuits/","title":"A Mathematical Framework for Transformer Circuits","text":"Properties authors Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, Christopher Olah year 2021 url https://transformer-circuits.pub/2021/framework/index.html <p>Abstract</p> <p>Transformer [1] language models are an emerging technology that is gaining increasingly broad real-world use, for example in systems like GPT-3 [2], LaMDA\u00a0[3], Codex\u00a0[4], Meena\u00a0[5], Gopher\u00a0[6], and similar models. \u00a0However, as these models scale, their open-endedness and high capacity creates an increasing scope for unexpected and sometimes harmful behaviors. \u00a0Even years after a large model is trained, both creators and users routinely discover model capabilities \u2013 including problematic behaviors \u2013 they were previously unaware of.</p> <p>One avenue for addressing these issues is\u00a0mechanistic interpretability, attempting to reverse engineer the detailed computations performed by transformers, similar to how a programmer might try to reverse engineer complicated binaries into human-readable source code. \u00a0If this were possible, it could potentially provide a more systematic approach to explaining current safety problems, identifying new ones, and perhaps even anticipating the safety problems of powerful future models that have not yet been built. \u00a0A previous project, the\u00a0Distill\u00a0Circuits\u00a0thread\u00a0[7], has attempted to reverse engineer vision models, but so far there hasn\u2019t been a comparable project for transformers or language models. </p> <p>In this paper, we attempt to take initial, very preliminary steps towards reverse-engineering transformers. \u00a0Given the incredible complexity and size of modern language models, we have found it most fruitful to start with the simplest possible models and work our way up from there. \u00a0Our aim is to discover simple algorithmic patterns, motifs, or frameworks that can subsequently be applied to larger and more complex models. \u00a0Specifically, in this paper we will study\u00a0transformers with two layers or less which have only attention blocks\u00a0\u2013 this is in contrast to a large, modern transformer like GPT-3, which has 96 layers and alternates attention blocks with MLP blocks.</p> <p>We find that by conceptualizing the operation of transformers in a new but mathematically equivalent way, we are able to make sense of these small models and gain significant understanding of how they operate internally. \u00a0Of particular note, we find that specific attention heads that we term \u201cinduction heads\u201d can explain in-context learning in these small models, and that these heads only develop in models with at least two attention layers. \u00a0We also go through some examples of these heads operating in action on specific data.</p> <p>We don\u2019t attempt to apply to our insights to larger models in this first paper, but in a\u00a0forthcoming paper, we will show that both our mathematical framework for understanding transformers, and the concept of induction heads, continues to be at least partially relevant for much larger and more realistic models \u2013 though we remain a very long way from being able to fully reverse engineer such models.</p>","tags":["paper","mechinterp","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/A%20general%20theory%20of%20correct%2C%20incorrect%2C%20and%20extrinsic%20equivariance/","title":"A general theory of correct, incorrect, and extrinsic equivariance","text":"Properties authors Dian Wang, Xupeng Zhu, Jung Yeon Park, Mingxi Jia, Guanang Su, Robert Platt, Robin Walters year 2024 url https://proceedings.neurips.cc/paper_files/paper/2023/hash/7dc7793c89b93887e126a86f22ef63c6-Abstract-Conference.html <p>Abstract</p> <p>Although equivariant machine learning has proven effective at many tasks, success depends heavily on the assumption that the ground truth function is symmetric over the entire domain matching the symmetry in an equivariant neural network. A missing piece in the equivariant learning literature is the analysis of equivariant networks when symmetry exists only partially in the domain. In this work, we present a general theory for such a situation. We propose pointwise definitions of correct, incorrect, and extrinsic equivariance, which allow us to quantify continuously the degree of each type of equivariance a function displays. We then study the impact of various degrees of incorrect or extrinsic symmetry on model error. We prove error lower bounds for invariant or equivariant networks in classification or regression settings with partially incorrect symmetry. We also analyze the potentially harmful effects of extrinsic equivariance. Experiments validate these results in three different environments.</p>","tags":["equivariance","relaxed_equivariance","dl_theory","paper"]},{"location":"100%20Reference%20notes/101%20Literature/A%20survey%20of%20quantization%20methods%20for%20efficient%20neural%20network%20inference/","title":"A survey of quantization methods for efficient neural network inference","text":"Properties authors Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W. Mahoney, Kurt Keutzer year 2021 url https://arxiv.org/abs/2103.13630 <p>Abstract</p> <p>As soon as abstract mathematical computations were adapted to computation on digital computers, the problem of efficient representation, manipulation, and communication of the numerical values in those computations arose. Strongly related to the problem of numerical representation is the problem of quantization: in what manner should a set of continuous real-valued numbers be distributed over a fixed discrete set of numbers to minimize the number of bits required and also to maximize the accuracy of the attendant computations? This perennial problem of quantization is particularly relevant whenever memory and/or computational resources are severely restricted, and it has come to the forefront in recent years due to the remarkable performance of Neural Network models in computer vision, natural language processing, and related areas. Moving from floating-point representations to low-precision fixed integer values represented in four bits or less holds the potential to reduce the memory footprint and latency by a factor of 16x; and, in fact, reductions of 4x to 8x are often realized in practice in these applications. Thus, it is not surprising that quantization has emerged recently as an important and very active sub-area of research in the efficient implementation of computations associated with Neural Networks. In this article, we survey approaches to the problem of quantizing the numerical values in deep Neural Network computations, covering the advantages/disadvantages of current methods. With this survey and its organization, we hope to have presented a useful snapshot of the current research in quantization for Neural Networks and to have given an intelligent organization to ease the evaluation of future research in this area.</p>","tags":["paper","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/AWQ%20-%20Activation-aware%20Weight%20Quantization%20for%20LLM%20Compression%20and%20Acceleration/","title":"AWQ   Activation aware Weight Quantization for LLM Compression and Acceleration","text":"Properties authors Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, Song Han year 2023 url https://arxiv.org/abs/2306.00978 <p>Abstract</p> <p>Large language models (LLMs) have fundamentally transformed the capabilities of numerous applications, from natural language processing to more intricate domain-specific tasks in robotics and autonomous driving. Moreover, the importance of on-device LLMs has grown significantly in the recent years. Running LLMs on edge devices not only promises reduced latency and improved user experience but also aligns with the increasing need for user privacy, as data processing can occur locally. However, the astronomical model sizes of modern LLMs and constraints of the edge devices, primarily in terms of memory size and bandwidth, pose significant deployment challenges. In this paper, we propose Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only quantization. Our method is based on the observation that weights are not equally important: protecting only 1% of salient weights can greatly reduce quantization error. We then propose to search for the optimal per-channel scaling that protects the salient weights by observing the activation, not weights. AWQ does not rely on any backpropagation or reconstruction, so it can well preserve LLMs' generalization ability on different domains and modalities, without overfitting to the calibration set. AWQ outperforms existing work on various language modeling and domain-specific benchmarks (coding and math). Thanks to better generalization, it achieves excellent quantization performance for instruction-tuned LMs and, for the first time, multi-modal LMs. Alongside AWQ, we implement TinyChat, an efficient and flexible inference framework tailored for on-device LLM/VLMs, offering more than 3x speedup over the Huggingface FP16 implementation on both desktop and mobile GPUs. It also democratizes the deployment of the 70B Llama-2 model on mobile GPUs.</p>","tags":["paper","efficient_dl","quantization"]},{"location":"100%20Reference%20notes/101%20Literature/AdaGlimpse%20-%20Active%20Visual%20Exploration%20with%20Arbitrary%20Glimpse%20Position%20and%20Scale/","title":"AdaGlimpse   Active Visual Exploration with Arbitrary Glimpse Position and Scale","text":"Properties authors Adam Pardyl, Micha\u0142 Wronka, Maciej Wo\u0142czyk, Kamil Adamczewski, Tomasz Trzci\u0144ski, Bartosz Zieli\u0144ski year 2025 url http://arxiv.org/abs/2404.03482 <p>Abstract</p> <p>Active Visual Exploration (AVE) is a task that involves dynamically selecting observations (glimpses), which is critical to facilitate comprehension and navigation within an environment. While modern AVE methods have demonstrated impressive performance, they are constrained to fixed-scale glimpses from rigid grids. In contrast, existing mobile platforms equipped with optical zoom capabilities can capture glimpses of arbitrary positions and scales. To address this gap between software and hardware capabilities, we introduce AdaGlimpse. It uses Soft Actor-Critic, a reinforcement learning algorithm tailored for exploration tasks, to select glimpses of arbitrary position and scale. This approach enables our model to rapidly establish a general awareness of the environment before zooming in for detailed analysis. Experimental results demonstrate that AdaGlimpse surpasses previous methods across various visual tasks while maintaining greater applicability in realistic AVE scenarios.</p>","tags":["paper","rl","active-visual-exploration","computer_vision"]},{"location":"100%20Reference%20notes/101%20Literature/AdaGlimpse%20-%20Active%20Visual%20Exploration%20with%20Arbitrary%20Glimpse%20Position%20and%20Scale/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","rl","active-visual-exploration","computer_vision"]},{"location":"100%20Reference%20notes/101%20Literature/Adapting%20Vision%20Foundation%20Models%20for%20Plant%20Phenotyping/","title":"Adapting Vision Foundation Models for Plant Phenotyping","text":"Properties authors Feng Chen, Mario Valerio Giuffrida, Sotirios A. Tsaftaris year 2023 url https://openaccess.thecvf.com/content/ICCV2023W/CVPPA/html/Chen_Adapting_Vision_Foundation_Models_for_Plant_Phenotyping_ICCVW_2023_paper.html <p>Abstract</p> <p>Foundation models are large models pre-trained on tremendous amount of data. They can be typically adapted to diverse downstream tasks with minimal effort. However, as foundation models are usually pre-trained on images or texts sourced from the Internet, their performance in specialized domains, such as plant phenotyping, comes into question. In addition, fully fine-tuning foundation models is time-consuming and requires high computational power. This paper investigates the efficient adaptation of foundation models for plant phenotyping settings and tasks. We perform extensive experiments on fine-tuning three foundation models, MAE, DINO, and DINOv2 on three essential plant phenotyping tasks: leaf counting, instance segmentation, and disease classification. In particular, the pre-trained backbones are kept frozen, while two distinct fine-tuning methods are evaluated, namely adapter tuning (using LoRA) and decoder tuning. The experimental results show that a foundation model can be efficiently adapted to multiple plant phenotyping tasks, yielding similar performance as the state-of-the-art (SoTA) models specifically designed or trained for each task. Despite exhibiting great transferability over different tasks, the fine-tuned foundation models perform slightly worse than the SoTA task-specific models in some scenarios, which requires further investigation.</p>","tags":["paper","peft","foundation_models"]},{"location":"100%20Reference%20notes/101%20Literature/Adapting%20Vision%20Foundation%20Models%20for%20Plant%20Phenotyping/#notes","title":"Notes","text":"<p>Motivation / Problem</p> <p>Foundation models struggle with specialized data like (plant phenotyping, cancer predictions)</p> <p>Research question</p> <p>Which efficient fine-tuning technique is most promising for adapting foundation models (MAE, DINO, DINOv2) in specialized data? </p> <p>Methods</p> <p>Benchmarked fine-tuning methods include decoder fine-tuning (aka linear probing) and adapter tuning (linear probing + LoRa)</p> <p>Results</p> <ol> <li>LoRa consistently beats DT</li> <li>VFM w/ LoRa are often competitive fully-trained/finetuned SOTA</li> <li>It's not clear that one vfm beats another, each model (DINO, DINOv2, MAE) have metrics and tasks where they shine</li> <li>LoRa can help dampen issues of data scarcity, domain shifts and class imbalance</li> </ol>","tags":["paper","peft","foundation_models"]},{"location":"100%20Reference%20notes/101%20Literature/An%20Image%20is%20Worth%20More%20Than%2016x16%20Patches%20-%20Exploring%20Transformers%20on%20Individual%20Pixels/","title":"An Image is Worth More Than 16x16 Patches   Exploring Transformers on Individual Pixels","text":"Properties authors Duy-Kien Nguyen, Mahmoud Assran, Unnat Jain, Martin R. Oswald, Cees G. M. Snoek, Xinlei Chen year 2024 url https://arxiv.org/abs/2406.09415v1 <p>Abstract</p> <p>This work does not introduce a new method. Instead, we present an interesting finding that questions the necessity of the inductive bias -- locality in modern computer vision architectures. Concretely, we find that vanilla Transformers can operate by directly treating each individual pixel as a token and achieve highly performant results. This is substantially different from the popular design in Vision Transformer, which maintains the inductive bias from ConvNets towards local neighborhoods (e.g. by treating each 16x16 patch as a token). We mainly showcase the effectiveness of pixels-as-tokens across three well-studied tasks in computer vision: supervised learning for object classification, self-supervised learning via masked autoencoding, and image generation with diffusion models. Although directly operating on individual pixels is less computationally practical, we believe the community must be aware of this surprising piece of knowledge when devising the next generation of neural architectures for computer vision.</p> <p>Comments: - Seems to contradict How do vision transformers work? in their position that inductive biases do improve vits.      - [ ] Might be useful to check this.</p>","tags":["paper","dl_theory","vit","computer_vision"]},{"location":"100%20Reference%20notes/101%20Literature/An%20Investigation%20into%20Neural%20Net%20Optimization%20via%20Hessian%20Eigenvalue%20Density/","title":"An Investigation into Neural Net Optimization via Hessian Eigenvalue Density","text":"Properties authors Behrooz Ghorbani, Shankar Krishnan, Ying Xiao <p>Abstract</p> <p>To understand the dynamics of optimization in deep neural networks, we develop a tool to study the evolution of the entire Hessian spectrum throughout the optimization process. Using this, we study a number of hypotheses concerning smoothness, curvature, and sharpness in the deep learning literature. We then thoroughly analyze a crucial structural feature of the spectra: in nonbatch normalized networks, we observe the rapid appearance of large isolated eigenvalues in the spectrum, along with a surprising concentration of the gradient in the corresponding eigenspaces. In batch normalized networks, these two effects are almost absent. We characterize these effects, and explain how they affect optimization speed through both theory and experiments. As part of this work, we adapt advanced tools from numerical linear algebra that allow scalable and accurate estimation of the entire Hessian spectrum of ImageNet-scale neural networks; this technique may be of independent interest in other applications</p>","tags":["dl_theory","optimizability","optimization","paper"]},{"location":"100%20Reference%20notes/101%20Literature/An%20image%20is%20worth%2016x16%20words%20-%20Transformers%20for%20image%20recognition%20at%20scale/","title":"An image is worth 16x16 words   Transformers for image recognition at scale","text":"Properties authors Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby url https://arxiv.org/abs/2010.11929 year 2020 <p>Abstract</p> <p>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</p>","tags":["vit","transformers","paper"]},{"location":"100%20Reference%20notes/101%20Literature/An%20image%20is%20worth%2016x16%20words%20-%20Transformers%20for%20image%20recognition%20at%20scale/#notes","title":"Notes","text":"","tags":["vit","transformers","paper"]},{"location":"100%20Reference%20notes/101%20Literature/An%20image%20is%20worth%2016x16%20words%20-%20Transformers%20for%20image%20recognition%20at%20scale/#regarding-inductive-biases","title":"Regarding inductive biases","text":"<p>Inductive bias. We note that Vision Transformer has much less image-specific inductive bias than CNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are baked into each layer throughout the whole model. In ViT, only MLP layers are local and translationally equivariant, while the self-attention layers are global. The two-dimensional neighborhood structure is used very sparingly: in the beginning of the model by cutting the image into patches and at fine-tuning time for adjusting the position embeddings for images of different resolution (as described below). Other than that, the position embeddings at initialization time carry no information about the 2D positions of the patches and all spatial relations between the patches have to be learned from scratch.</p> <p>Interesting insight about Hybrid ViTs (40 conv layers + transformer blocks):  - It is better on small data regimes but shows no improvement on large data regimes. </p>","tags":["vit","transformers","paper"]},{"location":"100%20Reference%20notes/101%20Literature/An%20image%20is%20worth%2016x16%20words%20-%20Transformers%20for%20image%20recognition%20at%20scale/#regarding-cls-vs-pooling","title":"Regarding cls vs pooling","text":"","tags":["vit","transformers","paper"]},{"location":"100%20Reference%20notes/101%20Literature/Apple%20Intelligence%20Foundation%20Language%20Models/","title":"Apple Intelligence Foundation Language Models","text":"Properties authors Tom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Ke Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Sam Wiseman, Syd Evans, Tao Lei, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, Anupama Mann Anupama, Areeba Kamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Erdenebileg, Chris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd, Fangping Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah Gillis Coleman, Jane Li, Jeffrey Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Maria Cordell, Meng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar, Xin Wang, Xin Zheng, Walker Cheng , Yael Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yunsong Meng, Zhao Tang Luo, Zhi Ouyang, Alp Aygar, Alvin Wan, Andrew Walkingshaw, Andy Narayanan, Antonie Lin, Arsalan Farooq, Brent Ramerth, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang Yang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, Irina Belousova, Joris Pelemans, Karen Yang, Keivan Alizadeh Vahid, Liangliang Cao, Mahyar Najibi, Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qichen Fu, Regan Poston, Sam Xu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui, Vivek Rangarajan Sridhar, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xingyu Zhou, Xinwen Liu, Yang Zhao, Yin Xia, Zhile Ren, Zhongzheng Ren year 2024 url https://arxiv.org/abs/2407.21075 <p>Abstract</p> <p>We present foundation language models developed to power Apple Intelligence features, including a ~3 billion parameter model designed to run efficiently on devices and a large server-based language model designed for Private Cloud Compute. These models are designed to perform a wide range of tasks efficiently, accurately, and responsibly. This report describes the model architecture, the data used to train the model, the training process, how the models are optimized for inference, and the evaluation results. We highlight our focus on Responsible AI and how the principles are applied throughout the model development.</p>","tags":["paper","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/Apple%20Intelligence%20Foundation%20Language%20Models/#notes","title":"Notes","text":"<p>\u201cA shared input/output embedding matrix [Press and Wolf, 2016] to reduce memory usage for parameters.\u201d (Gunter et al., 2024, p. 2)</p> <p>This reminds me of the Residual stream interpretation of transformers.</p> <p>\u201cThe model is compressed and quantized, on average under 4-bit-perweight, after the post-training stages (details of the quantization scheme will be discussed later). The quantized model often shows a moderate level of quality loss. Therefore, instead of directly passing the quantized model to application teams for feature development, we attach a set of parameter-efficient LoRa Adapters for quality recovery. We make sure that these LoRA adapters training recipes are consistent with pre-training and post-training processes. Then, products will fine-tune their own feature-specific LoRA adapters by initializing the adapter weights from the accuracy-recovery adapters, while keeping the quantized base model frozen.\u201d (Gunter et al., 2024, p. 16)</p> <p>So the recipe is: - Pre-training/Post-training - Compression? and Quantization (leads to accuracy loss) - LoRa fine-tuning to recover accuracy, call it LoRa Recovery, I'll assume this  - For a specific task, initialize LoRa adapter to the LoRa Recovery Some details: - Rank 16 LoRa - Does each LoRa adapter also share the same precision as the underlying weight block/matrix? I suppose so</p> <p>\u201cSpecifically, our AFM-on-device model running on Apple Neural Engine (ANE) uses Bit Palettization: for projection weights, every 16 columns/rows share the same quantization constants (i.e., lookup tables) and are quantized using K-means with 16 unique values (4-bit).\u201d (Gunter et al., 2024, p. 17)</p>","tags":["paper","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/Approximately%20equivariant%20networks%20for%20imperfectly%20symmetric%20dynamics/","title":"Approximately equivariant networks for imperfectly symmetric dynamics","text":"Properties authors Rui Wang, Robin Walters, Rose Yu year 2022 url https://proceedings.mlr.press/v162/wang22aa.html <p>Abstract</p> <p>Incorporating symmetry as an inductive bias into neural network architecture has led to improvements in generalization, data efficiency, and physical consistency in dynamics modeling. Methods such as CNNs or equivariant neural networks use weight tying to enforce symmetries such as shift invariance or rotational equivariance. However, despite the fact that physical laws obey many symmetries, real-world dynamical data rarely conforms to strict mathematical symmetry either due to noisy or incomplete data or to symmetry breaking features in the underlying dynamical system. We explore approximately equivariant networks which are biased towards preserving symmetry but are not strictly constrained to do so. By relaxing equivariance constraints, we find that our models can outperform both baselines with no symmetry bias and baselines with overly strict symmetry in both simulated turbulence domains and real-world multi-stream jet flow.</p>","tags":["relaxed_equivariance","equivariance","dl_theory","paper"]},{"location":"100%20Reference%20notes/101%20Literature/Approximation-Generalization%20Trade-offs%20under%20%28Approximate%29%20Group%20Equivariance/","title":"Approximation Generalization Trade offs under (Approximate) Group Equivariance","text":"Properties authors Mircea Petrache, Shubhendu Trivedi","tags":["paper"]},{"location":"100%20Reference%20notes/101%20Literature/Are%20-%20Hierarchical%20-%20Visual%20Representations%20Hierarchical/","title":"Are   Hierarchical   Visual Representations Hierarchical","text":"Properties authors Ethan Shen, Ali Farhadi, Aditya Kusupati year 2023 url http://arxiv.org/abs/2311.05784 <p>Abstract</p> <p>Learned visual representations often capture large amounts of semantic information for accurate downstream applications. Human understanding of the world is fundamentally grounded in hierarchy. To mimic this and further improve representation capabilities, the community has explored \u201chierarchical\u201d visual representations that aim at modeling the underlying hierarchy of the visual world. In this work, we set out to investigate if hierarchical visual representations truly capture the human perceived hierarchy better than standard learned representations. To this end, we create HierNet, a suite of 12 datasets spanning 3 kinds of hierarchy from the BREEDs subset of ImageNet. After extensive evaluation of Hyperbolic and Matryoshka Representations across training setups, we conclude that they do not capture hierarchy any better than the standard representations but can assist in other aspects like search efficiency and interpretability. Our benchmark and the datasets are open-sourced at https://github.com/ethanlshen/HierNet.</p>","tags":["paper","dl_theory","hyperbolic"]},{"location":"100%20Reference%20notes/101%20Literature/Are%20-%20Hierarchical%20-%20Visual%20Representations%20Hierarchical/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","dl_theory","hyperbolic"]},{"location":"100%20Reference%20notes/101%20Literature/Autoequivariant%20Network%20Search%20via%20Group%20Decomposition/","title":"Autoequivariant Network Search via Group Decomposition","text":"Properties authors Sourya Basu","tags":["paper"]},{"location":"100%20Reference%20notes/101%20Literature/Average%20entropy%20of%20Gaussian%20mixtures/","title":"Average entropy of Gaussian mixtures","text":"Properties authors Basheer Joudeh, Boris \u0160kori\u0107 year 2025 url http://arxiv.org/abs/2404.07311 <p>Abstract</p> <p>We calculate the average di\ufb00erential entropy of a q-component Gaussian mixture in Rn. For simplicity, all components have covariance matrix \u03c321, while the means {Wi}q i=1 are i.i.d.</p>","tags":["paper","information_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Average%20entropy%20of%20Gaussian%20mixtures/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","information_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Battle%20of%20the%20Backbones%20-%20A%20Large-Scale%20Comparison%20of%20Pretrained%20Models%20across%20Computer%20Vision%20Tasks/","title":"Battle of the Backbones   A Large Scale Comparison of Pretrained Models across Computer Vision Tasks","text":"Properties authors Micah Goldblum, Hossein Souri, Renkun Ni, Manli Shu, Viraj Prabhu, Gowthami Somepalli, Prithvijt Chattopadhyay, Mark Ibrahim, Adrien Bardes, Judy Hoffman, Rama Chellappa, Andrew Gordon Wilson, Tom Goldstein year 2023 url https://arxiv.org/abs/2310.19909 <p>Abstract</p> <p>Neural network based computer vision systems are typically built on a backbone, a pretrained or randomly initialized feature extractor. Several years ago, the default option was an ImageNet-trained convolutional neural network. However, the recent past has seen the emergence of countless backbones pretrained using various algorithms and datasets. While this abundance of choice has led to performance increases for a range of systems, it is difficult for practitioners to make informed decisions about which backbone to choose. Battle of the Backbones (BoB) makes this choice easier by benchmarking a diverse suite of pretrained models, including vision-language models, those trained via self-supervised learning, and the Stable Diffusion backbone, across a diverse set of computer vision tasks ranging from classification to object detection to OOD generalization and more. Furthermore, BoB sheds light on promising directions for the research community to advance computer vision by illuminating strengths and weakness of existing approaches through a comprehensive analysis conducted on more than 1500 training runs. While vision transformers (ViTs) and self-supervised learning (SSL) are increasingly popular, we find that convolutional neural networks pretrained in a supervised fashion on large training sets still perform best on most tasks among the models we consider. Moreover, in apples-to-apples comparisons on the same architectures and similarly sized pretraining datasets, we find that SSL backbones are highly competitive, indicating that future works should perform SSL pretraining with advanced architectures and larger pretraining datasets. We release the raw results of our experiments along with code that allows researchers to put their own backbones through the gauntlet here:\u00a0this https URL</p>","tags":["paper","foundation_models","computer_vision","vit","transformers","cnn"]},{"location":"100%20Reference%20notes/101%20Literature/Battle%20of%20the%20Backbones%20-%20A%20Large-Scale%20Comparison%20of%20Pretrained%20Models%20across%20Computer%20Vision%20Tasks/#notes","title":"Notes","text":"<p>It would be nice to see an update with DINOv2 - Learning Robust Visual Features without Supervision and EVA-02 - A Visual Representation for Neon Genesis.</p> <p>A performance comparison of ViTs and CNNs. Modern architectures strongly outperform vanilla ViTs. We see in Table 2 that the best performing backbone (ConvNeXt-Base) is convolutional, with a hierarchical transformer (SwinV2-Base) being a close second. The latter transformer architecture incorporates a strong spatial inductive bias. These findings suggest that the community should move past vanilla ViTs which are still used frequently. As a caveat, we do not evaluate very large models, and it is possible that ViTs might outperform their more advanced variants or convolutional networks at larger scales.</p> <p>Battle of the \u201csmall\u201d backbones. Keeping limited resources in mind, we also compare the \u201csmall\u201d subset of backbones in BoB (&lt; 30M parameters) \u2013 with ViT-Small, ConvNeXt-Tiny, Swin-Tiny and ResNet-50 architectures. Overall, we find Supervised ConvNeXt-T trained on IN-1k to be the best, followed by Supervised SwinV2-T trained on IN-1k and DINO ViT-S trained on IN-1k. Interestingly, supervised learning again dominates, and backbones pretrained on just IN-1k outperform ones trained on a considerably more diverse and larger dataset (MiDaS).</p> <p>Object Detection &amp; Segmentation. For object detection and instance segmentation, we find \u201cSupervised ConvNeXt-Base trained on IN-21K\u201d &gt; \u201cSupervised SwinV2-Base trained on IN-21k (finetuned on IN-1k)\u201d &gt; \u201cSupervised ConvNeXt-Base trained on IN-1k\u201d.</p> <p>These results are probably outdated since many foundation models already beat Swinv2 - SimPLR - A Simple and Plain Transformer for Scaling-Efficient Object Detection and Segmentation - Exploring Plain Vision Transformer Backbones for Object Detection</p>","tags":["paper","foundation_models","computer_vision","vit","transformers","cnn"]},{"location":"100%20Reference%20notes/101%20Literature/Beyond%20cls%20-%20Exploring%20the%20true%20potential%20of%20Masked%20Image%20Modeling%20representations/","title":"Beyond cls   Exploring the true potential of Masked Image Modeling representations","text":"Properties authors Marcin Przewi\u0119\u017alikowski, Randall Balestriero, Wojciech Jasi\u0144ski, Marek \u015amieja, Bartosz Zieli\u0144ski year 2024 url https://arxiv.org/abs/2412.03215 <p>Abstract</p> <p>Masked Image Modeling (MIM) has emerged as a popular method for Self-Supervised Learning (SSL) of visual representations. However, for high-level perception tasks, MIM-pretrained models offer lower out-of-the-box representation quality than the Joint-Embedding Architectures (JEA) - another prominent SSL paradigm. To understand this performance gap, we analyze the information flow in Vision Transformers (ViT) learned by both approaches. We reveal that whereas JEAs construct their representation on a selected set of relevant image fragments, MIM models aggregate nearly whole image content. Moreover, we demonstrate that MIM-trained ViTs retain valuable information within their patch tokens, which is not effectively captured by the global [cls] token representations. Therefore, selective aggregation of relevant patch tokens, without any fine-tuning, results in consistently higher-quality of MIM representations. To our knowledge, we are the first to highlight the lack of effective representation aggregation as an emergent issue of MIM and propose directions to address it, contributing to future advances in Self-Supervised Learning.</p>","tags":["paper","dl_theory","vit","transformers","thesis"]},{"location":"100%20Reference%20notes/101%20Literature/Beyond%20cls%20-%20Exploring%20the%20true%20potential%20of%20Masked%20Image%20Modeling%20representations/#notes","title":"Notes","text":"<p>\u201cAn alternative strategy is to summarize the image representation as the average value of patch tokens, i.e. PN i=1 zL,i N , sometimes even removing the [cls] token from the model [2, 33]. However, this typically leads to representations of worse quality [24]\u201d (Przewi\u0119\u017alikowski et al., 2024, p. 3)</p> <p>That's not true, 24 says that that pooling performs worse with the same lr as [cls], but if you change the learning rate they are both equivalent.</p> <p>Summary: - \u201cThe patch tokens of MAE assign more attention to themselves.\u201d (Przewi\u0119\u017alikowski et al., 2024, p. 6) - \u201cPatch tokens of MAE attend to patches more selectively than those of JEA\u201d (Przewi\u0119\u017alikowski et al., 2024, p. 6) - \u201cThe [cls] token of MAE attends primarily to itself.\u201d (Przewi\u0119\u017alikowski et al., 2024, p. 5) - \u201cThe [cls] token of MAE attends to the patches too uniformly to select only the relevant ones.\u201d (Przewi\u0119\u017alikowski et al., 2024, p. 5) - Paper proposes a small network that aggregates patch representations and gives SOTA to MAE in Image Classification</p>","tags":["paper","dl_theory","vit","transformers","thesis"]},{"location":"100%20Reference%20notes/101%20Literature/Block%20Transformer%20-%20Global-to-Local%20Language%20Modeling%20for%20Fast%20Inference/","title":"Block Transformer   Global to Local Language Modeling for Fast Inference","text":"Properties authors Namgyu Ho, Sangmin Bae, Taehyeon Kim, Hyunjik Jo, Yireun Kim, Tal Schuster, Adam Fisch, James Thorne, Se-Young Yun year 2024 url https://arxiv.org/abs/2406.02657 <p>Abstract</p> <p>This paper presents the Block Transformer architecture which adopts hierarchical global-to-local modeling to autoregressive transformers to mitigate the inference bottlenecks of self-attention. To apply self-attention, the key-value (KV) cache of all previous sequences must be retrieved from memory at every decoding step. Thereby, this KV cache IO becomes a significant bottleneck in batch inference. We notice that these costs stem from applying self-attention on the global context, therefore we isolate the expensive bottlenecks of global modeling to lower layers and apply fast local modeling in upper layers. To mitigate the remaining costs in the lower layers, we aggregate input tokens into fixed size blocks and then apply self-attention at this coarse level. Context information is aggregated into a single embedding to enable upper layers to decode the next block of tokens, without global attention. Free of global attention bottlenecks, the upper layers can fully utilize the compute hardware to maximize inference throughput. By leveraging global and local modules, the Block Transformer architecture demonstrates 10-20x gains in inference throughput compared to vanilla transformers with equivalent perplexity. Our work introduces a new approach to optimize language model inference through novel application of global-to-local modeling. Code is available at https://github.com/itsnamgyu/block-transformer.</p>","tags":["efficient_dl","transformers","paper"]},{"location":"100%20Reference%20notes/101%20Literature/BoxeR%20-%20Box-Attention%20for%202D%20and%203D%20Transformers/","title":"BoxeR   Box Attention for 2D and 3D Transformers","text":"Properties authors Duy-Kien Nguyen, Jihong Ju, Olaf Booij, Martin R. Oswald, Cees G. M. Snoek year 2021 url https://arxiv.org/abs/2111.13087 <p>Abstract</p> <p>In this paper, we propose a simple attention mechanism, we call box-attention. It enables spatial interaction between grid features, as sampled from boxes of interest, and improves the learning capability of transformers for several vision tasks. Specifically, we present BoxeR, short for Box Transformer, which attends to a set of boxes by predicting their transformation from a reference window on an input feature map. The BoxeR computes attention weights on these boxes by considering its grid structure. Notably, BoxeR-2D naturally reasons about box information within its attention module, making it suitable for end-to-end instance detection and segmentation tasks. By learning invariance to rotation in the box-attention module, BoxeR-3D is capable of generating discriminative information from a bird's-eye view plane for 3D end-to-end object detection. Our experiments demonstrate that the proposed BoxeR-2D achieves state-of-the-art results on COCO detection and instance segmentation. Besides, BoxeR-3D improves over the end-to-end 3D object detection baseline and already obtains a compelling performance for the vehicle category of Waymo Open, without any class-specific optimization. Code is available at\u00a0this https URL.</p>","tags":["paper","transformers","object_detection"]},{"location":"100%20Reference%20notes/101%20Literature/Building%20on%20Efficient%20Foundations%20-%20Effectively%20Training%20LLMs%20with%20Structured%20Feedforward%20Layers/","title":"Building on Efficient Foundations   Effectively Training LLMs with Structured Feedforward Layers","text":"Properties authors Xiuying Wei, Skander Moalla, Razvan Pascanu, Caglar Gulcehre year 2024 url https://arxiv.org/abs/2406.16450v1 <p>Abstract</p> <p>State-of-the-art results in large language models (LLMs) often rely on scale, which becomes computationally expensive. This has sparked a research agenda to reduce these models' parameter count and computational costs without significantly impacting their performance. Our study focuses on transformer-based LLMs, specifically targeting the computationally intensive feedforward networks (FFN), which are less studied than attention blocks. We consider three candidate linear layer approximations in the FFN by combining efficient low-rank and block-diagonal matrices. In contrast to many previous works that examined these approximations, our study i) explores these structures from the training-from-scratch perspective, ii) scales up to 1.3B parameters, and iii) is conducted within recent Transformer-based LLMs rather than convolutional architectures. We first demonstrate they can lead to actual computational gains in various scenarios, including online decoding when using a pre-merge technique. Additionally, we propose a novel training regime, called \\textit{self-guided training}, aimed at improving the poor training dynamics that these approximations exhibit when used from initialization. Experiments on the large RefinedWeb dataset show that our methods are both efficient and effective for training and inference. Interestingly, these structured FFNs exhibit steeper scaling curves than the original models. Further applying self-guided training to the structured matrices with 32\\% FFN parameters and 2.5\u00d7\u00a0speed-up enables only a 0.4 perplexity increase under the same training FLOPs. Finally, we develop the wide and structured networks surpassing the current medium-sized and large-sized Transformer in perplexity and throughput performance. Our code is available at \\url{this https URL}.</p>","tags":["paper","efficient_dl","llm","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/Building%20on%20Efficient%20Foundations%20-%20Effectively%20Training%20LLMs%20with%20Structured%20Feedforward%20Layers/#notes","title":"Notes","text":"<ul> <li> Note to self: Read this in depth \u23eb  #personal</li> </ul>","tags":["paper","efficient_dl","llm","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/Byte%20Latent%20Transformer%20-%20Patches%20Scale%20Better%20Than%20Tokens/","title":"Byte Latent Transformer   Patches Scale Better Than Tokens","text":"Properties authors Artidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret Li, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Ari Holtzman, Srinivasan Iyer year 2024 url http://arxiv.org/abs/2412.09871 <p>Abstract</p> <p>We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte, allocating more compute and model capacity where increased data complexity demands it. We present the first FLOP controlled scaling study of byte-level models up to 8B parameters and 4T training bytes. Our results demonstrate the feasibility of scaling models trained on raw bytes without a fixed vocabulary. Both training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. Overall, for fixed inference costs, BLT shows significantly better scaling than tokenization-based models, by simultaneously growing both patch and model size.</p>","tags":["paper","llm","nlp"]},{"location":"100%20Reference%20notes/101%20Literature/Byte%20Latent%20Transformer%20-%20Patches%20Scale%20Better%20Than%20Tokens/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","llm","nlp"]},{"location":"100%20Reference%20notes/101%20Literature/CKConv%20-%20Continuous%20Kernel%20Convolution%20For%20Sequential%20Data/","title":"CKConv   Continuous Kernel Convolution For Sequential Data","text":"Properties authors David W. Romero, Anna Kuzina, Erik J. Bekkers, Jakub M. Tomczak, Mark Hoogendoorn year 2021 url https://arxiv.org/abs/2102.02611 <p>Abstract</p> <p>Conventional neural architectures for sequential data present important limitations. Recurrent networks suffer from exploding and vanishing gradients, small effective memory horizons, and must be trained sequentially. Convolutional networks are unable to handle sequences of unknown size and their memory horizon must be defined a priori. In this work, we show that all these problems can be solved by formulating convolutional kernels in CNNs as continuous functions. The resulting Continuous Kernel Convolution (CKConv) allows us to model arbitrarily long sequences in a parallel manner, within a single operation, and without relying on any form of recurrence. We show that Continuous Kernel Convolutional Networks (CKCNNs) obtain state-of-the-art results in multiple datasets, e.g., permuted MNIST, and, thanks to their continuous nature, are able to handle non-uniformly sampled datasets and irregularly-sampled data natively. CKCNNs match or perform better than neural ODEs designed for these purposes in a faster and simpler manner.</p>","tags":["paper","convolutions","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Certifying%20Adapters%20-%20Enabling%20and%20Enhancing%20the%20Certification%20of%20Classifier%20Adversarial%20Robustness/","title":"Certifying Adapters   Enabling and Enhancing the Certification of Classifier Adversarial Robustness","text":"Properties authors Jieren Deng, Hanbin Hong, Aaron Palmer, Xin Zhou, Jinbo Bi, Kaleel Mahmood, Yuan Hong, Derek Aguiar year 2024 url http://arxiv.org/abs/2405.16036 <p>Abstract</p> <p>Randomized smoothing has become a leading method for achieving certified robustness in deep classifiers against \u2113p-norm adversarial perturbations. Current approaches for achieving certified robustness, like data augmentation with Gaussian noise and adversarial training, require expensive training procedures that tune large models for different Gaussian noise levels from scratch and thus cannot leverage high-performance pre-trained neural networks. In this work, we introduce a novel certifying adapters framework (CAF) that enables and enhances the certification of classifier adversarial robustness. Our approach makes few assumptions about the underlying training algorithm or feature extractor, and is thus broadly applicable to different feature extractor architectures (e.g., convolutional neural networks or vision transformers) and smoothing algorithms. We show that CAF (a) enables certification in uncertified models pre-trained on clean datasets and (b) substantially improves the performance of certified classifiers via randomized smoothing and SmoothAdv at multiple radii in CIFAR-10 and ImageNet. We demonstrate that CAF achieves improved certified accuracies when compared to methods based on random or denoised smoothing, and that CAF is insensitive to certifying adapter hyperparameters. Finally, we show that an ensemble of adapters enables a single pre-trained feature extractor to defend against a range of noise perturbation scales.</p>","tags":["paper","robustness"]},{"location":"100%20Reference%20notes/101%20Literature/Certifying%20Adapters%20-%20Enabling%20and%20Enhancing%20the%20Certification%20of%20Classifier%20Adversarial%20Robustness/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","robustness"]},{"location":"100%20Reference%20notes/101%20Literature/Color%20Equivariant%20Convolutional%20Networks/","title":"Color Equivariant Convolutional Networks","text":"Properties authors Attila Lengyel, Ombretta Strafforello, Robert-Jan Bruintjes, Alexander Gielisse, Jan van Gemert <p>References: - Learning Partial Equivariances from Data</p>","tags":["paper"]},{"location":"100%20Reference%20notes/101%20Literature/Color%20Space%20Transformation%20Network/","title":"Color Space Transformation Network","text":"Properties authors Alexandros Karargyris year 2015 url https://arxiv.org/abs/1511.01064 <p>Abstract</p> <p>Deep networks have become very popular over the past few years. The main reason for this widespread use is their excellent ability to learn and predict knowledge in a very easy and efficient way. Convolutional neural networks and auto-encoders have become the normal in the area of imaging and computer vision achieving unprecedented accuracy levels in many applications. The most common strategy is to build and train networks with many layers by tuning their hyper-parameters. While this approach has proven to be a successful way to build robust deep learning schemes it suffers from high complexity. In this paper we introduce a module that learns color space transformations within a network. Given a large dataset of colored images the color space transformation module tries to learn color space transformations that increase overall classification accuracy. This module has shown to increase overall accuracy for the same network design and to achieve faster convergence. It is part of a broader family of image transformations (e.g. spatial transformer network).</p>","tags":["cnn","paper"]},{"location":"100%20Reference%20notes/101%20Literature/ConViT%20-%20Improving%20Vision%20Transformers%20with%20Soft%20Convolutional%20Inductive%20Biases/","title":"ConViT   Improving Vision Transformers with Soft Convolutional Inductive Biases","text":"Properties authors St\u00e9phane d'Ascoli, Hugo Touvron, Matthew L. Leavitt, Ari S. Morcos, Giulio Biroli, Levent Sagun <p>Abstract</p> <p>TODO:  - [ ] Read paper - [ ] Add main text summary</p> <p>From Early Convolutions Help Transformers See Better, where [9] is this paper:</p> <p>We did not observe evidence that the hard locality constraint in early layers hampers the representational capacity of the network, as might be feared [9]. [...] This perspective resonates with the findings of [9], who observe that early transformer blocks prefer to learn more local attention patterns than later blocks.</p> <p>This is contrary to How do vision transformers work?, as they claim that locality constraint is beneficial to ViTs. </p> <p>Haven't fully read this paper, so the above contradiction might be incorrect.</p>","tags":["vit","computer_vision","cnn","transformers","inductive_bias","paper"]},{"location":"100%20Reference%20notes/101%20Literature/Correlational%20Image%20Modeling%20for%20Self-Supervised%20Visual%20Pre-Training/","title":"Correlational Image Modeling for Self Supervised Visual Pre Training","text":"Properties authors Wei Li, Jiahao Xie, Chen Change Loy year 2023 url http://arxiv.org/abs/2303.12670 <p>Abstract</p> <p>We introduce Correlational Image Modeling (CIM), a novel and surprisingly effective approach to self-supervised visual pre-training. Our CIM performs a simple pretext task: we randomly crop image regions (exemplars) from an input image (context) and predict correlation maps between the exemplars and the context. Three key designs enable correlational image modeling as a nontrivial and meaningful self-supervisory task. First, to generate useful exemplar-context pairs, we consider cropping image regions with various scales, shapes, rotations, and transformations. Second, we employ a bootstrap learning framework that involves online and target encoders. During pre-training, the former takes exemplars as inputs while the latter converts the context. Third, we model the output correlation maps via a simple cross-attention block, within which the context serves as queries and the exemplars offer values and keys. We show that CIM performs on par or better than the current state of the art on self-supervised and transfer benchmarks. Code is available at https://github.com/weivision/ Correlational-Image-Modeling.git.</p>","tags":["paper","ssl","dense_ssl"]},{"location":"100%20Reference%20notes/101%20Literature/Correlational%20Image%20Modeling%20for%20Self-Supervised%20Visual%20Pre-Training/#notes","title":"Notes","text":"<p>Zotero Link</p> <p>We take inspiration from visual tracking [81] in computer vision that defines the task of estimating the motion or trajectory of a target object (exemplar) in a sequence of scene images (contexts).</p> <p>maximizing the correlation between the specific exemplar and holistic contexts</p>","tags":["paper","ssl","dense_ssl"]},{"location":"100%20Reference%20notes/101%20Literature/Curiosity-driven%20Exploration%20by%20Self-supervised%20Prediction/","title":"Curiosity driven Exploration by Self supervised Prediction","text":"Properties authors Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, Trevor Darrell year 2017 url https://arxiv.org/abs/1705.05363 <p>Abstract</p> <p>In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at\u00a0this https URL</p> <p>(a) Which and how many environment(s) are the techniques tested on? 1. VizDoom: 2 maps, 1 for training and 1 for testing. Each map is different both in terms of layout and textures. 2. Super Mario Bros: 4 levels (1 for pre-training and 3 for testing). Levels might have different lightning (day/night), etc.</p> <p>(b) What methods is the proposed technique compared to? - In all experiments, the technique is ablated as follows:     - A3C (baseline)     - A3C + ICM     - A3C + ICM (pixels) - In experiment 3.1, ICM is compared to TRPO + VIME.</p> <p>(c) How are hyperparameters set? Does this result in a fair comparison? - For the comparison between ICM and TRPO (+ VIME), the hyperparameters for TRPO and TRPO + VIME are taken from a concurrent work that tests on the same environment. Unless we expect the other work to have chosen suboptimal hyperparameters in comparison to the current work, it doesn't strike me as unfair. - For the ICM ablation, no mention of the hyperparameters is made.  </p> <p>(d) Which quantities are measured?  - For the comparison between ICM and TRPO (+ VIME), mean and median score (at convergence) are used. They state ICM is also better in terms of convergence rate but don't provide any quantitative measure because both algorithms have \"different setups\". - For the ICM ablation, the Extrinsic Rewards per Episode is measured at each training iteration. This metric basically shows the average success rate of the agent (in reaching the goal).</p> <p>(e) Is it clear which experimental procedure was followed? (e.g. number of runs, ...)  (f) Is it clear what the spread is between such runs? How is this spread reported?  (g) Are results presented clear and interpretable way?</p>","tags":["paper","rl"]},{"location":"100%20Reference%20notes/101%20Literature/DETRs%20Beat%20YOLOs%20on%20Real-time%20Object%20Detection/","title":"DETRs Beat YOLOs on Real time Object Detection","text":"Properties authors Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu, Jie Chen year 2023 url https://arxiv.org/abs/2304.08069v3 <p>Abstract</p> <p>The YOLO series has become the most popular framework for real-time object detection due to its reasonable trade-off between speed and accuracy. However, we observe that the speed and accuracy of YOLOs are negatively affected by the NMS. Recently, end-to-end Transformer-based detectors (DETRs) have provided an alternative to eliminating NMS. Nevertheless, the high computational cost limits their practicality and hinders them from fully exploiting the advantage of excluding NMS. In this paper, we propose the Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge that addresses the above dilemma. We build RT-DETR in two steps, drawing on the advanced DETR: first we focus on maintaining accuracy while improving speed, followed by maintaining speed while improving accuracy. Specifically, we design an efficient hybrid encoder to expeditiously process multi-scale features by decoupling intra-scale interaction and cross-scale fusion to improve speed. Then, we propose the uncertainty-minimal query selection to provide high-quality initial queries to the decoder, thereby improving accuracy. In addition, RT-DETR supports flexible speed tuning by adjusting the number of decoder layers to adapt to various scenarios without retraining. Our RT-DETR-R50 / R101 achieves 53.1% / 54.3% AP on COCO and 108 / 74 FPS on T4 GPU, outperforming previously advanced YOLOs in both speed and accuracy. We also develop scaled RT-DETRs that outperform the lighter YOLO detectors (S and M models). Furthermore, RT-DETR-R50 outperforms DINO-R50 by 2.2% AP in accuracy and about 21 times in FPS. After pre-training with Objects365, RT-DETR-R50 / R101 achieves 55.3% / 56.2% AP. The project page:\u00a0this https URL.</p>","tags":["paper","computer_vision","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/DETRs%20with%20Collaborative%20Hybrid%20Assignments%20Training/","title":"DETRs with Collaborative Hybrid Assignments Training","text":"Properties authors Zhuofan Zong, Guanglu Song, Yu Liu year 2023 url https://arxiv.org/abs/2211.12860v5 <p>Abstract</p> <p>In this paper, we provide the observation that too few queries assigned as positive samples in DETR with one-to-one set matching leads to sparse supervision on the encoder's output which considerably hurt the discriminative feature learning of the encoder and vice visa for attention learning in the decoder. To alleviate this, we present a novel collaborative hybrid assignments training scheme, namely\u00a0\ue22fo-DETR, to learn more efficient and effective DETR-based detectors from versatile label assignment manners. This new training scheme can easily enhance the encoder's learning ability in end-to-end detectors by training the multiple parallel auxiliary heads supervised by one-to-many label assignments such as ATSS and Faster RCNN. In addition, we conduct extra customized positive queries by extracting the positive coordinates from these auxiliary heads to improve the training efficiency of positive samples in the decoder. In inference, these auxiliary heads are discarded and thus our method introduces no additional parameters and computational cost to the original detector while requiring no hand-crafted non-maximum suppression (NMS). We conduct extensive experiments to evaluate the effectiveness of the proposed approach on DETR variants, including DAB-DETR, Deformable-DETR, and DINO-Deformable-DETR. The state-of-the-art DINO-Deformable-DETR with Swin-L can be improved from 58.5% to 59.5% AP on COCO val. Surprisingly, incorporated with ViT-L backbone, we achieve 66.0% AP on COCO test-dev and 67.9% AP on LVIS val, outperforming previous methods by clear margins with much fewer model sizes. Codes are available at \\url{this https URL}.</p>","tags":["paper","object_detection","computer_vision","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/DETRs%20with%20Collaborative%20Hybrid%20Assignments%20Training/#notes","title":"Notes","text":"<p>Beats EVA-02 - A Visual Representation for Neon Genesis on object detection.</p> <p>Weights for CO-DINO Swin-L (64.1 box AP on COCO val): https://github.com/Sense-X/Co-DETR?tab=readme-ov-file</p>","tags":["paper","object_detection","computer_vision","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/DINOv2%20-%20Learning%20Robust%20Visual%20Features%20without%20Supervision/","title":"DINOv2   Learning Robust Visual Features without Supervision","text":"Properties authors Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Rusell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv\u00e9 Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski year 2023 url https://arxiv.org/abs/2304.07193 <p>Abstract</p> <p>The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.</p>","tags":["paper","foundation_models","computer_vision","vit"]},{"location":"100%20Reference%20notes/101%20Literature/Deep%20Learning%20Book/","title":"Deep Learning Book","text":"Properties authors Ian Goodfellow, Yoshua Bengio, Aaron Courville year 2016 url https://www.deeplearningbook.org/","tags":["dl_theory","textbook"]},{"location":"100%20Reference%20notes/101%20Literature/Deep%20Learning%20is%20Not%20So%20Mysterious%20or%20Different/","title":"Deep Learning is Not So Mysterious or Different","text":"Properties authors Andrew Gordon Wilson year 2025 url http://arxiv.org/abs/2503.02113 <p>Abstract</p> <p>Deep neural networks are often seen as different from other model classes by defying conventional notions of generalization. Popular examples of anomalous generalization behaviour include benign overfitting, double descent, and the success of overparametrization. We argue that these phenomena are not distinct to neural networks, or particularly mysterious. Moreover, this generalization behaviour can be intuitively understood, and rigorously characterized using long-standing generalization frameworks such as PAC-Bayes and countable hypothesis bounds. We present soft inductive biases as a key unifying principle in explaining these phenomena: rather than restricting the hypothesis space to avoid overfitting, embrace a flexible hypothesis space, with a soft preference for simpler solutions that are consistent with the data. This principle can be encoded in many model classes, and thus deep learning is not as mysterious or different from other model classes as it might seem. However, we also highlight how deep learning is relatively distinct in other ways, such as its ability for representation learning, phenomena such as mode connectivity, and its relative universality.</p>","tags":["paper","dl_theory","inductive_bias","equivariance"]},{"location":"100%20Reference%20notes/101%20Literature/Deep%20Learning%20is%20Not%20So%20Mysterious%20or%20Different/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","dl_theory","inductive_bias","equivariance"]},{"location":"100%20Reference%20notes/101%20Literature/Deformable%20Convolutional%20Networks/","title":"Deformable Convolutional Networks","text":"Properties authors Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, Yichen Wei year 2017 url http://arxiv.org/abs/1703.06211 <p>Abstract</p> <p>Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the \ufb01xed geometric structures in their building modules. In this work, we introduce two new modules to enhance the transformation modeling capability of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from the target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the performance of our approach. For the \ufb01rst time, we show that learning dense spatial transformation in deep CNNs is effective for sophisticated vision tasks such as object detection and semantic segmentation. The code is released at https://github.com/ msracver/Deformable-ConvNets.</p>","tags":["paper","cnn","computer_vision"]},{"location":"100%20Reference%20notes/101%20Literature/Deformable%20Convolutional%20Networks/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","cnn","computer_vision"]},{"location":"100%20Reference%20notes/101%20Literature/Deformable%20DETR%20-%20Deformable%20Transformers%20for%20End-to-End%20Object%20Detection/","title":"Deformable DETR   Deformable Transformers for End to End Object Detection","text":"Properties authors Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai year 2021 url http://arxiv.org/abs/2010.04159 <p>Abstract</p> <p>DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10\u00d7 less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach. Code is released at https:// github.com/fundamentalvision/Deformable-DETR.</p>","tags":["paper","computer_vision","object_detection","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/Deformable%20DETR%20-%20Deformable%20Transformers%20for%20End-to-End%20Object%20Detection/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","computer_vision","object_detection","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/DeiT%20III%20-%20Revenge%20of%20the%20ViT/","title":"DeiT III   Revenge of the ViT","text":"Properties authors Hugo Touvron, Matthieu Cord, Herv\u00e9 Jegou year 2022 url https://arxiv.org/abs/2204.07118 <p>Abstract</p> <p>A Vision Transformer (ViT) is a simple neural architecture amenable to serve several computer vision tasks. It has limited built-in architectural priors, in contrast to more recent architectures that incorporate priors either about the input data or of specific tasks. Recent works show that ViTs benefit from self-supervised pre-training, in particular BerT-like pre-training like BeiT. In this paper, we revisit the supervised training of ViTs. Our procedure builds upon and simplifies a recipe introduced for training ResNet-50. It includes a new simple data-augmentation procedure with only 3 augmentations, closer to the practice in self-supervised learning. Our evaluations on Image classification (ImageNet-1k with and without pre-training on ImageNet-21k), transfer learning and semantic segmentation show that our procedure outperforms by a large margin previous fully supervised training recipes for ViT. It also reveals that the performance of our ViT trained with supervision is comparable to that of more recent architectures. Our results could serve as better baselines for recent self-supervised approaches demonstrated on ViT.</p>","tags":["paper","vit"]},{"location":"100%20Reference%20notes/101%20Literature/DeiT%20III%20-%20Revenge%20of%20the%20ViT/#notes","title":"Notes","text":"<p>\"\"\" The main ingredients are as follows:  - We build upon the work of Wightman et al. [57] introduced for ResNet50. In particular we adopt a binary cross entropy loss for Imagenet1k only training. We adapt this method by including ingredients that significantly improve the training of large ViT [51], namely stochastic depth [24] and LayerScale [51].  - 3-Augment: is a simple data augmentation inspired by that employed for self-supervised learning. Surprisingly, with ViT we observe that it works better than the usual automatic/learned data-augmentation employed to train vision transformers like RandAugment [6].  - Simple Random Cropping is more effective than Random Resize Cropping when pre-training on a larger set like ImageNet-21k.  - A lower resolution at training time. This choice reduces the train-test discrepancy [53] but has not been much exploited with ViT. We observe that it also has a regularizing effect for the largest models by preventing overfitting. For instance, for a target resolution of 224 \u00d7 224, a ViT-H pre-trained at resolution 126 \u00d7 126 (81 tokens) achieves a better performance on ImageNet-1k than when pre-training at resolution 224 \u00d7 224 (256 tokens). This is also less demanding at pre-training time, as there are 70% fewer tokens. From this perspective it offers similar scaling properties as mask-autoencoders [19]. \"\"\"</p>","tags":["paper","vit"]},{"location":"100%20Reference%20notes/101%20Literature/Dense%20Contrastive%20Learning%20for%20Self-Supervised%20Visual%20Pre-Training/","title":"Dense Contrastive Learning for Self Supervised Visual Pre Training","text":"Properties authors Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, Lei Li year 2021 url http://arxiv.org/abs/2011.09157 <p>Abstract</p> <p>To date, most existing self-supervised learning methods are designed and optimized for image classi\ufb01cation. These pre-trained models can be sub-optimal for dense prediction tasks due to the discrepancy between image-level prediction and pixel-level prediction. To \ufb01ll this gap, we aim to design an effective, dense self-supervised learning method that directly works at the level of pixels (or local features) by taking into account the correspondence between local features. We present dense contrastive learning (DenseCL), which implements self-supervised learning by optimizing a pairwise contrastive (dis)similarity loss at the pixel level between two views of input images.</p>","tags":["paper","ssl","vit","contrastive_learning","thesis"]},{"location":"100%20Reference%20notes/101%20Literature/Dense%20Contrastive%20Learning%20for%20Self-Supervised%20Visual%20Pre-Training/#notes","title":"Notes","text":"<p>Zotero Link</p> <p></p> <p></p>","tags":["paper","ssl","vit","contrastive_learning","thesis"]},{"location":"100%20Reference%20notes/101%20Literature/DenseNets%20Reloaded%20-%20Paradigm%20Shift%20Beyond%20ResNets%20and%20ViTs/","title":"DenseNets Reloaded   Paradigm Shift Beyond ResNets and ViTs","text":"Properties authors Donghyun Kim, Byeongho Heo, Dongyoon Han year 2024 url https://arxiv.org/abs/2403.19588 <p>Abstract</p> <p>This paper revives Densely Connected Convolutional Networks (DenseNets) and reveals the underrated effectiveness over predominant ResNet-style architectures. We believe DenseNets' potential was overlooked due to untouched training methods and traditional design elements not fully revealing their capabilities. Our pilot study shows dense connections through concatenation are strong, demonstrating that DenseNets can be revitalized to compete with modern architectures. We methodically refine suboptimal components - architectural adjustments, block redesign, and improved training recipes towards widening DenseNets and boosting memory efficiency while keeping concatenation shortcuts. Our models, employing simple architectural elements, ultimately surpass Swin Transformer, ConvNeXt, and DeiT-III - key architectures in the residual learning lineage. Furthermore, our models exhibit near state-of-the-art performance on ImageNet-1K, competing with the very recent models and downstream tasks, ADE20k semantic segmentation, and COCO object detection/instance segmentation. Finally, we provide empirical analyses that uncover the merits of the concatenation over additive shortcuts, steering a renewed preference towards DenseNet-style designs. Our code is available at\u00a0this https URL.</p>","tags":["cnn","dl_theory","optimizability","paper"]},{"location":"100%20Reference%20notes/101%20Literature/Discovering%20Symmetry%20Breaking%20in%20Physical%20Systems%20with%20Relaxed%20Group%20Convolution/","title":"Discovering Symmetry Breaking in Physical Systems with Relaxed Group Convolution","text":"Properties authors Rui Wang, Elyssa Hofgard, Han Gao, Robin Walters, Tess E Smidt year 2024 url https://arxiv.org/abs/2310.02299 <p>Abstract</p> <p>Modeling symmetry breaking is essential for understanding the fundamental changes in the behaviors and properties of physical systems, from microscopic particle interactions to macroscopic phenomena like fluid dynamics and cosmic structures. Thus, identifying sources of asymmetry is an important tool for understanding physical systems. In this paper, we focus on learning asymmetries of data using relaxed group convolutions. We provide both theoretical and empirical evidence that this flexible convolution technique allows the model to maintain the highest level of equivariance that is consistent with data and discover the subtle symmetry-breaking factors in various physical systems. We employ various relaxed group convolution architectures to uncover various symmetry-breaking factors that are interpretable and physically meaningful in different physical systems, including the phase transition of crystal structure, the isotropy and homogeneity breaking in turbulent flow, and the time-reversal symmetry breaking in pendulum systems.</p> <p>Observations: - \"In the relaxed group convolution, the initial relaxed (equivariant) weights\u00a0{\ud835\udc64\ud835\udc59\u2062(\u210e)}\u00a0in each layer are set to be the same for all\u00a0\u210e, ensuring that the model exhibits equivariance prior to being trained. [...] we prove that these relaxed weights only deviate from being equal when the symmetries of the input and the output are lower than that of the model.\" (Related to Equivariance Initialization)</p>","tags":["equivariance","relaxed_equivariance","dl_theory","paper"]},{"location":"100%20Reference%20notes/101%20Literature/DropPos%20-%20Pre-Training%20Vision%20Transformers%20by%20Reconstructing%20Dropped%20Positions/","title":"DropPos   Pre Training Vision Transformers by Reconstructing Dropped Positions","text":"Properties authors Haochen Wang, Junsong Fan, Yuxi Wang, Kaiyou Song, Tong Wang, Zhaoxiang Zhang year 2023 url https://arxiv.org/abs/2309.03576 <p>Abstract</p> <p>As it is empirically observed that Vision Transformers (ViTs) are quite insensitive to the order of input tokens, the need for an appropriate self-supervised pretext task that enhances the location awareness of ViTs is becoming evident. To address this, we present DropPos, a novel pretext task designed to reconstruct Dropped Positions. The formulation of DropPos is simple: we first drop a large random subset of positional embeddings and then the model classifies the actual position for each non-overlapping patch among all possible positions solely based on their visual appearance. To avoid trivial solutions, we increase the difficulty of this task by keeping only a subset of patches visible. Additionally, considering there may be different patches with similar visual appearances, we propose position smoothing and attentive reconstruction strategies to relax this classification problem, since it is not necessary to reconstruct their exact positions in these cases. Empirical evaluations of DropPos show strong capabilities. DropPos outperforms supervised pre-training and achieves competitive results compared with state-of-the-art self-supervised alternatives on a wide range of downstream benchmarks. This suggests that explicitly encouraging spatial reasoning abilities, as DropPos does, indeed contributes to the improved location awareness of ViTs. The code is publicly available at\u00a0this https URL.</p>","tags":["paper","foundation_models","vit"]},{"location":"100%20Reference%20notes/101%20Literature/DropPos%20-%20Pre-Training%20Vision%20Transformers%20by%20Reconstructing%20Dropped%20Positions/#notes","title":"Notes","text":"<p>Maybe expected, but fixed position embeddings achieve better performance than learnable position embeddings.</p>","tags":["paper","foundation_models","vit"]},{"location":"100%20Reference%20notes/101%20Literature/EA-ViT%20-%20Efficient%20Adaptation%20for%20Elastic%20Vision%20Transformer/","title":"EA ViT   Efficient Adaptation for Elastic Vision Transformer","text":"Properties authors Chen Zhu, Wangbo Zhao, Huiwen Zhang, Samir Khaki, Yuhao Zhou, Weidong Tang, Shuo Wang, Zhihang Yuan, Yuzhang Shang, Xiaojiang Peng, Kai Wang, Dawei Yang year 2025 url http://arxiv.org/abs/2507.19360 <p>Abstract</p> <p>Vision Transformers (ViTs) have emerged as a foundational model in computer vision, excelling in generalization and adaptation to downstream tasks. However, deploying ViTs to support diverse resource constraints typically requires retraining multiple, size-specific ViTs, which is both time-consuming and energy-intensive. To address this issue, we propose an efficient ViT adaptation framework that enables a single adaptation process to generate multiple models of varying sizes for deployment on platforms with various resource constraints. Our approach comprises two stages. In the first stage, we enhance a pre-trained ViT with a nested elastic architecture that enables structural flexibility across MLP expansion ratio, number of attention heads, embedding dimension, and network depth. To preserve pre-trained knowledge and ensure stable adaptation, we adopt a curriculum-based training strategy that progressively increases elasticity. In the second stage, we design a lightweight router to select submodels according to computational budgets and downstream task demands. Initialized with Pareto-optimal configurations derived via a customized NSGA-II algorithm, the router is then jointly optimized with the backbone. Extensive experiments on multiple benchmarks demonstrate the effectiveness and versatility of EA-ViT. The code is available at https://github.com/zcxcf/EA-ViT.</p>","tags":["paper","efficient_vision"]},{"location":"100%20Reference%20notes/101%20Literature/EA-ViT%20-%20Efficient%20Adaptation%20for%20Elastic%20Vision%20Transformer/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","efficient_vision"]},{"location":"100%20Reference%20notes/101%20Literature/EVA-02%20-%20A%20Visual%20Representation%20for%20Neon%20Genesis/","title":"EVA 02   A Visual Representation for Neon Genesis","text":"Properties authors Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, Yue Cao year 2023 url https://arxiv.org/abs/2303.11331 <p>Abstract</p> <p>We launch EVA-02, a next-generation Transformer-based visual representation pre-trained to reconstruct strong and robust language-aligned vision features via masked image modeling. With an updated plain Transformer architecture as well as extensive pre-training from an open &amp; accessible giant CLIP vision encoder, EVA-02 demonstrates superior performance compared to prior state-of-the-art approaches across various representative vision tasks, while utilizing significantly fewer parameters and compute budgets. Notably, using exclusively publicly accessible training data, EVA-02 with only 304M parameters achieves a phenomenal 90.0 fine-tuning top-1 accuracy on ImageNet-1K val set. Additionally, our EVA-02-CLIP can reach up to 80.4 zero-shot top-1 on ImageNet-1K, outperforming the previous largest &amp; best open-sourced CLIP with only ~1/6 parameters and ~1/6 image-text training data. We offer four EVA-02 variants in various model sizes, ranging from 6M to 304M parameters, all with impressive performance. To facilitate open access and open research, we release the complete suite of EVA-02 to the community at\u00a0this https URL.</p>","tags":["paper","foundation_models","computer_vision","vit"]},{"location":"100%20Reference%20notes/101%20Literature/Early%20Convolutions%20Help%20Transformers%20See%20Better/","title":"Early Convolutions Help Transformers See Better","text":"Properties authors Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Doll\u00e1r, Ross Girshick <p>Hypothesis</p> <p>ViT's patchify convolution is contrary to standard early layers in CNNs. Maybe that's the cause?</p> <p>Main idea</p> <p>Replace patchify convolution with a small number of convolutional layers and drop one transformer block to make comparison fair.</p> <p></p> <p>Notes for myself: - Interesting experimentation regarding #optimizability , maybe take into account into hessian analysis</p>","tags":["cnn","transformers","vit","optimizability","paper"]},{"location":"100%20Reference%20notes/101%20Literature/Efficient%20Equivariant%20Transfer%20Learning%20from%20Pretrained%20Models/","title":"Efficient Equivariant Transfer Learning from Pretrained Models","text":"Properties authors Sourya Basu, Pulkit Katdare, Prasanna Sattigeri, Vijil Chenthamarakshan, Katherine Driggs-Campbell, Payel Das, Lav R. Varshney year 2023 url http://arxiv.org/abs/2305.09900 <p>Abstract</p> <p>Efficient transfer learning algorithms are key to the success of foundation models on diverse downstream tasks even with limited data. Recent works of Basu et al. (2023) and Kaba et al. (2022) propose group averaging (equitune) and optimizationbased methods, respectively, over features from group-transformed inputs to obtain equivariant outputs from non-equivariant neural networks. While Kaba et al. (2022) are only concerned with training from scratch, we find that equitune performs poorly on equivariant zero-shot tasks despite good finetuning results. We hypothesize that this is because pretrained models provide better quality features for certain transformations than others and simply averaging them is deleterious. Hence, we propose \u03bb-equitune that averages the features using importance weights, \u03bbs. These weights are learned directly from the data using a small neural network, leading to excellent zero-shot and finetuned results that outperform equitune. Further, we prove that \u03bb-equitune is equivariant and a universal approximator of equivariant functions. Additionally, we show that the method of Kaba et al. (2022) used with appropriate loss functions, which we call equizero, also gives excellent zero-shot and finetuned performance. Both equitune and equizero are special cases of \u03bbequitune. To show the simplicity and generality of our method, we validate on a wide range of diverse applications and models such as 1) image classification using CLIP, 2) deep Q-learning, 3) fairness in natural language generation (NLG), 4) compositional generalization in languages, and 5) image classification using pretrained CNNs such as Resnet and Alexnet.</p>","tags":["paper","equivariance"]},{"location":"100%20Reference%20notes/101%20Literature/Efficient%20Equivariant%20Transfer%20Learning%20from%20Pretrained%20Models/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","equivariance"]},{"location":"100%20Reference%20notes/101%20Literature/Efficient%20Modulation%20for%20Vision%20Networks/","title":"Efficient Modulation for Vision Networks","text":"Properties authors Xu Ma, Xiyang Dai, Jianwei Yang, Bin Xiao, Yinpeng Chen, Yun Fu, Lu Yuan year 2024 url https://arxiv.org/abs/2403.19963 <p>Abstract</p> <p>In this work, we present efficient modulation, a novel design for efficient vision networks. We revisit the modulation mechanism, which operates input through convolutional context modeling and feature projection layers, and fuses features via element-wise multiplication and an MLP block. We demonstrate that the modulation mechanism is particularly well suited for efficient networks and further tailor the modulation design by proposing the efficient modulation (EfficientMod) block, which is considered the essential building block for our networks. Benefiting from the prominent representational ability of modulation mechanism and the proposed efficient design, our network can accomplish better trade-offs between accuracy and efficiency and set new state-of-the-art performance in the zoo of efficient networks. When integrating EfficientMod with the vanilla self-attention block, we obtain the hybrid architecture which further improves the performance without loss of efficiency. We carry out comprehensive experiments to verify EfficientMod's performance. With fewer parameters, our EfficientMod-s performs 0.6 top-1 accuracy better than EfficientFormerV2-s2 and is 25% faster on GPU, and 2.9 better than MobileViTv2-1.0 at the same GPU latency. Additionally, our method presents a notable improvement in downstream tasks, outperforming EfficientFormerV2-s by 3.6 mIoU on the ADE20K benchmark. Code and checkpoints are available at\u00a0this https URL.</p>","tags":["efficient_dl","computer_vision"]},{"location":"100%20Reference%20notes/101%20Literature/EfficientViT-SAM%20-%20Accelerated%20Segment%20Anything%20Model%20Without%20Accuracy%20Loss/","title":"EfficientViT SAM   Accelerated Segment Anything Model Without Accuracy Loss","text":"Properties authors Zhuoyang Zhang, Han Cai, Song Han year 2024 url https://arxiv.org/abs/2402.05008 <p>Abstract</p> <p>We present EfficientViT-SAM, a new family of accelerated segment anything models. We retain SAM's lightweight prompt encoder and mask decoder while replacing the heavy image encoder with EfficientViT. For the training, we begin with the knowledge distillation from the SAM-ViT-H image encoder to EfficientViT. Subsequently, we conduct end-to-end training on the SA-1B dataset. Benefiting from EfficientViT's efficiency and capacity, EfficientViT-SAM delivers 48.9x measured TensorRT speedup on A100 GPU over SAM-ViT-H without sacrificing performance. Our code and pre-trained models are released at\u00a0this https URL.</p>","tags":["paper","efficient_dl","vit"]},{"location":"100%20Reference%20notes/101%20Literature/Emergent%20Equivariance%20in%20Deep%20Ensembles/","title":"Emergent Equivariance in Deep Ensembles","text":"Properties authors Jan E. Gerken, Pan Kessel year 2024 url https://arxiv.org/abs/2403.03103 <p>Abstract</p> <p>We demonstrate that deep ensembles are secretly equivariant models. More precisely, we show that deep ensembles become equivariant for all inputs and at all training times by simply using data augmentation. Crucially, equivariance holds off-manifold and for any architecture in the infinite width limit. The equivariance is emergent in the sense that predictions of individual ensemble members are not equivariant but their collective prediction is. Neural tangent kernel theory is used to derive this result and we verify our theoretical insights using detailed numerical experiments.</p>","tags":["equivariance","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Emerging%20Properties%20in%20Self-Supervised%20Vision%20Transformers/","title":"Emerging Properties in Self Supervised Vision Transformers","text":"Properties authors Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 Jegou, Julien Mairal, Piotr Bojanowski, Armand Joulin year 2021 url https://arxiv.org/abs/2104.14294 <p>Abstract</p> <p>In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.</p>","tags":["paper","foundation_models","computer_vision","vit"]},{"location":"100%20Reference%20notes/101%20Literature/Emerging%20Properties%20in%20Self-Supervised%20Vision%20Transformers/#notes","title":"Notes","text":"","tags":["paper","foundation_models","computer_vision","vit"]},{"location":"100%20Reference%20notes/101%20Literature/End-to-End%20Object%20Detection%20with%20Transformers/","title":"End to End Object Detection with Transformers","text":"Properties authors Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko year 2020 url https://arxiv.org/abs/2005.12872 <p>Abstract</p> <p>We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at\u00a0this https URL.</p>","tags":["paper","computer_vision","object_detection","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/Equi-Tuning%20-%20Group%20Equivariant%20Fine-Tuning%20of%20Pretrained%20Models/","title":"Equi Tuning   Group Equivariant Fine Tuning of Pretrained Models","text":"Properties authors Sourya Basu <p>Main idea</p> <p>Given non-equivariant pre-trained model \\(M(x)\\), define equivariant model \\(M_G(x)\\), as the average of the inverted predictions for all group actions on input \\(x\\)</p> \\[  M_G(x) = \\frac{1}{|G|} \\sum_{g \\in G} g^{-1}  M(g x) \\] <p>Abstract</p>","tags":["dl2"]},{"location":"100%20Reference%20notes/101%20Literature/Equivariance%20with%20Learned%20Canonicalization%20Functions/","title":"Equivariance with Learned Canonicalization Functions","text":"Properties authors S\u00e9kou-Oumar Kaba, Arnab Kumar Mondal, Yan Zhang, Yoshua Bengio, Siamak Ravanbakhsh <p>Main idea</p> <p>We learn a canonicalization function \\(h\\) either by a neural network or an optimization procedure. $$ \\phi(x) = h'(x) f(h(x)^{-1} x) $$</p> <p>Abstract</p>","tags":["dl2"]},{"location":"100%20Reference%20notes/101%20Literature/Equivariance-aware%20architectural%20optimization%20of%20neural%20networks/","title":"Equivariance aware architectural optimization of neural networks","text":"Properties authors Kaitlin Maile, Dennis G. Wilson, Patrick Forr\u00e9 <p>References: - Learning Partial Equivariances from Data</p> <p>Abstract</p>","tags":["dl2"]},{"location":"100%20Reference%20notes/101%20Literature/Equivariant%20Representation%20Learning%20via%20Class-Pose%20Decomposition/","title":"Equivariant Representation Learning via Class Pose Decomposition","text":"Properties authors Giovanni Luca Marchetti, Gustaf Tegn\u00e9r, Anastasiia Varava, Danica Kragic year 2023 url http://arxiv.org/abs/2207.03116 <p>Abstract</p> <p>We introduce a general method for learning representations that are equivariant to symmetries of data. Our central idea is to decompose the latent space into an invariant factor and the symmetry group itself. The components semantically correspond to intrinsic data classes and poses respectively. The learner is trained on a loss encouraging equivariance based on supervision from relative symmetry information. The approach is motivated by theoretical results from group theory and guarantees representations that are lossless, interpretable and disentangled. We provide an empirical investigation via experiments involving datasets with a variety of symmetries. Results show that our representations capture the geometry of data and outperform other equivariant representation learning frameworks.</p>","tags":["paper","equivariance","dl_theory","representation_learning","ssl"]},{"location":"100%20Reference%20notes/101%20Literature/Equivariant%20Representation%20Learning%20via%20Class-Pose%20Decomposition/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","equivariance","dl_theory","representation_learning","ssl"]},{"location":"100%20Reference%20notes/101%20Literature/Exact%20Conversion%20of%20In-Context%20Learning%20to%20Model%20Weights%20in%20Linearized-Attention%20Transformers/","title":"Exact Conversion of In Context Learning to Model Weights in Linearized Attention Transformers","text":"Properties authors Brian K Chen, Tianyang Hu, Hui Jin, Hwee Kuan Lee, Kenji Kawaguchi year 2024 url https://arxiv.org/abs/2406.02847 <p>Abstract</p> <p>In-Context Learning (ICL) has been a powerful emergent property of large language models that has attracted increasing attention in recent years. In contrast to regular gradient-based learning, ICL is highly interpretable and does not require parameter updates. In this paper, we show that, for linearized transformer networks, ICL can be made explicit and permanent through the inclusion of bias terms. We mathematically demonstrate the equivalence between a model with ICL demonstration prompts and the same model with the additional bias terms. Our algorithm (ICLCA) allows for exact conversion in an inexpensive manner. Existing methods are not exact and require expensive parameter updates. We demonstrate the efficacy of our approach through experiments that show the exact incorporation of ICL tokens into a linear transformer. We further suggest how our method can be adapted to achieve cheap approximate conversion of ICL tokens, even in regular transformer networks that are not linearized. Our experiments on GPT-2 show that, even though the conversion is only approximate, the model still gains valuable context from the included bias terms.</p>","tags":["paper","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/Exploiting%20Redundancy%20-%20Separable%20Group%20Convolutional%20Networks%20on%20Lie%20Groups/","title":"Exploiting Redundancy   Separable Group Convolutional Networks on Lie Groups","text":"Properties authors David M. Knigge, David W. Romero, Erik J. Bekkers <p>Abstract</p> <p>In this work, we investigate the properties of representations learned by regular G-CNNs, and show considerable parameter redundancy in group convolution kernels. This finding motivates further weight-tying by sharing convolution kernels over subgroups. To this end, we introduce convolution kernels that are separable over the subgroup and channel dimensions.</p> <p>Interesting because it reduces the total parameter count by separating group convolution kernels. This also has a regularisation effect.</p> <p>Citations: - Relaxing Equivariance Constraints with Non-stationary Continuous Filters</p>","tags":["dl2"]},{"location":"100%20Reference%20notes/101%20Literature/Exploring%20Plain%20Vision%20Transformer%20Backbones%20for%20Object%20Detection/","title":"Exploring Plain Vision Transformer Backbones for Object Detection","text":"Properties authors Yanghao Li, Hanzi Mao, Ross Girshick, Kaiming He year 2023 url https://arxiv.org/abs/2203.16527 <p>Abstract</p> <p>We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection. This design enables the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for fine-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is sufficient to build a simple feature pyramid from a single-scale feature map (without the common FPN design) and (ii) it is sufficient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain ViT backbones pre-trained as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the previous leading methods that were all based on hierarchical backbones, reaching up to 61.3 AP_box on the COCO dataset using only ImageNet-1K pre-training. We hope our study will draw attention to research on plain-backbone detectors. Code for ViTDet is available in Detectron2.</p>","tags":["paper","computer_vision","object_detection","transformers","vit"]},{"location":"100%20Reference%20notes/101%20Literature/Exploring%20Plain%20Vision%20Transformer%20Backbones%20for%20Object%20Detection/#notes","title":"Notes","text":"<ul> <li>It effectively adapts a pre-trained vision transformers as backbones and decoder heads by adding minimal layers in between to make them work</li> <li>Requires full fine-tuning</li> <li>Ranks #16 on https://paperswithcode.com/sota/object-detection-on-coco-minival, ~4 box map points lower than the first spot </li> </ul> <p>Code and weights at: https://github.com/facebookresearch/detectron2/tree/main/projects/ViTDet</p>","tags":["paper","computer_vision","object_detection","transformers","vit"]},{"location":"100%20Reference%20notes/101%20Literature/Exploring%20Simple%20Siamese%20Representation%20Learning/","title":"Exploring Simple Siamese Representation Learning","text":"Properties authors Xinlei Chen, Kaiming He year 2020 url http://arxiv.org/abs/2011.10566 <p>Abstract</p> <p>Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These models maximize the similarity between two augmentations of one image, subject to certain conditions for avoiding collapsing solutions. In this paper, we report surprising empirical results that simple Siamese networks can learn meaningful representations even using none of the following: (i) negative sample pairs, (ii) large batches, (iii) momentum encoders. Our experiments show that collapsing solutions do exist for the loss and structure, but a stop-gradient operation plays an essential role in preventing collapsing. We provide a hypothesis on the implication of stop-gradient, and further show proof-of-concept experiments verifying it. Our \u201cSimSiam\u201d method achieves competitive results on ImageNet and downstream tasks. We hope this simple baseline will motivate people to rethink the roles of Siamese architectures for unsupervised representation learning. Code will be made available.</p>","tags":["paper","dl_theory","ssl","computer_vision"]},{"location":"100%20Reference%20notes/101%20Literature/Exploring%20Simple%20Siamese%20Representation%20Learning/#notes","title":"Notes","text":"<p>Zotero Link</p> <p></p>","tags":["paper","dl_theory","ssl","computer_vision"]},{"location":"100%20Reference%20notes/101%20Literature/FLSL%20-%20Feature-level%20Self-supervised%20Learning/","title":"FLSL   Feature level Self supervised Learning","text":"Properties authors Qing Su, Anton Netchaev, Hai Li, Shihao Ji year 2023 url http://arxiv.org/abs/2306.06203 <p>Abstract</p> <p>Current self-supervised learning (SSL) methods (e.g., SimCLR, DINO, VICReg, MOCOv3) target primarily on representations at instance level and do not generalize well to dense prediction tasks, such as object detection and segmentation. Towards aligning SSL with dense predictions, this paper demonstrates for the first time the underlying mean-shift clustering process of Vision Transformers (ViT), which aligns well with natural image semantics (e.g., a world of objects and stuffs). By employing transformer for joint embedding and clustering, we propose a bi-level feature clustering SSL method, coined Feature-Level Self-supervised Learning (FLSL). We present the formal definition of the FLSL problem and construct the objectives from the mean-shift and k-means perspectives. We show that FLSL promotes remarkable semantic cluster representations and learns an encoding scheme amenable to intra-view and inter-view feature clustering. Experiments show that FLSL yields significant improvements in dense prediction tasks, achieving 44.9 (+2.8)% AP and 46.5% AP in object detection, as well as 40.8 (+2.3)% AP and 42.1% AP in instance segmentation on MS-COCO, using Mask R-CNN with ViT-S/16 and ViT-S/8 as backbone, respectively. FLSL consistently outperforms existing SSL methods across additional benchmarks, including UAV object detection on UAVDT, and video instance segmentation on DAVIS 2017. We conclude by presenting visualization and various ablation studies to better understand the success of FLSL. The source code is available at https://github.com/ISL-CV/FLSL.</p>","tags":["paper","vit","ssl","thesis"]},{"location":"100%20Reference%20notes/101%20Literature/FLSL%20-%20Feature-level%20Self-supervised%20Learning/#notes","title":"Notes","text":"<p>Zotero Link</p> <p></p>","tags":["paper","vit","ssl","thesis"]},{"location":"100%20Reference%20notes/101%20Literature/FNet%20-%20Mixing%20Tokens%20with%20Fourier%20Transforms/","title":"FNet   Mixing Tokens with Fourier Transforms","text":"Properties authors James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon year 2022 url http://arxiv.org/abs/2105.03824 <p>Abstract</p> <p>We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that \"mix\" input tokens. These linear mixers, along with standard nonlinearities in feed-forward layers, prove competent at modeling semantic relationships in several text classification tasks. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the \"efficient\" Transformers on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.</p>","tags":["paper","spectral","transformers","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/FNet%20-%20Mixing%20Tokens%20with%20Fourier%20Transforms/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","spectral","transformers","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Fast%2C%20Expressive%20SE%28n%29%20Equivariant%20Networks%20through%20Weight-Sharing%20in%20Position-Orientation%20Space/","title":"Fast, Expressive SE(n) Equivariant Networks through Weight Sharing in Position Orientation Space","text":"Properties authors Erik J. Bekkers, Sharvaree Vadgama, Rob D. Hesselink, Putri A. van der Linden, David W. Romero <p>Abstract</p>","tags":["dl2"]},{"location":"100%20Reference%20notes/101%20Literature/Fixing%20the%20train-test%20resolution%20discrepancy/","title":"Fixing the train test resolution discrepancy","text":"Properties authors Hugo Touvron, Andrea Vedaldi, Matthijs Douze, Herv\u00e9 Jegou year 2019 url https://arxiv.org/abs/1906.06423 <p>Abstract</p> <p>Data-augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the typical size of the objects seen by the classifier at train and test time. We experimentally validate that, for a target test resolution, using a lower train resolution offers better classification at test time. We then propose a simple yet effective and efficient strategy to optimize the classifier performance when the train and test resolutions differ. It involves only a computationally cheap fine-tuning of the network at the test resolution. This enables training strong classifiers using small training images. For instance, we obtain 77.1% top-1 accuracy on ImageNet with a ResNet-50 trained on 128x128 images, and 79.8% with one trained on 224x224 image. In addition, if we use extra training data we get 82.5% with the ResNet-50 train with 224x224 images. Conversely, when training a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images at resolution 224x224 and further optimizing for test resolution 320x320, we obtain a test top-1 accuracy of 86.4% (top-5: 98.0%) (single-crop). To the best of our knowledge this is the highest ImageNet single-crop, top-1 and top-5 accuracy to date.</p>","tags":["paper","vit"]},{"location":"100%20Reference%20notes/101%20Literature/Fixing%20the%20train-test%20resolution%20discrepancy/#notes","title":"Notes","text":"<p>train: RandomCrop, Resize test: CenterCrop</p> <p>generates object size discrepancies in train and test images</p> <p></p>","tags":["paper","vit"]},{"location":"100%20Reference%20notes/101%20Literature/FlexTok%20-%20Resampling%20Images%20into%201D%20Token%20Sequences%20of%20Flexible%20Length/","title":"FlexTok   Resampling Images into 1D Token Sequences of Flexible Length","text":"Properties authors Roman Bachmann, Jesse Allardice, David Mizrahi, Enrico Fini, O\u011fuzhan Fatih Kar, Elmira Amirloo, Alaaeldin El-Nouby, Amir Zamir, Afshin Dehghan year 2025 url http://arxiv.org/abs/2502.13967 <p>Abstract</p> <p>Image tokenization has enabled major advances in autoregressive image generation by providing compressed, discrete representations that are more efficient to process than raw pixels. While traditional approaches use 2D grid tokenization, recent methods like TiTok have shown that 1D tokenization can achieve high generation quality by eliminating grid redundancies. However, these methods typically use a fixed number of tokens and thus cannot adapt to an image's inherent complexity. We introduce FlexTok, a tokenizer that projects 2D images into variable-length, ordered 1D token sequences. For example, a 256x256 image can be resampled into anywhere from 1 to 256 discrete tokens, hierarchically and semantically compressing its information. By training a rectified flow model as the decoder and using nested dropout, FlexTok produces plausible reconstructions regardless of the chosen token sequence length. We evaluate our approach in an autoregressive generation setting using a simple GPT-style Transformer. On ImageNet, this approach achieves an FID&lt;2 across 8 to 128 tokens, outperforming TiTok and matching state-of-the-art methods with far fewer tokens. We further extend the model to support to text-conditioned image generation and examine how FlexTok relates to traditional 2D tokenization. A key finding is that FlexTok enables next-token prediction to describe images in a coarse-to-fine \"visual vocabulary\", and that the number of tokens to generate depends on the complexity of the generation task.</p>","tags":["paper","tokenization","vit","computer_vision"]},{"location":"100%20Reference%20notes/101%20Literature/FlexTok%20-%20Resampling%20Images%20into%201D%20Token%20Sequences%20of%20Flexible%20Length/#notes","title":"Notes","text":"<p>Zotero Link</p> <p></p> <p>Figure 5: FlexTok rate-distortion tradeoff. We show ImageNet-1k reconstruction metrics for three different FlexTok sizes. The more tokens used, the closer the reconstructions get to the original RGB images. Scaling the tokenizer size significantly improves reconstruction FID, but is not as crucial in terms of MAE and DreamSim score. For each of the different FlexTok model sizes we use the optimal inference hyperparameters detailed in Appendix F. We show additional reconstruction metrics in Table 8.</p>","tags":["paper","tokenization","vit","computer_vision"]},{"location":"100%20Reference%20notes/101%20Literature/FlexiViT%20-%20One%20Model%20for%20All%20Patch%20Sizes/","title":"FlexiViT   One Model for All Patch Sizes","text":"Properties authors Lucas Beyer, Pavel Izmailov, Alexander Kolesnikov, Mathilde Caron, Simon Kornblith, Xiaohua Zhai, Matthias Minderer, Michael Tschannen, Ibrahim Alabdulmoshin, Filip Pavetic year 2022 url https://arxiv.org/abs/2212.08013 <p>Abstract</p> <p>Vision Transformers convert images to sequences by slicing them into patches. The size of these patches controls a speed/accuracy tradeoff, with smaller patches leading to higher accuracy at greater computational cost, but changing the patch size typically requires retraining the model. In this paper, we demonstrate that simply randomizing the patch size at training time leads to a single set of weights that performs well across a wide range of patch sizes, making it possible to tailor the model to different compute budgets at deployment time. We extensively evaluate the resulting model, which we call FlexiViT, on a wide range of tasks, including classification, image-text retrieval, open-world detection, panoptic segmentation, and semantic segmentation, concluding that it usually matches, and sometimes outperforms, standard ViT models trained at a single patch size in an otherwise identical setup. Hence, FlexiViT training is a simple drop-in improvement for ViT that makes it easy to add compute-adaptive capabilities to most models relying on a ViT backbone architecture. Code and pre-trained models are available at\u00a0this https URL</p>","tags":["paper","foundation_models","computer_vision","transformers","thesis"]},{"location":"100%20Reference%20notes/101%20Literature/FlexiViT%20-%20One%20Model%20for%20All%20Patch%20Sizes/#notes","title":"Notes","text":"<ul> <li>Google already filed a patent for this: https://patents.google.com/patent/US20240169715A1/en</li> </ul>","tags":["paper","foundation_models","computer_vision","transformers","thesis"]},{"location":"100%20Reference%20notes/101%20Literature/Fourier%20Transformer%20-%20Fast%20Long%20Range%20Modeling%20by%20Removing%20Sequence%20Redundancy%20with%20FFT%20Operator/","title":"Fourier Transformer   Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator","text":"Properties authors Ziwei He, Meng Yang, Minwei Feng, Jingcheng Yin, Xinbing Wang, Jingwen Leng, Zhouhan Lin year 2023 url http://arxiv.org/abs/2305.15099 <p>Abstract</p> <p>The transformer model is known to be computationally demanding, and prohibitively costly for long sequences, as the self-attention module uses a quadratic time and space complexity with respect to sequence length. Many researchers have focused on designing new forms of self-attention or introducing new parameters to overcome this limitation, however a large portion of them prohibits the model to inherit weights from large pretrained models. In this work, the transformer's inefficiency has been taken care of from another perspective. We propose Fourier Transformer, a simple yet effective approach by progressively removing redundancies in hidden sequence using the ready-made Fast Fourier Transform (FFT) operator to perform Discrete Cosine Transformation (DCT). Fourier Transformer is able to significantly reduce computational costs while retain the ability to inherit from various large pretrained models. Experiments show that our model achieves state-of-the-art performances among all transformer-based models on the long-range modeling benchmark LRA with significant improvement in both speed and space. For generative seq-to-seq tasks including CNN/DailyMail and ELI5, by inheriting the BART weights our model outperforms the standard BART and other efficient models. Our code is publicly available at https://github.com/LUMIA-Group/FourierTransformer</p>","tags":["paper","dl_theory","spectral"]},{"location":"100%20Reference%20notes/101%20Literature/Fourier%20Transformer%20-%20Fast%20Long%20Range%20Modeling%20by%20Removing%20Sequence%20Redundancy%20with%20FFT%20Operator/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","dl_theory","spectral"]},{"location":"100%20Reference%20notes/101%20Literature/Franca%20-%20Nested%20Matryoshka%20Clustering%20for%20Scalable%20Visual%20Representation%20Learning/","title":"Franca   Nested Matryoshka Clustering for Scalable Visual Representation Learning","text":"Properties authors Shashanka Venkataramanan, Valentinos Pariza, Mohammadreza Salehi, Lukas Knobel, Spyros Gidaris, Elias Ramzi, Andrei Bursuc, Yuki M. Asano year 2025 url http://arxiv.org/abs/2507.14137 <p>Abstract</p> <p>We present Franca (pronounced Fran-ka): free one; the first fully open-source (data, code, weights) vision foundation model that matches and in many cases surpasses the performance of state-of-the-art proprietary models, e.g., DINOv2, CLIP, SigLIPv2, etc. Our approach is grounded in a transparent training pipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and a subset of ReLAION-2B. Beyond model release, we tackle critical limitations in SSL clustering methods. While modern models rely on assigning image features to large codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to account for the inherent ambiguity in clustering semantics. To address this, we introduce a parameter-efficient, multi-head clustering projector based on nested Matryoshka representations. This design progressively refines features into increasingly fine-grained clusters without increasing the model size, enabling both performance and memory efficiency. Additionally, we propose a novel positional disentanglement strategy that explicitly removes positional biases from dense representations, thereby improving the encoding of semantic content. This leads to consistent gains on several downstream benchmarks, demonstrating the utility of cleaner feature spaces. Our contributions establish a new standard for transparent, high-performance vision models and open a path toward more reproducible and generalizable foundation models for the broader AI community. The code and model checkpoints are available at https://github.com/valeoai/Franca.</p>","tags":["paper","foundation_models","computer_vision","vit"]},{"location":"100%20Reference%20notes/101%20Literature/Franca%20-%20Nested%20Matryoshka%20Clustering%20for%20Scalable%20Visual%20Representation%20Learning/#notes","title":"Notes","text":"<p>Zotero Link</p> <p>\"The standard Matryoshka approach slices the encoder\u2019s output along the feature dimension and applies the\u00a0same\u00a0projection head to each sub-embedding. In contrast, we extend this setup by attaching a\u00a0dedicated projection head and clustering objective\u00a0to each subspace. This allows each slice to produce distinct prototypes and prototype assignments, encouraging specialization across representational granularities. Additionally, we reduce the number of prototypes per head proportionally to the subspace size\u2014yielding a form of hierarchical clustering that aligns naturally with the granularity of the features across training steps.\"</p> <p>Interesting, this would make sense if vectors are disjoint, but since each matryoshka emb is a subset (prefix) of the other, then wouldn't it make more sense to have the same projection head? </p>","tags":["paper","foundation_models","computer_vision","vit"]},{"location":"100%20Reference%20notes/101%20Literature/From%20Pixels%20to%20Components%20-%20Eigenvector%20Masking%20for%20Visual%20Representation%20Learning/","title":"From Pixels to Components   Eigenvector Masking for Visual Representation Learning","text":"Properties authors Alice Bizeul, Thomas Sutter, Alain Ryser, Bernhard Sch\u00f6lkopf, Julius von K\u00fcgulgen, Julia E. Vogt year 2025 url https://arxiv.org/abs/2502.06314 <p>Abstract</p> <p>Predicting masked from visible parts of an image is a powerful self-supervised approach for visual representation learning. However, the common practice of masking random patches of pixels exhibits certain failure modes, which can prevent learning meaningful high-level features, as required for downstream tasks. We propose an alternative masking strategy that operates on a suitable transformation of the data rather than on the raw pixels. Specifically, we perform principal component analysis and then randomly mask a subset of components, which accounts for a fixed ratio of the data variance. The learning task then amounts to reconstructing the masked components from the visible ones. Compared to local patches of pixels, the principal components of images carry more global information. We thus posit that predicting masked from visible components involves more high-level features, allowing our masking strategy to extract more useful representations. This is corroborated by our empirical findings which demonstrate improved image classification performance for component over pixel masking. Our method thus constitutes a simple and robust data-driven alternative to traditional masked image modeling approaches.</p>","tags":["paper","vit","ssl","masked_image_modelling","representation_learning"]},{"location":"100%20Reference%20notes/101%20Literature/G-SGD%20-%20Optimizing%20ReLU%20Neural%20Networks%20in%20its%20Positively%20Scale-Invariant%20Space/","title":"G SGD   Optimizing ReLU Neural Networks in its Positively Scale Invariant Space","text":"Properties authors Qi Meng, Shuxin Zheng, Huishuai Zhang, Wei Chen, Zhi-Ming Ma, Tie-Yan Liu year 2018 url https://arxiv.org/abs/1802.03713 <p>Abstract</p> <p>It is well known that neural networks with rectified linear units (ReLU) activation functions are positively scale-invariant. Conventional algorithms like stochastic gradient descent optimize the neural networks in the vector space of weights, which is, however, not positively scale-invariant. This mismatch may lead to problems during the optimization process. Then, a natural question is: \\emph{can we construct a new vector space that is positively scale-invariant and sufficient to represent ReLU neural networks so as to better facilitate the optimization process }? In this paper, we provide our positive answer to this question. First, we conduct a formal study on the positive scaling operators which forms a transformation group, denoted as\u00a0\ue233. We show that the value of a path (i.e. the product of the weights along the path) in the neural network is invariant to positive scaling and prove that the value vector of all the paths is sufficient to represent the neural networks under mild conditions. Second, we show that one can identify some basis paths out of all the paths and prove that the linear span of their value vectors (denoted as\u00a0\ue233-space) is an invariant space with lower dimension under the positive scaling group. Finally, we design stochastic gradient descent algorithm in\u00a0\ue233-space (abbreviated as\u00a0\ue233-SGD) to optimize the value vector of the basis paths of neural networks with little extra cost by leveraging back-propagation. Our experiments show that\u00a0\ue233-SGD significantly outperforms the conventional SGD algorithm in optimizing ReLU networks on benchmark datasets.</p>","tags":["dl_theory","dl2"]},{"location":"100%20Reference%20notes/101%20Literature/Grokked%20Transformers%20are%20Implicit%20Reasoners%20-%20A%20Mechanistic%20Journey%20to%20the%20Edge%20of%20Generalization/","title":"Grokked Transformers are Implicit Reasoners   A Mechanistic Journey to the Edge of Generalization","text":"Properties authors Boshi Wang, Xiang Yue, Yu Su, Huan Sun year 2024 url https://arxiv.org/abs/2405.15071 <p>Abstract</p> <p>We study whether transformers can learn to implicitly reason over parametric knowledge, a skill that even the most capable language models struggle with. Focusing on two representative reasoning types, composition and comparison, we consistently find that transformers can learn implicit reasoning, but only through grokking, i.e., extended training far beyond overfitting. The levels of generalization also vary across reasoning types: when faced with out-of-distribution examples, transformers fail to systematically generalize for composition but succeed for comparison. We delve into the model's internals throughout training, conducting analytical experiments that reveal: 1) the mechanism behind grokking, such as the formation of the generalizing circuit and its relation to the relative efficiency of generalizing and memorizing circuits, and 2) the connection between systematicity and the configuration of the generalizing circuit. Our findings guide data and training setup to better induce implicit reasoning and suggest potential improvements to the transformer architecture, such as encouraging cross-layer knowledge sharing. Furthermore, we demonstrate that for a challenging reasoning task with a large search space, GPT-4-Turbo and Gemini-1.5-Pro based on non-parametric memory fail badly regardless of prompting styles or retrieval augmentation, while a fully grokked transformer can achieve near-perfect accuracy, showcasing the power of parametric memory for complex reasoning.</p>","tags":["paper","transformers","mechinterp"]},{"location":"100%20Reference%20notes/101%20Literature/Guillotine%20Regularization%20-%20Why%20removing%20layers%20is%20needed%20to%20improve%20generalization%20in%20Self-Supervised%20Learning/","title":"Guillotine Regularization   Why removing layers is needed to improve generalization in Self Supervised Learning","text":"Properties authors Florian Bordes, Randall Balestriero, Quentin Garrido, Adrien Bardes, Pascal Vincent year 2023 url http://arxiv.org/abs/2206.13378 <p>Abstract</p> <p>One unexpected technique that emerged in recent years consists in training a Deep Network (DN) with a Self-Supervised Learning (SSL) method, and using this network on downstream tasks but with its last few projector layers entirely removed. This trick of throwing away the projector is actually critical for SSL methods to display competitive performances on ImageNet for which more than 30 percentage points can be gained that way. This is a little vexing, as one would hope that the network layer at which invariance is explicitly enforced by the SSL criterion during training (the last projector layer) should be the one to use for best generalization performance downstream. But it seems not to be, and this study sheds some light on why. This trick, which we name Guillotine Regularization (GR), is in fact a generically applicable method that has been used to improve generalization performance in transfer learning scenarios. In this work, we identify the underlying reasons behind its success and show that the optimal layer to use might change significantly depending on the training setup, the data or the downstream task. Lastly, we give some insights on how to reduce the need for a projector in SSL by aligning the pretext SSL task and the downstream task.</p>","tags":["paper","dl_theory","regularization","ssl","representation_learning"]},{"location":"100%20Reference%20notes/101%20Literature/Guillotine%20Regularization%20-%20Why%20removing%20layers%20is%20needed%20to%20improve%20generalization%20in%20Self-Supervised%20Learning/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","dl_theory","regularization","ssl","representation_learning"]},{"location":"100%20Reference%20notes/101%20Literature/Harmonics%20of%20Learning%20-%20Universal%20Fourier%20Features%20Emerge%20in%20Invariant%20Networks/","title":"Harmonics of Learning   Universal Fourier Features Emerge in Invariant Networks","text":"Properties authors Giovanni Luca Marchetti, Christopher Hillar, Danica Kragic, Sophia Sanborn year 2023 url https://arxiv.org/abs/2312.08550 <p>Abstract</p> <p>In this work, we formally prove that, under certain conditions, if a neural network is invariant to a finite group then its weights recover the Fourier transform on that group. This provides a mathematical explanation for the emergence of Fourier features -- a ubiquitous phenomenon in both biological and artificial learning systems. The results hold even for non-commutative groups, in which case the Fourier transform encodes all the irreducible unitary group representations. Our findings have consequences for the problem of symmetry discovery. Specifically, we demonstrate that the algebraic structure of an unknown group can be recovered from the weights of a network that is at least approximately invariant within certain bounds. Overall, this work contributes to a foundation for an algebraic learning theory of invariant neural network representations.</p> <p></p>","tags":["theory","equivariance","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/HoPE%20-%20A%20Novel%20Positional%20Encoding%20Without%20Long-Term%20Decay%20for%20Enhanced%20Context%20Awareness%20and%20Extrapolation/","title":"HoPE   A Novel Positional Encoding Without Long Term Decay for Enhanced Context Awareness and Extrapolation","text":"Properties authors Yuhan Chen, Ang Lv, Jian Luan, Bin Wang, Wei Lu year 2024 url https://arxiv.org/abs/2410.21216 <p>Abstract</p> <p>Many positional encodings (PEs) are designed to exhibit long-term decay, based on an entrenched and long-standing inductive opinion: tokens farther away from the current position carry less relevant information. We argue that long-term decay is outdated in the era of LLMs, as LLMs are now applied to tasks demanding precise retrieval of in-context information from arbitrary positions. Firstly, we present empirical analyses on various PEs, demonstrating that models inherently learn attention with only a local-decay pattern while forming a U-shape pattern globally, contradicting the principle of long-term decay. Furthermore, we conduct a detailed analysis of rotary position encoding (RoPE, a prevalent relative positional encoding in LLMs), and found that the U-shape attention is caused by some learned components, which are also the key factor limiting RoPE's expressiveness and\u00a0this http URL\u00a0by these insights, we propose High-frequency rotary Position Encoding (HoPE). HoPE replaces the specific components in RoPE with position-independent ones, retaining only high-frequency signals, which also breaks the principle of long-term decay in theory. HoPE achieves two major advantages: (1) Without constraints imposed by long-term decay, contradictory factors that limit spontaneous attention optimization and model extrapolation performance are removed. (2) Components representing positions and semantics are are optimized. These enhances model's context awareness and extrapolation, as validated by extensive experiments.</p>","tags":["paper","dl_theory","transformers","posembed"]},{"location":"100%20Reference%20notes/101%20Literature/How%20Does%20SimSiam%20Avoid%20Collapse%20Without%20Negative%20Samples%20A%20Unified%20Understanding%20with%20Self-supervised%20Contrastive%20Learning/","title":"How Does SimSiam Avoid Collapse Without Negative Samples A Unified Understanding with Self supervised Contrastive Learning","text":"Properties authors Chaoning Zhang, Kang Zhang, Chenshuang Zhang, Trung X. Pham, Chang D. Yoo, In So Kweon year 2022 url http://arxiv.org/abs/2203.16262 <p>Abstract</p> <p>To avoid collapse in self-supervised learning (SSL), a contrastive loss is widely used but often requires a large number of negative samples. Without negative samples yet achieving competitive performance, a recent work (Chen &amp; He, 2021) has attracted signi\ufb01cant attention for providing a minimalist simple Siamese (SimSiam) method to avoid collapse. However, the reason for how it avoids collapse without negative samples remains not fully clear and our investigation starts by revisiting the explanatory claims in the original SimSiam. After refuting their claims, we introduce vector decomposition for analyzing the collapse based on the gradient analysis of the l2-normalized representation vector. This yields a uni\ufb01ed perspective on how negative samples and SimSiam alleviate collapse. Such a uni\ufb01ed perspective comes timely for understanding the recent progress in SSL.</p>","tags":["paper","dl_theory","computer_vision","contrastive_learning","ssl"]},{"location":"100%20Reference%20notes/101%20Literature/How%20Does%20SimSiam%20Avoid%20Collapse%20Without%20Negative%20Samples%20A%20Unified%20Understanding%20with%20Self-supervised%20Contrastive%20Learning/#notes","title":"Notes","text":"<p>Zotero Link</p> <ul> <li> Read Exploring Simple Siamese Representation Learning</li> <li> Read How Does SimSiam Avoid Collapse Without Negative Samples A Unified Understanding with Self-supervised Contrastive Learning</li> </ul> <p>Cool blog: https://www.nowozin.net/sebastian/blog/thoughts-on-trace-estimation-in-deep-learning.html</p> <p>Now I undestand better </p> <p>Here\u2019s a concise recap of the unified theoretical framework for covariance-regularized SSL methods:</p> <ol> <li> <p>Core Objective     Every method optimizes</p> <p>L\u2005\u200a=\u2005\u200aLalign(z1,z2)\u23dfenforce\u00a0view\u2010invariance\u2005\u200a+\u2005\u200a\u03bb\u2009\u03a9\u2009\u2063(\u03a3)\u23dfprevent\u00a0collapse\u00a0by\u00a0\u201cexpanding\u201d\u00a0features,\\mathcal L \\;=\\;\\underbrace{\\mathcal L_{\\rm align}(z_1,z_2)}{\\text{enforce view\u2010invariance}} \\;+\\;\\lambda\\,\\underbrace{\\Omega!\\bigl(\\Sigma\\bigr)}{\\text{prevent collapse by \u201cexpanding\u201d features}},</p> <p>where \u03a3\\Sigma is the batch covariance of the dd-dimensional embeddings.</p> </li> <li> <p>Alignment Term</p> <ul> <li> <p>Matches two augmented views of the same image (e.g. MSE or cosine loss).</p> </li> <li> <p>Drives invariance but by itself admits the trivial solution z\u2261z\\equiv constant.</p> </li> </ul> </li> <li> <p>Regularizer \u03a9(\u03a3)\\Omega(\\Sigma)     Encodes two essential second-moment effects (per Zhang et al., 2022):</p> <ul> <li> <p>De-centering: enforces non-zero variance in every dimension (so features can\u2019t collapse to a constant).</p> </li> <li> <p>De-correlation: penalizes linear correlations between dimensions (so features can\u2019t collapse onto a lower\u2010dimensional subspace).</p> </li> </ul> </li> <li> <p>VICReg vs. Coding-Rate (SimDINO)</p> <ul> <li> <p>Coding-rate uses the exact log-det term     R\u03b5(\u03a3)=12log\u2061det\u2061(I+\u03b1\u03a3)\\displaystyle R_\\varepsilon(\\Sigma)=\\tfrac12\\log\\det\\bigl(I+\\alpha\\Sigma\\bigr),     which simultaneously maximizes variance (all eigenvalues &gt; 0) and enforces isotropy (decorrelation).</p> </li> <li> <p>VICReg approximates \u2212R\u03b5-R_\\varepsilon via its second-order Taylor expansion around \u03a3=0\\Sigma=0:</p> <p>\u2212log\u2061det\u2061(I+\u03b1\u03a3)\u2005\u200a\u2248\u2005\u200a\u2212\u03b1\u2009tr\u2009\u03a3\u2005\u200a+\u2005\u200a\u03b122\u2009tr\u2009\u03a32, -\\log\\det(I+\\alpha\\Sigma) \\;\\approx\\; -\\alpha\\,\\mathrm{tr}\\,\\Sigma \\;+\\;\\tfrac{\\alpha<sup>2}{2}\\,\\mathrm{tr}\\,\\Sigma</sup>2,</p> <p>yielding two simple surrogates:</p> <ul> <li> <p>A variance hinge \u2211jmax\u2061(0,1\u2212\u03c3j)2\\sum_j\\max(0,1-\\sigma_j)^2 enforcing \u03c3j&gt;1\\sigma_j&gt;1,</p> </li> <li> <p>A covariance penalty \u2211i\u2260j\u03a3ij2\\sum_{i\\neq j}\\Sigma_{ij}^2 pushing off-diagonals to zero.</p> </li> </ul> </li> </ul> </li> <li> <p>Information-Theoretic Interpretation</p> <ul> <li> <p>log\u2061det\u2061\u03a3\\log\\det\\Sigma (up to constants) is the differential entropy of a Gaussian with covariance \u03a3\\Sigma.</p> </li> <li> <p>Maximizing R\u03b5R_\\varepsilon under the alignment constraint is an instance of InfoMax (maximize representation entropy while matching positives).</p> </li> <li> <p>Contrasting contrastive losses (InfoNCE) and these non-contrastive criteria reveals they are dual formulations of the same underlying entropy-alignment trade-off.</p> </li> </ul> </li> <li> <p>Practical Consequences</p> <ul> <li> <p>Stability &amp; Robustness: explicit \u03a9\\Omega avoids the need for large negatives, momentum encoders, or architectural tricks\u2014works with moderate batch sizes.</p> </li> <li> <p>Interpretability: VICReg\u2019s variance/covariance terms correspond directly to whitening; coding-rate gives a principled entropy measure.</p> </li> <li> <p>Hyperparameters: surrogates (VICReg) need two weights (\u03bbvar,\u03bbcov\\lambda_{\\rm var},\\lambda_{\\rm cov}); coding-rate needs just one strength parameter \u03b3\\gamma (plus \u03b5\\varepsilon).</p> </li> </ul> </li> </ol> <p>Take-home: All successful negative-free SSL methods boil down to \u201calign your positives\u201d and \u201ckeep your covariance full-rank and (ideally) isotropic.\u201d VICReg-style losses do this by simple variance + covariance penalties; coding-rate methods do it by directly maximizing a log-det entropy objective.</p> <p>Here\u2019s a concise explanation:</p> <p>Decorrelation in SSL means removing linear correlations between different feature\u2010dimensions so that each coordinate carries unique information. Concretely, if Z\u2208RB\u00d7dZ\\in\\mathbb R^{B\\times d} are batch embeddings, their (empirical) covariance</p> <p>\u03a3=1B\u2009(Z\u2212Z\u02c9)\u22a4(Z\u2212Z\u02c9)\\Sigma = \\frac1B\\,(Z-\\bar Z)^\\top(Z-\\bar Z)</p> <p>has off-diagonal entries \u03a3ij\\Sigma_{ij} measuring linear correlation between dimensions ii and jj. Decorrelation drives \u03a3ij\u21920\\Sigma_{ij}\\to0 for i\u2260ji\\neq j, ensuring the learned features span the full dd-dimensional subspace rather than collapse onto a lower-dimensional manifold (Wikipedia).</p> <ul> <li> <p>In VICReg, this is done explicitly by adding a term \u2211i\u2260j\u03a3ij2\\sum_{i\\neq j}\\Sigma_{ij}^2 to the loss, which directly penalizes off-diagonal covariance (arXiv).</p> </li> <li> <p>More generally, feature decorrelation has been shown to prevent dimensional collapse\u2014where all information piles into a few axes\u2014by standardizing the covariance matrix toward an identity form (arXiv).</p> </li> </ul>","tags":["paper","dl_theory","computer_vision","contrastive_learning","ssl"]},{"location":"100%20Reference%20notes/101%20Literature/How%20Does%20SimSiam%20Avoid%20Collapse%20Without%20Negative%20Samples%20A%20Unified%20Understanding%20with%20Self-supervised%20Contrastive%20Learning/#why-does-the-coding-rate-regularizer-also-decorrelate","title":"Why does the coding rate regularizer also decorrelate?","text":"<p>The coding-rate regularizer used in SimDINO (and related Maximal Coding Rate Reduction (MCR\u00b2) methods) is</p> <p>R\u03b5(\u03a3)=12log\u2061det\u2061\u2009\u2063(I+d\u03b52\u2009\u03a3),R_\\varepsilon(\\Sigma) =\\tfrac12\\log\\det!\\bigl(I + \\tfrac{d}{\\varepsilon^2}\\,\\Sigma\\bigr),</p> <p>and the SSL loss includes \u2212\u03b3\u2009R\u03b5(\u03a3)-\\gamma\\,R_\\varepsilon(\\Sigma) to maximize this quantity (arXiv).</p> <ol> <li> <p>Determinant \u221d volume: det\u2061(\u03a3)\\det(\\Sigma) equals the squared volume of the ellipsoid defined by \u03a3\\Sigma; the term log\u2061det\u2061(I+\u03b1\u03a3)\\log\\det(I+\\alpha\\Sigma) thus measures the log-volume of the representation cloud (Wikipedia).</p> </li> <li> <p>Maximizing volume enforces spread: To maximize det\u2061\\det, the model must push all eigenvalues \u03bbi(\u03a3)\\lambda_i(\\Sigma) away from zero\u2014i.e. preserve variance in every direction.</p> </li> <li> <p>Isotropy \u2192 decorrelation: For a fixed total variance \u2211i\u03bbi\\sum_i \\lambda_i, the product \u220fi\u03bbi\\prod_i\\lambda_i (hence det\u2061\\det) is maximized precisely when all \u03bbi\\lambda_i are equal; that is, \u03a3\\Sigma becomes proportional to the identity, which implies zero off-diagonal entries (perfect decorrelation) (Mathematics Stack Exchange).</p> </li> <li> <p>Global surrogate: Unlike VICReg\u2019s separate variance and off-diagonal penalties, log\u2061det\u2061\\log\\det is a single global measure of entropy (volume) that inherently couples variance preservation and decorrelation\u2014a principled InfoMax criterion under Gaussian assumptions (arXiv).</p> </li> </ol> <p>Put simply, by maximizing the coding rate (log-det of the covariance), the model is driven to occupy an isotropic, full-rank region in feature space, which automatically decorrelates the dimensions without needing an explicit off-diagonal penalty (Cross Validated, ICML).</p> <p>References</p> <ul> <li> <p>Decorrelation definition: Wikipedia \u201cDecorrelation\u201d (Wikipedia)</p> </li> <li> <p>Dimensional collapse &amp; need for decorrelation: Hua et al. (2021) (arXiv)</p> </li> <li> <p>VICReg\u2019s explicit covariance penalty: Bardes et al. (2021) (arXiv)</p> </li> <li> <p>Determinant as volume: Wikipedia \u201cDeterminant\u201d (Wikipedia)</p> </li> <li> <p>Geometric intuition of det\u2061(\u03a3)\\det(\\Sigma): Math.SE (Mathematics Stack Exchange)</p> </li> <li> <p>Log-det in InfoMax SSL: Statistics.SE on log-det \ufe59Jacobian, log-likelihood\ufe5a (Cross Validated)</p> </li> <li> <p>SimDINO coding rate reg.: Wu et al. (2025) \u201cSimplifying DINO\u2026\u201d (arXiv)</p> </li> <li> <p>MCR\u00b2 principle: Yu et al. (2020) (arXiv)</p> </li> <li> <p>CorInfoMax\u2019s second-order MI: Ozsoy et al. (2022) (NeurIPS Proceedings)</p> </li> <li> <p>Matrix Info Theory unifying view: Zhang et al. (ICML 2024) (ICML)</p> </li> </ul>","tags":["paper","dl_theory","computer_vision","contrastive_learning","ssl"]},{"location":"100%20Reference%20notes/101%20Literature/How%20JEPA%20Avoids%20Noisy%20Features%20-%20The%20Implicit%20Bias%20of%20DeepLinear%20Self%20Distillation%20Networks/","title":"How JEPA Avoids Noisy Features   The Implicit Bias of DeepLinear Self Distillation Networks","text":"Properties authors Etai Littwin, Omid Saremi, Madhu Advani, Vimal Thilak, Preetum Nakkiran, Chen Huang, Joshua Susskind year 2024 url https://arxiv.org/abs/2407.03475 <p>Abstract</p> <p>Two competing paradigms exist for self-supervised learning of data representations. Joint Embedding Predictive Architecture (JEPA) is a class of architectures in which semantically similar inputs are encoded into representations that are predictive of each other. A recent successful approach that falls under the JEPA framework is self-distillation, where an online encoder is trained to predict the output of the target encoder, sometimes using a lightweight predictor network. This is contrasted with the Masked AutoEncoder (MAE) paradigm, where an encoder and decoder are trained to reconstruct missing parts of the input in the data space rather, than its latent representation. A common motivation for using the JEPA approach over MAE is that the JEPA objective prioritizes abstract features over fine-grained pixel information (which can be unpredictable and uninformative). In this work, we seek to understand the mechanism behind this empirical observation by analyzing the training dynamics of deep linear models. We uncover a surprising mechanism: in a simplified linear setting where both approaches learn similar representations, JEPAs are biased to learn high-influence features, i.e., features characterized by having high regression coefficients. Our results point to a distinct implicit bias of predicting in latent space that may shed light on its success in practice.</p>","tags":["paper","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/How%20do%20vision%20transformers%20work%3F/","title":"How do vision transformers work?","text":"Properties authors Namuk Park, Songkuk Kim year 2022 url https://arxiv.org/abs/2202.06709 <p>Abstract</p> <p>The success of multi-head self-attentions (MSAs) for computer vision is now indisputable. However, little is known about how MSAs work. We present fundamental explanations to help better understand the nature of MSAs. In particular, we demonstrate the following properties of MSAs and Vision Transformers (ViTs): (1) MSAs improve not only accuracy but also generalization by flattening the loss landscapes. Such improvement is primarily attributable to their data specificity, not long-range dependency. On the other hand, ViTs suffer from non-convex losses. Large datasets and loss landscape smoothing methods alleviate this problem; (2) MSAs and Convs exhibit opposite behaviors. For example, MSAs are low-pass filters, but Convs are high-pass filters. Therefore, MSAs and Convs are complementary; (3) Multi-stage neural networks behave like a series connection of small individual models. In addition, MSAs at the end of a stage play a key role in prediction. Based on these insights, we propose AlterNet, a model in which Conv blocks at the end of a stage are replaced with MSA blocks. AlterNet outperforms CNNs not only in large data regimes but also in small data regimes. The code is available at\u00a0this https URL.</p>","tags":["vit","computer_vision","cnn","optimizability"]},{"location":"100%20Reference%20notes/101%20Literature/How%20do%20vision%20transformers%20work%3F/#notes","title":"Notes","text":"","tags":["vit","computer_vision","cnn","optimizability"]},{"location":"100%20Reference%20notes/101%20Literature/How%20do%20vision%20transformers%20work%3F/#the-question-of-inductive-biases","title":"The question of inductive biases","text":"<p>Contrary to our expectations, experimental results show that the stronger the inductive bias, the lower both the test error and the training NLL. This indicates that ViT does not overfit training datasets. In addition, appropriate inductive biases, such as locality constraints for MSAs, helps NNs learn strong representations. We also observe these phenomena on CIFAR-10 and ImageNet as shown in Fig. C.1. Figure C.2 also supports that weak inductive biases disrupt NN training. In this experiment, extremely small patch sizes for the embedding hurt the predictive performance of ViT.</p> <p>Long range (global) attention is worse than local attention. MSA are good because they smooth loss landscape and are input dependent.</p> <p>What properties of MSAs do we need to improve optimization? We present various evidences to support that MSA is generalized spatial smoothing. It means that MSAs improve performance because their formulation\u2014Eq. (1)\u2014is an appropriate inductive bias. Their weak inductive bias disrupts NN training. In particular, a key feature of MSAs is their data specificity, not long-range dependency. As an extreme example, local MSAs with a 3 \u00d7 3 receptive field outperforms global MSA because they reduce unnecessary degrees of freedom. </p> <p>As far as my understanding goes, local MSA is not translation equivariant because it still is input dependent. So Local MSA has locality inductive bias but not translation equivariance. This is interesting, normal ConvNets do locality inductive bias by translation equivariance and it is not straight forward to remove their translation equivariance. Tracking at Input-dependent convolutions and Non-translationally equivariant convolutions.</p> <p>Locality inductive biases help with more stable training dynamics </p>","tags":["vit","computer_vision","cnn","optimizability"]},{"location":"100%20Reference%20notes/101%20Literature/How%20do%20vision%20transformers%20work%3F/#hessian-spectra","title":"Hessian Spectra","text":"<p> Legend:  ViT (red), CNN (blue) - ViT has small magnitude and negative values - CNN has large magnitude and positive values</p>","tags":["vit","computer_vision","cnn","optimizability"]},{"location":"100%20Reference%20notes/101%20Literature/How%20to%20represent%20part-whole%20hierarchies%20in%20a%20neural%20network/","title":"How to represent part whole hierarchies in a neural network","text":"Properties authors Geoffrey Hinton year 2021 url http://arxiv.org/abs/2102.12627 <p>Abstract</p> <p>This paper does not describe a working system. Instead, it presents a single idea about representation which allows advances made by several di\ufb00erent groups to be combined into an imaginary system called GLOM1. The advances include transformers, neural \ufb01elds, contrastive representation learning, distillation and capsules. GLOM answers the question: How can a neural network with a \ufb01xed architecture parse an image into a partwhole hierarchy which has a di\ufb00erent structure for each image? The idea is simply to use islands of identical vectors to represent the nodes in the parse tree. If GLOM can be made to work, it should signi\ufb01cantly improve the interpretability of the representations produced by transformer-like systems when applied to vision or language.</p>","tags":["paper","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/How%20to%20represent%20part-whole%20hierarchies%20in%20a%20neural%20network/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Hydra%20-%20Bidirectional%20State%20Space%20Models%20Through%20Generalized%20Matrix%20Mixers/","title":"Hydra   Bidirectional State Space Models Through Generalized Matrix Mixers","text":"Properties authors Sukjun Hwang, Aakash Lahoti, Tri Dao, Albert Gu year 2024 url https://arxiv.org/abs/2407.09941 <p>Abstract</p> <p>A wide array of sequence models are built on a framework modeled after Transformers, comprising alternating sequence mixer and channel mixer layers. This paper studies a unifying matrix mixer view of sequence mixers that can be conceptualized as a linear map on the input sequence. This framework encompasses a broad range of well-known sequence models, including the self-attention of Transformers as well as recent strong alternatives such as structured state space models (SSMs), and allows understanding downstream characteristics such as efficiency and expressivity through properties of their structured matrix class. We identify a key axis of matrix parameterizations termed sequence alignment, which increases the flexibility and performance of matrix mixers, providing insights into the strong performance of Transformers and recent SSMs such as Mamba. Furthermore, the matrix mixer framework offers a systematic approach to developing sequence mixers with desired properties, allowing us to develop several new sub-quadratic sequence models. In particular, we propose a natural bidirectional extension of the Mamba model (Hydra), parameterized as a quasiseparable matrix mixer, which demonstrates superior performance over other sequence models including Transformers on non-causal tasks. As a drop-in replacement for attention layers, Hydra outperforms BERT by 0.8 points on the GLUE benchmark and ViT by 2% Top-1 accuracy on ImageNet.</p>","tags":["paper","sequence_models"]},{"location":"100%20Reference%20notes/101%20Literature/HydraViT%20-%20Stacking%20Heads%20for%20a%20Scalable%20ViT/","title":"HydraViT   Stacking Heads for a Scalable ViT","text":"Properties authors Janek Haberer, Ali Hojjat, Olaf Landsiedel year 2024 <p>Abstract</p> <p>The architecture of Vision Transformers (ViTs), particularly the Multi-head Attention (MHA) mechanism, imposes substantial hardware demands. Deploying ViTs on devices with varying constraints, such as mobile phones, requires multiple models of different sizes. However, this approach has limitations, such as training and storing each required model separately. This paper introduces HydraViT, a novel approach that addresses these limitations by stacking attention heads to achieve a scalable ViT. By repeatedly changing the size of the embedded dimensions throughout each layer and their corresponding number of attention heads in MHA during training, HydraViT induces multiple subnetworks. Thereby, HydraViT achieves adaptability across a wide spectrum of hardware environments while maintaining performance. Our experimental results demonstrate the efficacy of HydraViT in achieving a scalable ViT with up to 10 subnetworks, covering a wide range of resource constraints. HydraViT achieves up to 5 p.p. more accuracy with the same GMACs and up to 7 p.p. more accuracy with the same throughput on ImageNet-1K compared to the baselines, making it an effective solution for scenarios where hardware availability is diverse or varies over time. The source code is available at https://github.com/ds-kiel/HydraViT.</p>","tags":["paper","flexible_vit","vit","efficient_vision"]},{"location":"100%20Reference%20notes/101%20Literature/HydraViT%20-%20Stacking%20Heads%20for%20a%20Scalable%20ViT/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","flexible_vit","vit","efficient_vision"]},{"location":"100%20Reference%20notes/101%20Literature/Hyperspherical%20Variational%20Auto-Encoders/","title":"Hyperspherical Variational Auto Encoders","text":"Properties authors Tim R. Davidson, Luca Falorsi, Nicola de Cao, Thomas Kipf, Jakub M. Tomczak year 2018 url https://arxiv.org/abs/1804.00891 <p>Abstract</p> <p>The Variational Auto-Encoder (VAE) is one of the most used unsupervised machine learning models. But although the default choice of a Gaussian distribution for both the prior and posterior represents a mathematically convenient distribution often leading to competitive results, we show that this parameterization fails to model data with a latent hyperspherical structure. To address this issue we propose using a von Mises-Fisher (vMF) distribution instead, leading to a hyperspherical latent space. Through a series of experiments we show how such a hyperspherical VAE, or\u00a0\ue23f-VAE, is more suitable for capturing data with a hyperspherical latent structure, while outperforming a normal,\u00a0\ue23a-VAE, in low dimensions on other data types. Code at\u00a0this http URL\u00a0and\u00a0this https URL</p>","tags":["paper","geometric_dl"]},{"location":"100%20Reference%20notes/101%20Literature/Hyperspherical%20Variational%20Auto-Encoders/#notes","title":"Notes","text":"<ul> <li>\"However, even for\u00a0m&gt;20\u00a0we observe a\u00a0vanishing surface problem\u00a0(see Figure\u00a06\u00a0in Appendix\u00a0E). This could thus lead to unstable behavior of hyperspherical models in high dimensions.\"<ul> <li>Basically, the hypesphere's surface area starts collapsing on high dimensions (m&gt;20), which makes it unsuitable choice, as embeddings in this manifold lose discriminative power. This is backed by the paper's results, where s-vae outperforms n-vae up to d=40.</li> <li> </li> </ul> </li> </ul>","tags":["paper","geometric_dl"]},{"location":"100%20Reference%20notes/101%20Literature/Improving%20Convergence%20and%20Generalization%20Using%20Parameter%20Symmetries/","title":"Improving Convergence and Generalization Using Parameter Symmetries","text":"Properties authors Bo Zhao, Robert M Gower, Robin Walters, Rose Yu year 2023 url https://arxiv.org/abs/2305.13404 <p>Abstract</p> <p>In overparametrized models, different values of the parameters may result in the same loss value. Parameter space symmetries are transformations that change the model parameters but leave the loss invariant. Teleportation applies such transformations to accelerate optimization. However, the exact mechanism behind this algorithm's success is not well understood. In this paper, we show that teleportation not only speeds up optimization in the short-term, but gives overall faster time to convergence. Additionally, we show that teleporting to minima with different curvatures improves generalization and provide insights on the connection between the curvature of the minima and generalization ability. Finally, we show that integrating teleportation into a wide range of optimization algorithms and optimization-based meta-learning improves convergence.</p>","tags":["equivariance","relaxed_equivariance","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Improving%20Self%20Consistency%20in%20LLMs%20through%20Probabilistic%20Tokenization/","title":"Improving Self Consistency in LLMs through Probabilistic Tokenization","text":"Properties authors Ashutosh Sathe, Divyanshu Aggarwal, Sunayana Sitaram year 2024 url https://arxiv.org/abs/2407.03678v1 <p>Abstract</p> <p>Prior research has demonstrated noticeable performance gains through the use of probabilistic tokenizations, an approach that involves employing multiple tokenizations of the same input string during the training phase of a language model. Despite these promising findings, modern large language models (LLMs) have yet to be trained using probabilistic tokenizations. Interestingly, while the tokenizers of these contemporary LLMs have the capability to generate multiple tokenizations, this property remains underutilized.  </p> <p>In this work, we propose a novel method to leverage the multiple tokenization capabilities of modern LLM tokenizers, aiming to enhance the self-consistency of LLMs in reasoning tasks. Our experiments indicate that when utilizing probabilistic tokenizations, LLMs generate logically diverse reasoning paths, moving beyond mere surface-level linguistic\u00a0this http URL\u00a0carefully study probabilistic tokenization and offer insights to explain the self consistency improvements it brings through extensive experimentation on 5 LLM families and 4 reasoning benchmarks.</p>","tags":["paper","llm","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Improving%20Self%20Consistency%20in%20LLMs%20through%20Probabilistic%20Tokenization/#notes","title":"Notes","text":"<p>\"Our analysis shows that the primary reason for success of probabilistic tokenization on reasoning tasks is its ability to generate logically diverse reasoning paths.\" </p>","tags":["paper","llm","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/In%20Search%20of%20Projectively%20Equivariant%20Networks/","title":"In Search of Projectively Equivariant Networks","text":"Properties authors Georg Bokman, Axel Flinth, Fredrik Kahl year 2022 url https://arxiv.org/abs/2209.14719 <p>Abstract</p> <p>Equivariance of linear neural network layers is well studied. In this work, we relax the equivariance condition to only be true in a projective sense. We propose a way to construct a projectively equivariant neural network through building a standard equivariant network where the linear group representations acting on each intermediate feature space are\"multiplicatively modified lifts\"of projective group representations. By theoretically studying the relation of projectively and linearly equivariant linear layers, we show that our approach is the most general possible when building a network out of linear layers. The theory is showcased in two simple experiments.</p>","tags":["equivariance","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Into%20the%20Rabbit%20Hull%20-%20From%20Task-Relevant%20Concepts%20in%20DINO%20to%20Minkowski%20Geometry/","title":"Into the Rabbit Hull   From Task Relevant Concepts in DINO to Minkowski Geometry","text":"Properties authors Thomas Fel, Binxu Wang, Michael A. Lepori, Matthew Kowal, Andrew Lee, Randall Balestriero, Sonia Joseph, Ekdeep S. Lubana, Talia Konkle, Demba Ba, Martin Wattenberg year 2025 url http://arxiv.org/abs/2510.08638 <p>Abstract</p> <p>DINOv2 is routinely deployed to recognize objects, scenes, and actions; yet the nature of what it perceives remains unknown. As a working baseline, we adopt the Linear Representation Hypothesis (LRH) and operationalize it using SAEs, producing a 32,000-unit dictionary that serves as the interpretability backbone of our study, which unfolds in three parts. In the first part, we analyze how different downstream tasks recruit concepts from our learned dictionary, revealing functional specialization: classification exploits \"Elsewhere\" concepts that fire everywhere except on target objects, implementing learned negations; segmentation relies on boundary detectors forming coherent subspaces; depth estimation draws on three distinct monocular depth cues matching visual neuroscience principles. Following these functional results, we analyze the geometry and statistics of the concepts learned by the SAE. We found that representations are partly dense rather than strictly sparse. The dictionary evolves toward greater coherence and departs from maximally orthogonal ideals (Grassmannian frames). Within an image, tokens occupy a low dimensional, locally connected set persisting after removing position. These signs suggest representations are organized beyond linear sparsity alone. Synthesizing these observations, we propose a refined view: tokens are formed by combining convex mixtures of archetypes (e.g., a rabbit among animals, brown among colors, fluffy among textures). This structure is grounded in Gardenfors' conceptual spaces and in the model's mechanism as multi-head attention produces sums of convex mixtures, defining regions bounded by archetypes. We introduce the Minkowski Representation Hypothesis (MRH) and examine its empirical signatures and implications for interpreting vision-transformer representations.</p>","tags":["paper","dl_theory","ssl"]},{"location":"100%20Reference%20notes/101%20Literature/Into%20the%20Rabbit%20Hull%20-%20From%20Task-Relevant%20Concepts%20in%20DINO%20to%20Minkowski%20Geometry/#notes","title":"Notes","text":"<p>Zotero Link</p> <p>Figure 6: DINO encodes diverse monocular depth cues. Visualization of key concepts used in monocular depth estimation tasks. We identify three dominant types: projective geometry cues (e.g., vanishing lines, converging structures), shadow-based cues (e.g., soft lighting gradients and cast shadows), and frequency-based cues (e.g., transitions between high- and low-texture regions). These findings suggest that DINO learns a rich basis of 3D perception primitives from 2D data alone.</p> <p>\"\"\" We trained linear decoders to decode the token position from each layer\u2019s activations and extracted the corresponding decoding vectors at each spatial location (i, j) to form a position encoding matrix P \u2208 R256\u00d7d at each layer (see Section I for more details) [...]u Still in Figure 14, we showed that the position basis is not responsible for the main part of the structure observed in the PCA visualization. To test qualitatively this effect, we project token embeddings orthogonal to the positional subspace, completely removing positional information by projecting the token on the orthogonal subspace of the classifier. \"\"\"</p> <p>This is effectively the same as Franca's RASA.</p>","tags":["paper","dl_theory","ssl"]},{"location":"100%20Reference%20notes/101%20Literature/Intuitive%20physics%20understanding%20emerges%20from%20self-supervised%20pretraining%20on%20natural%20videos/","title":"Intuitive physics understanding emerges from self supervised pretraining on natural videos","text":"Properties authors Quentin Garrido, Nicolas Ballas, Mahmoud Assran, Adrien Bardes, Laurent Najman, Michael Rabbat, Emmanuel Dupoux, Yann LeCun year 2025 url http://arxiv.org/abs/2502.11831 <p>Abstract</p> <p>We investigate the emergence of intuitive physics understanding in general-purpose deep neural network models trained to predict masked regions in natural videos. Leveraging the violation-of-expectation framework, we find that video prediction models trained to predict outcomes in a learned representation space demonstrate an understanding of various intuitive physics properties, such as object permanence and shape consistency. In contrast, video prediction in pixel space and multimodal large language models, which reason through text, achieve performance closer to chance. Our comparisons of these architectures reveal that jointly learning an abstract representation space while predicting missing parts of sensory input, akin to predictive coding, is sufficient to acquire an understanding of intuitive physics, and that even models trained on one week of unique video achieve above chance performance. This challenges the idea that core knowledge -- a set of innate systems to help understand the world -- needs to be hardwired to develop an understanding of intuitive physics.</p>","tags":["paper","world-models","video","ssl"]},{"location":"100%20Reference%20notes/101%20Literature/Intuitive%20physics%20understanding%20emerges%20from%20self-supervised%20pretraining%20on%20natural%20videos/#notes","title":"Notes","text":"<p>Zotero Link</p> <p>The mechanism consisting of predicting in representation space is congruentwith the predictive coding hypothesis of cognitive neuroscience</p> <p>violation-of-expectation framework to probe for intuitive physics understanding without requiring any task-specific training or adaptation</p> <p>directly from the input withouthard-wiring a cascade of intermediate representations like object contours or pose estimation</p> <p>prediction error \u2014 the distance between thepredicted video representations and the actual encoded video representations \u2014 at each time-step, we obtaina temporally aligned quantitative measure of the model\u2019s surprise throughout the video</p> <p>dev set of IntPhys</p> <p>object permanence</p> <p>continuity</p> <p>shape</p> <p>color constancy</p> <p>gravity</p> <p>support</p> <p>solidity</p> <p>inertia</p> <p>collision</p> <p>VideoMAEv2 can be evaluated in the sameway as V-JEPA, by predicting the future and measuring surprise via prediction error</p> <p>I wonder if both models are trained with the same masking strategy</p> <p>we give the model a pair of videos, asking which one of the two isimpossible</p> <p>The V-JEPA score uses the maximum surprise from each video, which generalizesbetter for single-video classification</p> <p>V-JEPA excels at properties related to the scene\u2019s content (e.g., object permanence), but struggles with categories that require knowledge of a contextualizing event (gravity and solidity in InfLevel-lab) or themodeling of precise object interactions such as collisions</p> <p>model\u2019s framerate constraints.</p> <p>2 shows that although the accuracies are high for physicalviolations that imply properties intrinsic to objects (except for the color property), violations implicatinginteractions between objects, like solidity or collision, are close to chance</p> <p>While at training time the corruption used is the removal of spatio-temporal blocks, we can see that if weinstead use the first C frames to predict the rest of the video, this objective turns into a measure of error forthe prediction of the future.</p>","tags":["paper","world-models","video","ssl"]},{"location":"100%20Reference%20notes/101%20Literature/Is%20ImageNet%20worth%201%20video%20Learning%20strong%20image%20encoders%20from%201%20long%20unlabelled%20video/","title":"Is ImageNet worth 1 video Learning strong image encoders from 1 long unlabelled video","text":"Properties authors Shashanka Venkataramanan, Mamshad Nayeem Rizve, Jo\u00e3o Carreira, Yuki M. Asano, Yannis Avrithis year 2024 url http://arxiv.org/abs/2310.08584 <p>Abstract</p> <p>Self-supervised learning has unlocked the potential of scaling up pretraining to billions of images, since annotation is unnecessary. But are we making the best use of data? How more economical can we be? In this work, we attempt to answer this question by making two contributions. First, we investigate first-person videos and introduce a \u201cWalking Tours\u201d dataset. These videos are high-resolution, hourslong, captured in a single uninterrupted take, depicting a large number of objects and actions with natural scene transitions. They are unlabeled and uncurated, thus realistic for self-supervision and comparable with human learning.</p>","tags":["paper","time-ssl","ssl","temporal"]},{"location":"100%20Reference%20notes/101%20Literature/Is%20ImageNet%20worth%201%20video%20Learning%20strong%20image%20encoders%20from%201%20long%20unlabelled%20video/#notes","title":"Notes","text":"<p>Zotero Link</p> <p>DoRA (\u201cDiscOver and tRAck\u201d) is a self\u2011supervised pipeline that converts a pair of video frames into an object\u2011aware training view by first discovering prototypes in an anchor frame and then tracking them into a target frame before masking.</p> <p>The teacher ViT processes \\(X_s\\); for \\(k\\) random heads the [CLS]\u2192patch attention row gives \\(k\\) score maps which, multiplied by the patch\u2011query matrix, yield prototypes \\(P\\in\\mathbb R^{k\\times d}\\). A cost matrix \\(C=-PZ_s^\\top\\) against anchor patch embeddings enters three Sinkhorn\u2011Knopp normalisations; the entropic OT assignment \\(M^{*}\\) produces refined, disjoint prototypes \\(P'=M^{*}Z_s\\). The same prototypes are projected onto target keys \\(K_t\\) via one cross\u2011attention \\(A_t=\\text{softmax}(P'K_t^{\\top}/\\sqrt d)\\), the first operation that touches the second frame and therefore the moment tracking happens.  Rows of \\(A_t\\) act as heat\u2011maps that indicate where each anchor\u2011object now lies.  Random rows of \\(A_t\\) are binarised and inverted to remove those objects from \\(X_t\\), giving a masked image \\(\\tilde X_t\\); the teacher still sees full \\(X_t\\). A DINO\u2011style EMA loss then matches teacher and student [CLS] embeddings, enforcing temporal consistency yet object sensitivity.</p>","tags":["paper","time-ssl","ssl","temporal"]},{"location":"100%20Reference%20notes/101%20Literature/KV%20Cache%20Steering%20for%20Inducing%20Reasoning%20in%20Small%20Language%20Models/","title":"KV Cache Steering for Inducing Reasoning in Small Language Models","text":"Properties authors Max Belitsky, Dawid J. Kopiczko, Michael Dorkenwald, M. Jehanzeb Mirza, Cees G. M. Snoek, Yuki M. Asano year 2025 url http://arxiv.org/abs/2507.08799 <p>Abstract</p> <p>We propose cache steering, a lightweight method for implicit steering of language models via a one-shot intervention applied directly to the key-value cache. To validate its effectiveness, we apply cache steering to induce chain-of-thought reasoning in small language models. Our approach leverages GPT-4o-generated reasoning traces to construct steering vectors that shift model behavior toward more explicit, multi-step reasoning without fine-tuning or prompt modifications. Experimental evaluations on diverse reasoning benchmarks demonstrate that cache steering improves both the qualitative structure of model reasoning and quantitative task performance. Compared to prior activation steering techniques that require continuous interventions, our one-shot cache steering offers substantial advantages in terms of hyperparameter stability, inference-time efficiency, and ease of integration, making it a more robust and practical solution for controlled generation.</p>","tags":["paper","llm","efficient_dl","reasoning"]},{"location":"100%20Reference%20notes/101%20Literature/KV%20Cache%20Steering%20for%20Inducing%20Reasoning%20in%20Small%20Language%20Models/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","llm","efficient_dl","reasoning"]},{"location":"100%20Reference%20notes/101%20Literature/Knowledge%20Transfer%20from%20Vision%20Foundation%20Models%20for%20Efficient%20Training%20of%20Small%20Task-specific%20Models/","title":"Knowledge Transfer from Vision Foundation Models for Efficient Training of Small Task specific Models","text":"Properties authors Raviteja Vemulapalli, Hadi Pouransari, Fartash Faghri, Sachin Mehta, Mehrdad Farajtabar, Mohammad Rastegari, Oncel Tuzel year 2023 url https://arxiv.org/abs/2311.18237 <p>Abstract</p> <p>Vision Foundation Models (VFMs) pretrained on massive datasets exhibit impressive performance on various downstream tasks, especially with limited labeled target data. However, due to their high inference compute cost, these models cannot be deployed for many real-world applications. Motivated by this, we ask the following important question, \"How can we leverage the knowledge from a large VFM to train a small task-specific model for a new target task with limited labeled training data?\", and propose a simple task-oriented knowledge transfer approach as a highly effective solution to this problem. Our experimental results on five target tasks show that the proposed approach outperforms task-agnostic VFM distillation, web-scale CLIP pretraining, supervised ImageNet pretraining, and self-supervised DINO pretraining by up to 11.6%, 22.1%, 13.7%, and 29.8%, respectively. Furthermore, the proposed approach also demonstrates up to 9x, 4x and 15x reduction in pretraining compute cost when compared to task-agnostic VFM distillation, ImageNet pretraining and DINO pretraining, respectively, while outperforming them. We also show that the dataset used for transferring knowledge has a significant effect on the final target task performance, and introduce a retrieval-augmented knowledge transfer strategy that uses web-scale image retrieval to curate effective transfer sets.</p>","tags":["efficient_dl","paper","foundation_models"]},{"location":"100%20Reference%20notes/101%20Literature/LRP-QViT%20-%20Mixed-Precision%20Vision%20Transformer%20Quantization%20via%20Layer-wise%20Relevance%20Propagation/","title":"LRP QViT   Mixed Precision Vision Transformer Quantization via Layer wise Relevance Propagation","text":"Properties authors Navin Ranjan, Andreas Savakis year 2024 url https://arxiv.org/abs/2401.11243 <p>Abstract</p> <p>Vision transformers (ViTs) have demonstrated remarkable performance across various visual tasks. However, ViT models suffer from substantial computational and memory requirements, making it challenging to deploy them on resource-constrained platforms. Quantization is a popular approach for reducing model size, but most studies mainly focus on equal bit-width quantization for the entire network, resulting in sub-optimal solutions. While there are few works on mixed precision quantization (MPQ) for ViTs, they typically rely on search space-based methods or employ mixed precision arbitrarily. In this paper, we introduce LRP-QViT, an explainability-based method for assigning mixed-precision bit allocations to different layers based on their importance during classification. Specifically, to measure the contribution score of each layer in predicting the target class, we employ the Layer-wise Relevance Propagation (LRP) method. LRP assigns local relevance at the output layer and propagates it through all layers, distributing the relevance until it reaches the input layers. These relevance scores serve as indicators for computing the layer contribution score. Additionally, we have introduced a clipped channel-wise quantization aimed at eliminating outliers from post-LayerNorm activations to alleviate severe inter-channel variations. To validate and assess our approach, we employ LRP-QViT across ViT, DeiT, and Swin transformer models on various datasets. Our experimental findings demonstrate that both our fixed-bit and mixed-bit post-training quantization methods surpass existing models in the context of 4-bit and 6-bit quantization.</p>","tags":["paper","vit","computer_vision","peft","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/Layer%20by%20Layer%20-%20Uncovering%20Hidden%20Representations%20in%20Language%20Models/","title":"Layer by Layer   Uncovering Hidden Representations in Language Models","text":"Properties authors Oscar Skean, Md Rifat Arefin, Dan Zhao, Niket Patel, Jalal Naghiyev, Yann LeCun, Ravid Shwartz-Ziv year 2025 url http://arxiv.org/abs/2502.02013 <p>Abstract</p> <p>From extracting features to generating text, the outputs of large language models (LLMs) typically rely on the final layers, following the conventional wisdom that earlier layers capture only low-level cues. However, our analysis shows that intermediate layers can encode even richer representations, often improving performance on a range of downstream tasks. To explain and quantify these hidden-layer properties, we propose a unified framework of representation quality metrics based on information theory, geometry, and invariance to input perturbations. Our framework highlights how each layer balances information compression and signal preservation, revealing why mid-depth embeddings can exceed the last layer\u2019s performance. Through extensive experiments on 32 text-embedding tasks across various architectures (transformers, state-space models) and domains (language, vision), we demonstrate that intermediate layers consistently provide stronger features, challenging the standard view on final-layer embeddings and opening new directions on using mid-layer representations for more robust and accurate representations.</p>","tags":["paper","interpretability"]},{"location":"100%20Reference%20notes/101%20Literature/Layer%20by%20Layer%20-%20Uncovering%20Hidden%20Representations%20in%20Language%20Models/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","interpretability"]},{"location":"100%20Reference%20notes/101%20Literature/Learned%20Gridification%20for%20Efficient%20Point%20Cloud%20Processing/","title":"Learned Gridification for Efficient Point Cloud Processing","text":"Properties authors Putri A. van der Linden, Erik J. Bekkers, David W. Romero <p>Abstract</p>","tags":["dl2"]},{"location":"100%20Reference%20notes/101%20Literature/Learning%20Continually%20by%20Spectral%20Regularization/","title":"Learning Continually by Spectral Regularization","text":"Properties authors Alex Lewandowski, Micha\u0142 Bortkiewicz, Saurabh Kumar, Andr\u00e1s Gy\u00f6rgy, Dale Schuurmans, Mateusz Ostaszewski, Marlos C. Machado year 2024 url http://arxiv.org/abs/2406.06811 <p>Abstract</p> <p>Loss of plasticity is a phenomenon where neural networks can become more difficult to train over the course of learning. Continual learning algorithms seek to mitigate this effect by sustaining good performance while maintaining network trainability. We develop a new technique for improving continual learning inspired by the observation that the singular values of the neural network parameters at initialization are an important factor for trainability during early phases of learning. From this perspective, we derive a new spectral regularizer for continual learning that better sustains these beneficial initialization properties throughout training. In particular, the regularizer keeps the maximum singular value of each layer close to one. Spectral regularization directly ensures that gradient diversity is maintained throughout training, which promotes continual trainability, while minimally interfering with performance in a single task. We present an experimental analysis that shows how the proposed spectral regularizer can sustain trainability and performance across a range of model architectures in continual supervised and reinforcement learning settings. Spectral regularization is less sensitive to hyperparameters while demonstrating better training in individual tasks, sustaining trainability as new tasks arrive, and achieving better generalization performance.</p>","tags":["paper","continual_learning","spectral","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Learning%20Continually%20by%20Spectral%20Regularization/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","continual_learning","spectral","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Learning%20Partial%20Equivariances%20from%20Data/","title":"Learning Partial Equivariances from Data","text":"Properties authors David W. Romero, Suhas Lohit year 2021 url https://arxiv.org/abs/2110.10211 <p>Monte Carlo Approximation of Group Convolutions</p> <p>We can approximate Group Convolutions on the expectation by uniformly sampling group actions \\(v_j\\). $$  (\\psi \\hat{*} f)(u_i) = \\sum_j \\psi (v_j^{-1} u_i)f(v_j) \\bar{\\mu}_{\\mathcal{G}} (v_j) $$</p> <p>Main idea</p> <ol> <li>Prioritize sampling of specific group elements during the group convolution by learning a probability distribution over them.</li> <li>1D continuous groups: use reparametrization trick on the Lie algebra of the group, which is uniform over a connected set of group elements but zero otherwise. \\(\\to\\) Partial Equivariance</li> <li>1D discrete groups: Bernoulli Distribution over all possible element combinations</li> </ol> <p>Citations: - Self-Supervised Detection of Perfect and Partial Input-Dependent Symmetries - Color Equivariant Convolutional Networks - Equivariance-aware architectural optimization of neural networks - Approximation-Generalization Trade-offs under (Approximate) Group Equivariance</p>","tags":["dl2","equivariance","partial_equivariance"]},{"location":"100%20Reference%20notes/101%20Literature/Learning%20Representations%20on%20the%20Unit%20Sphere%20-%20Investigating%20Angular%20Gaussian%20and%20von%20Mises-Fisher%20Distributions%20for%20Online%20Continual%20Learning/","title":"Learning Representations on the Unit Sphere   Investigating Angular Gaussian and von Mises Fisher Distributions for Online Continual Learning","text":"Properties authors Nicolas Michel, Giovanni Chierchia, Romain Negrel, Jean-Fran\u00e7ois Bercher year 2024 url http://arxiv.org/abs/2306.03364 <p>Abstract</p> <p>We use the maximum a posteriori estimation principle for learning representations distributed on the unit sphere. We propose to use the angular Gaussian distribution, which corresponds to a Gaussian projected on the unit-sphere and derive the associated loss function. We also consider the von Mises-Fisher distribution, which is the conditional of a Gaussian in the unit-sphere. The learned representations are pushed toward fixed directions, which are the prior means of the Gaussians; allowing for a learning strategy that is resilient to data drift. This makes it suitable for online continual learning, which is the problem of training neural networks on a continuous data stream, where multiple classification tasks are presented sequentially so that data from past tasks are no longer accessible, and data from the current task can be seen only once. To address this challenging scenario, we propose a memory-based representation learning technique equipped with our new loss functions. Our approach does not require negative data or knowledge of task boundaries and performs well with smaller batch sizes while being computationally efficient. We demonstrate with extensive experiments that the proposed method outperforms the current state-of-the-art methods on both standard evaluation scenarios and realistic scenarios with blurry task boundaries. For reproducibility, we use the same training pipeline for every compared method and share the code at https://github.com/Nicolas1203/ocl-fd.</p>","tags":["paper","continual_learning","representation_learning","non_euclidean"]},{"location":"100%20Reference%20notes/101%20Literature/Learning%20Representations%20on%20the%20Unit%20Sphere%20-%20Investigating%20Angular%20Gaussian%20and%20von%20Mises-Fisher%20Distributions%20for%20Online%20Continual%20Learning/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","continual_learning","representation_learning","non_euclidean"]},{"location":"100%20Reference%20notes/101%20Literature/Learning%20Unseen%20Modality%20Interaction/","title":"Learning Unseen Modality Interaction","text":"Properties authors Yunhua Zhang, Hazel Doughty, Cees G. M. Snoek year 2023 url http://arxiv.org/abs/2306.12795 <p>Abstract</p> <p>Multimodal learning assumes all modality combinations of interest are available during training to learn cross-modal correspondences. In this paper, we challenge this modality-complete assumption for multimodal learning and instead strive for generalization to unseen modality combinations during inference. We pose the problem of unseen modality interaction and introduce a first solution. It exploits a module that projects the multidimensional features of different modalities into a common space with rich information preserved. This allows the information to be accumulated with a simple summation operation across available modalities. To reduce overfitting to less discriminative modality combinations during training, we further improve the model learning with pseudo-supervision indicating the reliability of a modality\u2019s prediction. We demonstrate that our approach is effective for diverse tasks and modalities by evaluating it for multimodal video classification, robot state regression, and multimedia retrieval. Project website: https://xiaobai1217.github.io/Unseen-Modality-Interaction/.</p>","tags":["paper","multimodal","llm"]},{"location":"100%20Reference%20notes/101%20Literature/Learning%20Unseen%20Modality%20Interaction/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","multimodal","llm"]},{"location":"100%20Reference%20notes/101%20Literature/Learning%20both%20Weights%20and%20Connections%20for%20Efficient%20Neural%20Networks/","title":"Learning both Weights and Connections for Efficient Neural Networks","text":"Properties authors Song Han, Jeff Pool, John Tran, William J. Dally year 2015 url https://arxiv.org/abs/1506.02626 <p>Abstract</p> <p>Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.</p>","tags":["paper","efficient_dl","pruning","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Learning%20to%20Learn%20without%20Forgetting%20using%20Attention/","title":"Learning to Learn without Forgetting using Attention","text":"Properties authors Anna Vettoruzzo, Joaquin Vanschoren, Mohamed-Rafik Bouguelia, Thorsteinn R\u00f6gnvaldsson year 2024 url http://arxiv.org/abs/2408.03219 <p>Abstract</p> <p>Continual learning (CL) refers to the ability to continually learn over time by accommodating new knowledge while retaining previously learned experience. While this concept is inherent in human learning, current machine learning methods are highly prone to overwrite previously learned patterns and thus forget past experience. Instead, model parameters should be updated selectively and carefully, avoiding unnecessary forgetting while optimally leveraging previously learned patterns to accelerate future learning. Since hand-crafting effective update mechanisms is difficult, we propose meta-learning a transformer-based optimizer to enhance CL. This meta-learned optimizer uses attention to learn the complex relationships between model parameters across a stream of tasks, and is designed to generate effective weight updates for the current task while preventing catastrophic forgetting on previously encountered tasks. Evaluations on benchmark datasets like SplitMNIST, RotatedMNIST, and SplitCIFAR-100 affirm the efficacy of the proposed approach in terms of both forward and backward transfer, even on small sets of labeled data, highlighting the advantages of integrating a meta-learned optimizer within the continual learning framework.</p>","tags":["paper","continual_learning"]},{"location":"100%20Reference%20notes/101%20Literature/Learning%20to%20Learn%20without%20Forgetting%20using%20Attention/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","continual_learning"]},{"location":"100%20Reference%20notes/101%20Literature/Learning%20with%20Unmasked%20Tokens%20Drives%20Stronger%20Vision%20Learners/","title":"Learning with Unmasked Tokens Drives Stronger Vision Learners","text":"Properties authors Taekyung Kim, Sanghyuk Chun, Byeongho Heo, Dongyoon Han year 2024 url https://arxiv.org/abs/2310.13593 <p>Abstract</p> <p>Masked image modeling (MIM) has become a leading self-supervised learning strategy. MIMs such as Masked Autoencoder (MAE) learn strong representations by randomly masking input tokens for the encoder to process, with the decoder reconstructing the masked tokens to the input. However, MIM pre-trained encoders often exhibit a limited attention span, attributed to MIM's sole focus on regressing masked tokens only, which may impede the encoder's broader context learning. To tackle the limitation, we improve MIM by explicitly incorporating unmasked tokens into the training process. Specifically, our method enables the encoder to learn from broader context supervision, allowing unmasked tokens to experience broader contexts while the decoder reconstructs masked tokens. Thus, the encoded unmasked tokens are equipped with extensive contextual information, empowering masked tokens to leverage the enhanced unmasked tokens for MIM. As a result, our simple remedy trains more discriminative representations revealed by achieving 84.2% top-1 accuracy with ViT-B on ImageNet-1K with 0.6%p gain. We attribute the success to the enhanced pre-training method, as evidenced by the singular value spectrum and attention analyses. Finally, our models achieve significant performance gains at the downstream semantic segmentation and fine-grained visual classification tasks; and on diverse robust evaluation metrics. Code is available at\u00a0this https URL</p>","tags":["paper","foundation_models","computer_vision","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/Learning%20with%20Unmasked%20Tokens%20Drives%20Stronger%20Vision%20Learners/#notes","title":"Notes","text":"<p>Some notes regarding MIM as a good objective are on Masked Image Modelling.</p> <p>However, MIM strategies often encounter challenges, such as local dependency on attention to understand entire context of an image. For example, liu\u00a0et al.\u00a0[36]\u00a0revealed that MAE\u00a0[22], a state-of-the-art MIM method, exhibits shorter average attention distances. Furthermore, we observe that attention map patterns by MAE substantiate extremely local behavior (See Fig.\u00a01) indeed. In other words, the MAE-trained attention mechanism less integrates information across the entire image pixels and tends to focus on specific input regions. This is presumably attributed to MIM-pretraining, primarily dedicated to predicting low-level pixel details (e.g., color or texture) without a comprehensive understanding of less-regional information (e.g., the input structure or shape).</p> <p>This maybe should not really be an issue: How do vision transformers work? explicitly constraint ViTs to only use local attention and they improve performance. So maybe this is an advantage? See Are less inductive biases better or worse?.</p> <p></p>","tags":["paper","foundation_models","computer_vision","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/LieRE%20-%20Generalizing%20Rotary%20Position%20Encodings/","title":"LieRE   Generalizing Rotary Position Encodings","text":"Properties authors Sophie Ostmeier, Brian Axelrod, Michael E. Moseley, Akshay Chaudhari, Curtis Langlotz year 2024 url https://arxiv.org/abs/2406.10322 <p>Abstract</p> <p>While Rotary Position Embeddings (RoPE) for large language models have become widely adopted, their application for other modalities has been slower. Here, we introduce Lie group Relative position Encodings (LieRE) that goes beyond RoPE in supporting n-dimensional inputs. We evaluate the performance of LieRE on 2D and 3D image classification tasks and observe that LieRE leads to marked relative improvements in performance (up to 9.7% for 2D and up to 25.5% for 3D), training efficiency (3.5x reduction), data efficiency (30%) compared to the baselines of DeiT III, RoPE-Mixed and Vision-Llama.\u00a0this https URL</p>","tags":["paper","dl_theory","vit"]},{"location":"100%20Reference%20notes/101%20Literature/Llama%202%20-%20Open%20Foundation%20and%20Fine-Tuned%20Chat%20Models/","title":"Llama 2   Open Foundation and Fine Tuned Chat Models","text":"Properties authors Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, Thomas Scialom year 2023 url https://arxiv.org/abs/2307.09288 <p>Abstract</p> <p>In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.</p>","tags":["paper","foundation_models","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/LoRA%20-%20Low-Rank%20Adaptation%20of%20Large%20Language%20Models/","title":"LoRA   Low Rank Adaptation of Large Language Models","text":"Properties authors Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen year 2021 url https://arxiv.org/abs/2106.09685 <p>Abstract</p> <p>An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at\u00a0this https URL.</p>","tags":["paper","efficient_dl","peft"]},{"location":"100%20Reference%20notes/101%20Literature/LoRA%20vs%20Full%20Fine-tuning%20-%20An%20Illusion%20of%20Equivalence/","title":"LoRA vs Full Fine tuning   An Illusion of Equivalence","text":"Properties authors Reece Shuttleworth, Jacob Andreas, Antonio Torralba, Pratyusha Sharma year 2024 url https://arxiv.org/abs/2410.21228 <p>Abstract</p> <p>Fine-tuning is a crucial paradigm for adapting pre-trained large language models to downstream tasks. Recently, methods like Low-Rank Adaptation (LoRA) have been shown to match the performance of fully fine-tuned models on various tasks with an extreme reduction in the number of trainable parameters. Even in settings where both methods learn similarly accurate models, \\emph{are their learned solutions really equivalent?} We study how different fine-tuning methods change pre-trained models by analyzing the model's weight matrices through the lens of their spectral properties. We find that full fine-tuning and LoRA yield weight matrices whose singular value decompositions exhibit very different structure; moreover, the fine-tuned models themselves show distinct generalization behaviors when tested outside the adaptation task's distribution. More specifically, we first show that the weight matrices trained with LoRA have new, high-ranking singular vectors, which we call \\emph{intruder dimensions}. Intruder dimensions do not appear during full fine-tuning. Second, we show that LoRA models with intruder dimensions, despite achieving similar performance to full fine-tuning on the target task, become worse models of the pre-training distribution and adapt less robustly to multiple tasks sequentially. Higher-rank, rank-stabilized LoRA models closely mirror full fine-tuning, even when performing on par with lower-rank LoRA models on the same tasks. These results suggest that models updated with LoRA and full fine-tuning access different parts of parameter space, even when they perform equally on the fine-tuned distribution. We conclude by examining why intruder dimensions appear in LoRA fine-tuned models, why they are undesirable, and how their effects can be minimized.</p>","tags":["paper","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Location-Aware%20Self-Supervised%20Transformers%20for%20Semantic%20Segmentation/","title":"Location Aware Self Supervised Transformers for Semantic Segmentation","text":"Properties authors Mathilde Caron, Neil Houlsby, Cordelia Schmid year 2022 url https://arxiv.org/abs/2212.02400 <p>Abstract</p> <p>Pixel-level labels are particularly expensive to acquire. Hence, pretraining is a critical step to improve models on a task like semantic segmentation. However, prominent algorithms for pretraining neural networks use image-level objectives, e.g. image classification, image-text alignment a la CLIP, or self-supervised contrastive learning. These objectives do not model spatial information, which might be sub-optimal when finetuning on downstream tasks with spatial reasoning. In this work, we pretrain network with a location-aware (LOCA) self-supervised method which fosters the emergence of strong dense features. Specifically, we use both a patch-level clustering scheme to mine dense pseudo-labels and a relative location prediction task to encourage learning about object parts and their spatial arrangements. Our experiments show that LOCA pretraining leads to representations that transfer competitively to challenging and diverse semantic segmentation datasets.</p>","tags":["paper","vit","thesis"]},{"location":"100%20Reference%20notes/101%20Literature/Lost%20in%20Time%20-%20A%20New%20Temporal%20Benchmark%20for%20VideoLLMs/","title":"Lost in Time   A New Temporal Benchmark for VideoLLMs","text":"Properties authors Daniel Cores, Michael Dorkenwald, Manuel Mucientes, Cees G. M. Snoek, Yuki M. Asano year 2025 url http://arxiv.org/abs/2410.07752 <p>Abstract</p> <p>Large language models have demonstrated impressive performance when integrated with vision models even enabling video understanding. However, evaluating video models presents its own unique challenges, for which several benchmarks have been proposed. In this paper, we show that the currently most used video-language benchmarks can be solved without requiring much temporal reasoning. We identified three main issues in existing datasets: (i) static information from single frames is often sufficient to solve the tasks (ii) the text of the questions and candidate answers is overly informative, allowing models to answer correctly without relying on any visual input (iii) world knowledge alone can answer many of the questions, making the benchmarks a test of knowledge replication rather than video reasoning. In addition, we found that open-ended questionanswering benchmarks for video understanding suffer from similar issues while the automatic evaluation process with LLMs is unreliable, making it an unsuitable alternative. As a solution, we propose TVBench, a novel open-source video multiple-choice question-answering benchmark, and demonstrate through extensive evaluations that it requires a high level of temporal understanding. Surprisingly, we find that most recent state-of-the-art video-language models perform similarly to random performance on TVBench, with only a few models such as Qwen2-VL, and Tarsier clearly surpassing this baseline.</p>","tags":["paper","llm","video","temporal"]},{"location":"100%20Reference%20notes/101%20Literature/Lost%20in%20Time%20-%20A%20New%20Temporal%20Benchmark%20for%20VideoLLMs/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","llm","video","temporal"]},{"location":"100%20Reference%20notes/101%20Literature/Low-Resource%20Vision%20Challenges%20for%20Foundation%20Models/","title":"Low Resource Vision Challenges for Foundation Models","text":"Properties authors Yunhua Zhang, Hazel Doughty, Cees G. M. Snoek year 2024 url http://arxiv.org/abs/2401.04716 <p>Abstract</p> <p>Low-resource settings are well-established in natural language processing, where many languages lack sufficient data for deep learning at scale. However, low-resource problems are under-explored in computer vision. In this paper, we address this gap and explore the challenges of low-resource image tasks with vision foundation models. We first collect a benchmark of genuinely low-resource image data, covering historic maps, circuit diagrams, and mechanical drawings. These low-resource settings all share three challenges: data scarcity, fine-grained differences, and the distribution shift from natural images to the specialized domain of interest. While existing foundation models have shown impressive generalizability, we find they cannot transfer well to our low-resource tasks. To begin to tackle the challenges of low-resource vision, we introduce one simple baseline per challenge. Specifically, we i) enlarge the data space by generative models, ii) adopt the best sub-kernels to encode local regions for fine-grained difference discovery and iii) learn attention for specialized domains. Experiments on our three low-resource tasks demonstrate our proposals already provide a better baseline than transfer learning, data augmentation, and fine-grained methods. This highlights the unique characteristics and challenges of low-resource vision for foundation models that warrant further investigation. Project page: https://xiaobai1217.github.io/ Low-Resource-Vision/.</p>","tags":["paper","efficient_dl","foundation_models","computer_vision"]},{"location":"100%20Reference%20notes/101%20Literature/Low-Resource%20Vision%20Challenges%20for%20Foundation%20Models/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","efficient_dl","foundation_models","computer_vision"]},{"location":"100%20Reference%20notes/101%20Literature/Mamba%20-%20Linear-Time%20Sequence%20Modeling%20with%20Selective%20State%20Spaces/","title":"Mamba   Linear Time Sequence Modeling with Selective State Spaces","text":"Properties authors Albert Gu, Tri Dao year 2023 <p>Abstract</p> <p>Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\u00d7\u00a0higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.</p>","tags":["foundation_models","convolutions"]},{"location":"100%20Reference%20notes/101%20Literature/Masked%20Autoencoders%20Are%20Scalable%20Vision%20Learners/","title":"Masked Autoencoders Are Scalable Vision Learners","text":"Properties authors Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, Ross Girshick year 2021 url https://arxiv.org/abs/2111.06377 <p>Abstract</p> <p>This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.</p>","tags":["paper","computer_vision","vit"]},{"location":"100%20Reference%20notes/101%20Literature/Masked%20Autoencoders%20Are%20Scalable%20Vision%20Learners/#notes","title":"Notes","text":"<p>{ width=\"1000\" }</p> <ul> <li>[[The role of decoder depth in MAE-like models|The role of decoder depth in MAE-like models]]: Decoder depth is important for linear probing accuracy but not so much for finetuned accuracy:<ul> <li>\"\"\"A sufficiently deep decoder is important for linear probing. This can be explained by the gap between a pixel reconstruction task and a recognition task: the last several layers in an autoencoder are more specialized for reconstruction, but are less relevant for recognition. A reasonably deep decoder can account for the reconstruction specialization, leaving the latent representations at a more abstract level\"\"\"</li> </ul> </li> <li>The decoder can be narrower than the encoder :D</li> </ul>","tags":["paper","computer_vision","vit"]},{"location":"100%20Reference%20notes/101%20Literature/MatFormer%20-%20Nested%20Transformer%20for%20Elastic%20Inference/","title":"MatFormer   Nested Transformer for Elastic Inference","text":"Properties authors Sneha Kudugunta, Aditya Kusupati, Tim Dettmers, Kaifeng Chen, Inderjit Dhillon, Yulia Tsvetkov, Hannaneh Hajishirzi, Sham Kakade, Ali Farhadi, Prateek Jain year 2023 <p>Abstract</p> <p>Transformer models are deployed in a wide range of settings, from multiaccelerator clusters to standalone mobile phones. The diverse inference constraints in these scenarios necessitate practitioners to train foundation models such as PaLM 2, Llama, &amp; ViTs as a series of models of varying sizes. Due to significant training costs, only a select few model sizes are trained and supported, limiting more fine-grained control over relevant tradeoffs, including latency, cost, and accuracy. This work introduces MatFormer2, a nested Transformer architecture designed to offer elasticity in a variety of deployment constraints. Each Feed Forward Network (FFN) block of a MatFormer model is jointly optimized with a few nested smaller FFN blocks. This training procedure allows for the Mix\u2019n\u2019Match of model granularities across layers \u2013 i.e., a trained universal MatFormer model enables extraction of hundreds of accurate smaller models, which were never explicitly optimized. We empirically demonstrate MatFormer\u2019s effectiveness across different model classes (decoders &amp; encoders), modalities (language &amp; vision), and scales (up to 2.6B parameters). We find that a 2.6B decoder-only MatFormer language model (MatLM) allows us to extract smaller models spanning from 1.5B to 2.6B, each exhibiting comparable validation loss and one-shot downstream evaluations to their independently trained counterparts. Furthermore, we observe that smaller encoders extracted from a universal MatFormer-based ViT (MatViT) encoder preserve the metric-space structure for adaptive large-scale retrieval. Finally, we showcase that speculative decoding with the accurate and consistent submodels extracted from MatFormer can further reduce inference latency.</p>","tags":["paper","vit","flexible_vit","efficient_vision"]},{"location":"100%20Reference%20notes/101%20Literature/MatFormer%20-%20Nested%20Transformer%20for%20Elastic%20Inference/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","vit","flexible_vit","efficient_vision"]},{"location":"100%20Reference%20notes/101%20Literature/Matryoshka%20Representation%20Learning/","title":"Matryoshka Representation Learning","text":"Properties authors Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek Jain, Ali Farhadi year 2024 url http://arxiv.org/abs/2205.13147 <p>Abstract</p> <p>Learned representations are a central component in modern ML systems, serving a multitude of downstream tasks. When training such representations, it is often the case that computational and statistical constraints for each downstream task are unknown. In this context, rigid fixed-capacity representations can be either over or under-accommodating to the task at hand. This leads us to ask: can we design a flexible representation that can adapt to multiple downstream tasks with varying computational resources? Our main contribution is Matryoshka Representation Learning (MRL) which encodes information at different granularities and allows a single embedding to adapt to the computational constraints of downstream tasks. MRL minimally modifies existing representation learning pipelines and imposes no additional cost during inference and deployment. MRL learns coarse-to-fine representations that are at least as accurate and rich as independently trained low-dimensional representations. The flexibility within the learned Matryoshka Representations offer: (a) up to 14\u00d7 smaller embedding size for ImageNet-1K classification at the same level of accuracy; (b) up to 14\u00d7 real-world speed-ups for large-scale retrieval on ImageNet-1K and 4K; and (c) up to 2% accuracy improvements for long-tail few-shot classification, all while being as robust as the original representations. Finally, we show that MRL extends seamlessly to web-scale datasets (ImageNet, JFT) across various modalities \u2013 vision (ViT, ResNet), vision + language (ALIGN) and language (BERT). MRL code and pretrained models are open-sourced at https://github.com/RAIVNLab/MRL.</p>","tags":["paper","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Matryoshka%20Representation%20Learning/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Memorization%20Through%20the%20Lens%20of%20Curvature%20of%20Loss%20Function%20Around%20Samples/","title":"Memorization Through the Lens of Curvature of Loss Function Around Samples","text":"Properties authors Isha Garg, Deepak Ravikumar, Kaushik Roy year 2024 url https://openreview.net/forum?id=WQbDS9RydY <p>Abstract</p> <p>Deep neural networks are over-parameterized and easily overfit to and memorize the datasets that they train on. In the extreme case, it has been shown that networks can memorize a randomly labeled dataset. In this paper, we propose using the curvature of the loss function around each training sample, averaged over training epochs, as a measure of memorization of a sample. We show that this curvature metric effectively captures memorization statistics, both qualitatively and quantitatively in popular image datasets. We provide quantitative validation of the proposed metric against memorization scores released by Feldman &amp; Zhang (2020). Further, experiments on mislabeled data detection show that corrupted samples are learned with high curvature and using curvature for identifying mislabelled examples outperforms existing approaches. Qualitatively, we find that high curvature samples correspond to long-tailed, mislabeled, or conflicting instances, indicating a likelihood of memorization. Notably, this analysis helps us find, to the best of our knowledge, a novel failure mode on the CIFAR100 and ImageNet datasets: that of duplicated images with differing labels.</p>","tags":["paper","dl_theory","llm"]},{"location":"100%20Reference%20notes/101%20Literature/Mixture%20of%20LoRa%20Experts/","title":"Mixture of LoRa Experts","text":"Properties authors Xun Wu, Shaohan Huang, Furu Wei year 2024 url https://arxiv.org/abs/2404.13628 <p>Abstract</p> <p>LoRA has gained widespread acceptance in the fine-tuning of large pre-trained models to cater to a diverse array of downstream tasks, showcasing notable effectiveness and efficiency, thereby solidifying its position as one of the most prevalent fine-tuning techniques. Due to the modular nature of LoRA's plug-and-play plugins, researchers have delved into the amalgamation of multiple LoRAs to empower models to excel across various downstream tasks. Nonetheless, extant approaches for LoRA fusion grapple with inherent challenges. Direct arithmetic merging may result in the loss of the original pre-trained model's generative capabilities or the distinct identity of LoRAs, thereby yielding suboptimal outcomes. On the other hand, Reference tuning-based fusion exhibits limitations concerning the requisite flexibility for the effective combination of multiple LoRAs. In response to these challenges, this paper introduces the Mixture of LoRA Experts (MoLE) approach, which harnesses hierarchical control and unfettered branch selection. The MoLE approach not only achieves superior LoRA fusion performance in comparison to direct arithmetic merging but also retains the crucial flexibility for combining LoRAs effectively. Extensive experimental evaluations conducted in both the Natural Language Processing (NLP) and Vision &amp; Language (V&amp;L) domains substantiate the efficacy of MoLE.</p>","tags":["paper","peft","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/MoSiC%20-%20Optimal-Transport%20Motion%20Trajectory%20for%20Dense%20Self-Supervised%20Learning/","title":"MoSiC   Optimal Transport Motion Trajectory for Dense Self Supervised Learning","text":"Properties authors Mohammadreza Salehi, Shashanka Venkataramanan, Ioana Simion, Efstratios Gavves, Cees G. M. Snoek, Yuki M. Asano year 2025 url http://arxiv.org/abs/2506.08694 <p>Abstract</p> <p>Dense self-supervised learning has shown great promise for learning pixel- and patch-level representations, but extending it to videos remains challenging due to the complexity of motion dynamics. Existing approaches struggle as they rely on static augmentations that fail under object deformations, occlusions, and camera movement, leading to inconsistent feature learning over time. We propose a motion-guided self-supervised learning framework that clusters dense point tracks to learn spatiotemporally consistent representations. By leveraging an off-the-shelf point tracker, we extract long-range motion trajectories and optimize feature clustering through a momentum-encoder-based optimal transport mechanism. To ensure temporal coherence, we propagate cluster assignments along tracked points, enforcing feature consistency across views despite viewpoint changes. Integrating motion as an implicit supervisory signal, our method learns representations that generalize across frames, improving robustness in dynamic scenes and challenging occlusion scenarios. By initializing from strong image-pretrained models and leveraging video data for training, we improve state-of-the-art by 1% to 6% on six image and video datasets and four evaluation benchmarks. The implementation is publicly available at our GitHub repository: https://github.com/SMSD75/MoSiC/tree/main</p>","tags":["paper","ssl","dense_ssl","computer_vision"]},{"location":"100%20Reference%20notes/101%20Literature/MoSiC%20-%20Optimal-Transport%20Motion%20Trajectory%20for%20Dense%20Self-Supervised%20Learning/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","ssl","dense_ssl","computer_vision"]},{"location":"100%20Reference%20notes/101%20Literature/MobileCLIP%20-%20Fast%20Image-Text%20Models%20through%20Multi-Modal%20Reinforced%20Training/","title":"MobileCLIP   Fast Image Text Models through Multi Modal Reinforced Training","text":"Properties authors Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, Oncel Tuzel year 2023 url https://arxiv.org/abs/2311.17049 <p>Abstract</p> <p>Contrastive pretraining of image-text foundation models, such as CLIP, demonstrated excellent zero-shot performance and improved robustness on a wide range of downstream tasks. However, these models utilize large transformer-based encoders with significant memory and latency overhead which pose challenges for deployment on mobile devices. In this work, we introduce MobileCLIP -- a new family of efficient image-text models optimized for runtime performance along with a novel and efficient training approach, namely multi-modal reinforced training. The proposed training approach leverages knowledge transfer from an image captioning model and an ensemble of strong CLIP encoders to improve the accuracy of efficient models. Our approach avoids train-time compute overhead by storing the additional knowledge in a reinforced dataset. MobileCLIP sets a new state-of-the-art latency-accuracy tradeoff for zero-shot classification and retrieval tasks on several datasets. Our MobileCLIP-S2 variant is 2.3\u00d7\u00a0faster while more accurate compared to previous best CLIP model based on ViT-B/16. We further demonstrate the effectiveness of our multi-modal reinforced training by training a CLIP model based on ViT-B/16 image backbone and achieving +2.9% average performance improvement on 38 evaluation benchmarks compared to the previous best. Moreover, we show that the proposed approach achieves 10\u00d7-1000\u00d7\u00a0improved learning efficiency when compared with non-reinforced CLIP training. Code and models are available at\u00a0this https URL\u00a0.</p>","tags":["paper","efficient_dl","efficient_vision","computer_vision","multimodal"]},{"location":"100%20Reference%20notes/101%20Literature/MobileViT%20-%20light-weight%2C%20general-purpose%2C%20and%20mobile-friendly%20vision%20transformer/","title":"MobileViT   light weight, general purpose, and mobile friendly vision transformer","text":"Properties authors Sachin Mehta, Mohammad Rastegari year 2022 url https://arxiv.org/abs/2110.02178 <p>Abstract</p> <p>Light-weight convolutional neural networks (CNNs) are the de-facto for mobile vision tasks. Their spatial inductive biases allow them to learn representations with fewer parameters across different vision tasks. However, these networks are spatially local. To learn global representations, self-attention-based vision trans-formers (ViTs) have been adopted. Unlike CNNs, ViTs are heavy-weight. In this paper, we ask the following question: is it possible to combine the strengths of CNNs and ViTs to build a light-weight and low latency network for mobile vision tasks? Towards this end, we introduce MobileViT, a light-weight and general-purpose vision transformer for mobile devices. MobileViT presents a different perspective for the global processing of information with transformers, i.e., transformers as convolutions. Our results show that MobileViT significantly outperforms CNN- and ViT-based networks across different tasks and datasets. On the ImageNet-1k dataset, MobileViT achieves top-1 accuracy of 78.4% with about 6 million parameters, which is 3.2% and 6.2% more accurate than MobileNetv3 (CNN-based) and DeIT (ViT-based) for a similar number of parameters. On the MS-COCO object detection task, MobileViT is 5.7% more accurate than MobileNetv3 for a similar number of parameters.  </p> <p>Our source code is open-source and available at:\u00a0this https URL</p>","tags":["paper","efficient_dl","efficient_vision"]},{"location":"100%20Reference%20notes/101%20Literature/Model%20Compression%20in%20Practice%20-%20Lessons%20Learned%20from%20Practitioners%20Creating%20On-device%20Machine%20Learning%20Experiences/","title":"Model Compression in Practice   Lessons Learned from Practitioners Creating On device Machine Learning Experiences","text":"Properties authors Fred Hohman, Mary Beth Kery, Donghao Ren, Dominik Moritz year 2024 url https://arxiv.org/abs/2310.04621 <p>Abstract</p> <p>On-device machine learning (ML) promises to improve the privacy, responsiveness, and proliferation of new, intelligent user experiences by moving ML computation onto everyday personal devices. However, today's large ML models must be drastically compressed to run efficiently on-device, a hurtle that requires deep, yet currently niche expertise. To engage the broader human-centered ML community in on-device ML experiences, we present the results from an interview study with 30 experts at Apple that specialize in producing efficient models. We compile tacit knowledge that experts have developed through practical experience with model compression across different hardware platforms. Our findings offer pragmatic considerations missing from prior work, covering the design process, trade-offs, and technical strategies that go into creating efficient models. Finally, we distill design recommendations for tooling to help ease the difficulty of this work and bring on-device ML into to more widespread practice.</p>","tags":["paper","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/Model%20Compression%20in%20Practice%20-%20Lessons%20Learned%20from%20Practitioners%20Creating%20On-device%20Machine%20Learning%20Experiences/#notes","title":"Notes","text":"<p>Specific techniques on models weights help reduce size, but to get an efficient model comes from more careful design of the loss function, the system, which parts should and should not be modeled with ML. - [ ] How does the design of a loss function affect a model's efficiency? Note to myself to look into this in the future.</p> <p></p> <p>Although posttraining quantization is considered \u201ceasy\u201d [E9] as far as ML compression techniques go, practitioners emphasized that it still often takes complex code to implement and there are many algorithm variations A survey of quantization methods for efficient neural network inference to experiment with [T5]. For models that need high accuracy, post-training quantization may not be enough to hit budget without unacceptable accuracy degradation [E9, E4, E13, E5].</p> <ul> <li>Okay, so it's important to try a bunch of quantization techniques. Got it.</li> </ul> <p>\u201cIf you want to go to lower bit quantization, such as 4 or below, it\u2019s almost impossible to use post-training quantization because the difference in accuracy gets way too big. So for this level of compression you need to do training-aware compression.\u201d \u2014 E9</p> <ul> <li>Cool, I didn't know that training aware compression was such an important thing to consider, from an industry perspective, not just research.</li> </ul> <p>Although training-aware compression is considered the best form of optimization A survey of quantization methods for efficient neural network inference, a major drawback is that is must be included in initial model training: \u201cNot starting early with compression is a dead end,\u201d [E3].</p> <ul> <li> Why is that though? Why should compression-aware training happen from the start and not in the middle of training or even in finetuning? #rq</li> </ul> <p>[...] practitioners suggest estimating how much compression will be feasible with simple post-training quantization. To estimate quantization savings before training a model, first initialize the ML model architecture with random weights, then quantize, and test the model\u2019s speed and size on-device</p> <p>Strategy #6: Compression can degrade the accuracy of a model and change its behavior in unpredictable ways. It is essential to create a robust evaluation pipeline (e.g., defining metrics, curating test sets) before you start optimizing your model, so that you can reliably observe shifts in model error afterwards. To prevent degradation from a failed optimization, compare optimized models with varying amounts of compression to your original model, inspecting the metrics, subpopulation behaviors, and internals, such as weights and activations, to ensure they are within expected ranges.</p> <p>Okay, Robust evaluation pipeline is fundamental: Need to create unit tests, and for quantization specifically check that the distributions of weights (obviously) and activations (less obviously) are within the expected ranges. The latter might happen because of compounding degradation, this mean that errors in early layers caused by quantization might compound to later layers in unexpected ways.</p>","tags":["paper","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/Near%2C%20far%20-%20Patch-ordering%20enhances%20vision%20foundation%20models%27%20scene%20understanding/","title":"Near, far   Patch ordering enhances vision foundation models' scene understanding","text":"Properties authors Valentinos Pariza, Mohammadreza Salehi, Gertjan J. Burghouts, Francesco Locatello, Yuki M. Asano year 2025 url http://arxiv.org/abs/2408.11054 <p>Abstract</p> <p>We introduce NeCo: Patch Neighbor Consistency, a novel self-supervised training loss that enforces patch-level nearest neighbor consistency across a student and teacher model. Compared to contrastive approaches that only yield binary learning signals, i.e. \u2018attract\u2019 and \u2018repel\u2019, this approach benefits from the more fine-grained learning signal of sorting spatially dense features relative to reference patches. Our method leverages differentiable sorting applied on top of pretrained representations, such as DINOv2-registers to bootstrap the learning signal and further improve upon them. This dense post-pretraining leads to superior performance across various models and datasets, despite requiring only 19 hours on a single GPU. This method generates high-quality dense feature encoders and establishes several new state-of-the-art results such as +5.5 % and +6% for non-parametric in-context semantic segmentation on ADE20k and Pascal VOC, +7.2% and +5.7% for linear segmentation evaluations on COCO-Things and -Stuff and improvements in the 3D understanding of multi-view consistency on SPair-71k, by more than 1.5%.</p>","tags":["paper","post_training","vit","ssl","dense","patch_level","computer_vision"]},{"location":"100%20Reference%20notes/101%20Literature/Near%2C%20far%20-%20Patch-ordering%20enhances%20vision%20foundation%20models%27%20scene%20understanding/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","post_training","vit","ssl","dense","patch_level","computer_vision"]},{"location":"100%20Reference%20notes/101%20Literature/NeoBabel%20-%20A%20Multilingual%20Open%20Tower%20for%20Visual%20Generation/","title":"NeoBabel   A Multilingual Open Tower for Visual Generation","text":"Properties authors Mohammad Mahdi Derakhshani, Dheeraj Varghese, Marzieh Fadaee, Cees G. M. Snoek year 2025 url http://arxiv.org/abs/2507.06137 <p>Abstract</p> <p>Text-to-image generation advancements have been predominantly English-centric, creating barriers for non-English speakers and perpetuating digital inequities. While existing systems rely on translation pipelines, these introduce semantic drift, computational overhead, and cultural misalignment. We introduce NeoBabel, a novel multilingual image generation framework that sets a new Pareto frontier in performance, efficiency and inclusivity, supporting six languages: English, Chinese, Dutch, French, Hindi, and Persian. The model is trained using a combination of large-scale multilingual pretraining and high-resolution instruction tuning. To evaluate its capabilities, we expand two English-only benchmarks to multilingual equivalents: m-GenEval and m-DPG. NeoBabel achieves state-of-the-art multilingual performance while retaining strong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG. Notably, it performs on par with leading models on English tasks while outperforming them by +0.11 and +0.09 on multilingual benchmarks, even though these models are built on multilingual base LLMs. This demonstrates the effectiveness of our targeted alignment training for preserving and extending crosslingual generalization. We further introduce two new metrics to rigorously assess multilingual alignment and robustness to code-mixed prompts. Notably, NeoBabel matches or exceeds English-only models while being 2\u20134\u00d7 smaller. We release an open toolkit, including all code, model checkpoints, a curated dataset of 124M multilingual text-image pairs, and standardized multilingual evaluation protocols, to advance inclusive AI research. Our work demonstrates that multilingual capability is not a trade-off but a catalyst for improved robustness, efficiency, and cultural fidelity in generative AI.</p>","tags":["paper","multimodal","llm"]},{"location":"100%20Reference%20notes/101%20Literature/NeoBabel%20-%20A%20Multilingual%20Open%20Tower%20for%20Visual%20Generation/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","multimodal","llm"]},{"location":"100%20Reference%20notes/101%20Literature/Neural%20Mechanics%20-%20Symmetry%20and%20Broken%20Conservation%20Laws%20in%20Deep%20Learning%20Dynamics/","title":"Neural Mechanics   Symmetry and Broken Conservation Laws in Deep Learning Dynamics","text":"Properties authors Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel L.K. Yamins, Hidenori Tanaka year 2020 url https://arxiv.org/abs/2012.04728 <p>Abstract</p> <p>Understanding the dynamics of neural network parameters during training is one of the key challenges in building a theoretical foundation for deep learning. A central obstacle is that the motion of a network in high-dimensional parameter space undergoes discrete finite steps along complex stochastic gradients derived from real-world datasets. We circumvent this obstacle through a unifying theoretical framework based on intrinsic symmetries embedded in a network's architecture that are present for any dataset. We show that any such symmetry imposes stringent geometric constraints on gradients and Hessians, leading to an associated conservation law in the continuous-time limit of stochastic gradient descent (SGD), akin to Noether's theorem in physics. We further show that finite learning rates used in practice can actually break these symmetry induced conservation laws. We apply tools from finite difference methods to derive modified gradient flow, a differential equation that better approximates the numerical trajectory taken by SGD at finite learning rates. We combine modified gradient flow with our framework of symmetries to derive exact integral expressions for the dynamics of certain parameter combinations. We empirically validate our analytic expressions for learning dynamics on VGG-16 trained on Tiny ImageNet. Overall, by exploiting symmetry, our work demonstrates that we can analytically describe the learning dynamics of various parameter combinations at finite learning rates and batch sizes for state of the art architectures trained on any dataset.</p>","tags":["dl2","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/No%20Train%2C%20all%20Gain%20-%20Self-Supervised%20Gradients%20Improve%20Deep%20Frozen%20Representations%20Yuki%20M%20Asano/","title":"No Train, all Gain   Self Supervised Gradients Improve Deep Frozen Representations Yuki M Asano","text":"Properties authors Walter Simoncini, Spyros Gidaris, Andrei Bursuc, Yuki M. Asano year 2024 url https://arxiv.org/abs/2407.10964 <p>Abstract</p> <p>This paper introduces FUNGI, Features from UNsupervised GradIents, a method to enhance the features of transformer encoders by leveraging self-supervised gradients. Our method is simple: given any pretrained model, we first compute gradients from various self-supervised objectives for each input. These gradients are projected to a lower dimension and then concatenated with the model\u2019s output embedding. The resulting features are evaluated on k-nearest neighbor classification over 11 datasets from vision, 5 from natural language processing, and 2 from audio. Across backbones spanning various sizes and pretraining strategies, FUNGI features provide consistent performance improvements over the embeddings. We also show that using FUNGI features can benefit linear classification, clustering and image retrieval, and that they significantly improve the retrieval-based in-context scene understanding abilities of pretrained models, for example improving upon DINO by +17% for semantic segmentation \u2013 without any training. Code is available at https://github.com/WalterSimoncini/fungivision.</p>","tags":["paper"]},{"location":"100%20Reference%20notes/101%20Literature/No%20Train%2C%20all%20Gain%20-%20Self-Supervised%20Gradients%20Improve%20Deep%20Frozen%20Representations%20Yuki%20M%20Asano/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper"]},{"location":"100%20Reference%20notes/101%20Literature/Occam%27s%20model%20-%20Selecting%20simpler%20representations%20for%20better%20transferability%20estimation/","title":"Occam's model   Selecting simpler representations for better transferability estimation","text":"Properties authors Prabhant Singh, Sibylle Hess, Joaquin Vanschoren year 2025 url http://arxiv.org/abs/2502.06925 <p>Abstract</p> <p>Fine-tuning models that have been pre-trained on large datasets has become a cornerstone of modern machine learning workflows. With the widespread availability of online model repositories, such as Hugging Face, it is now easier than ever to fine-tune pre-trained models for specific tasks. This raises a critical question: which pretrained model is most suitable for a given task? This problem is called transferability estimation. In this work, we introduce two novel and effective metrics for estimating the transferability of pre-trained models. Our approach is grounded in viewing transferability as a measure of how easily a pre-trained model\u2019s representations can be trained to separate target classes, providing a unique perspective on transferability estimation. We rigorously evaluate the proposed metrics against state-of-the-art alternatives across diverse problem settings, demonstrating their robustness and practical utility. Additionally, we present theoretical insights that explain our metrics\u2019 efficacy and adaptability to various scenarios. We experimentally show that our metrics increase Kendall\u2019s Tau by up to 32% compared to the state-of-the-art baselines.</p>","tags":["paper"]},{"location":"100%20Reference%20notes/101%20Literature/Occam%27s%20model%20-%20Selecting%20simpler%20representations%20for%20better%20transferability%20estimation/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper"]},{"location":"100%20Reference%20notes/101%20Literature/On%20Good%20Practices%20for%20Task-Specific%20Distillation%20of%20Large%20Pretrained%20Visual%20Models/","title":"On Good Practices for Task Specific Distillation of Large Pretrained Visual Models","text":"Properties authors Juliette Marrie, Michael Arbel, Julien Mairal, Diane Larlus year 2024 url https://arxiv.org/abs/2402.11305 <p>Abstract</p> <p>Large pretrained visual models exhibit remarkable generalization across diverse recognition tasks. Yet, real-world applications often demand compact models tailored to specific problems. Variants of knowledge distillation have been devised for such a purpose, enabling task-specific compact models (the students) to learn from a generic large pretrained one (the teacher). In this paper, we show that the excellent robustness and versatility of recent pretrained models challenge common practices established in the literature, calling for a new set of optimal guidelines for task-specific distillation. To address the lack of samples in downstream tasks, we also show that a variant of Mixup based on stable diffusion complements standard data augmentation. This strategy eliminates the need for engineered text prompts and improves distillation of generic models into streamlined specialized networks.</p>","tags":["paper","distillation","foundation_models","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/On%20Good%20Practices%20for%20Task-Specific%20Distillation%20of%20Large%20Pretrained%20Visual%20Models/#notes","title":"Notes","text":"","tags":["paper","distillation","foundation_models","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/On%20the%20Relationship%20between%20Self-Attention%20and%20Convolutional%20Layers/","title":"On the Relationship between Self Attention and Convolutional Layers","text":"Properties authors Jean-Baptiste Cordonnier, Andreas Loukas, Martin Jaggi year 2020 url https://arxiv.org/abs/1911.03584 <p>Abstract</p> <p>Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Stand-Alone Self-Attention in Vision Models showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that self-attention layers attend to pixel-grid patterns similarly to CNN layers, corroborating our analysis. </p>","tags":["transformers","convolutions","theory"]},{"location":"100%20Reference%20notes/101%20Literature/On%20the%20Relationship%20between%20Self-Attention%20and%20Convolutional%20Layers/#notes","title":"Notes","text":"<ul> <li> Note to self: fully read article, it looks fun \u23eb #personal </li> </ul>","tags":["transformers","convolutions","theory"]},{"location":"100%20Reference%20notes/101%20Literature/On%20the%20Symmetries%20of%20Deep%20Learning%20Models%20and%20their%20Internal%20Representations/","title":"On the Symmetries of Deep Learning Models and their Internal Representations","text":"Properties authors Charles Godfrey, Davis Brown, Tegan Emerson, Henry Kvnige year 2022 url https://arxiv.org/abs/2205.14258 <p>Abstract</p> <p>Symmetry is a fundamental tool in the exploration of a broad range of complex systems. In machine learning symmetry has been explored in both models and data. In this paper we seek to connect the symmetries arising from the architecture of a family of models with the symmetries of that family's internal representation of data. We do this by calculating a set of fundamental symmetry groups, which we call the intertwiner groups of the model. We connect intertwiner groups to a model's internal representations of data through a range of experiments that probe similarities between hidden states across models with the same architecture. Our work suggests that the symmetries of a network are propagated into the symmetries in that network's representation of data, providing us with a better understanding of how architecture affects the learning and prediction process. Finally, we speculate that for ReLU networks, the intertwiner groups may provide a justification for the common practice of concentrating model interpretability exploration on the activation basis in hidden layers rather than arbitrary linear combinations thereof.</p> <p>Notes: - The following papers study the effect of weight space symmetries on training dynamics:     - Neural Mechanics - Symmetry and Broken Conservation Laws in Deep Learning Dynamics     - Understanding symmetries in deep networks     - G-SGD - Optimizing ReLU Neural Networks in its Positively Scale-Invariant Space      - Deep Learning Book</p>","tags":["dl_theory","dl2"]},{"location":"100%20Reference%20notes/101%20Literature/On%20the%20duality%20between%20contrastive%20and%20non-contrastive%20self-supervised%20learning/","title":"On the duality between contrastive and non contrastive self supervised learning","text":"Properties authors Quentin Garrido, Yubei Chen, Adrien Bardes, Laurent Najman, Yann Lecun year 2022 url http://arxiv.org/abs/2206.02574 <p>Abstract</p> <p>Recent approaches in self-supervised learning of image representations can be categorized into different families of methods and, in particular, can be divided into contrastive and non-contrastive approaches. While differences between the two families have been thoroughly discussed to motivate new approaches, we focus more on the theoretical similarities between them. By designing contrastive and covariance based non-contrastive criteria that can be related algebraically and shown to be equivalent under limited assumptions, we show how close those families can be. We further study popular methods and introduce variations of them, allowing us to relate this theoretical result to current practices and show the influence (or lack thereof) of design choices on downstream performance. Motivated by our equivalence result, we investigate the low performance of SimCLR and show how it can match VICReg\u2019s with careful hyperparameter tuning, improving significantly over known baselines. We also challenge the popular assumption that non-contrastive methods need large output dimensions. Our theoretical and quantitative results suggest that the numerical gaps between contrastive and noncontrastive methods in certain regimes can be closed given better network design choices and hyperparameter tuning. The evidence shows that unifying different SOTA methods is an important direction to build a better understanding of selfsupervised learning.</p>","tags":["paper","dl_theory","computer_vision","ssl","self-distillation","representation_learning","hyperspherical"]},{"location":"100%20Reference%20notes/101%20Literature/On%20the%20duality%20between%20contrastive%20and%20non-contrastive%20self-supervised%20learning/#notes","title":"Notes","text":"<p>Zotero Link</p> <p>We focus on covariance regularization-based non-contrastive methods (Zbontar et al., 2021; Ermolov et al., 2021; Bardes et al., 2021) and demonstrate that these methods can be seen as contrastive between the dimensions of the embeddings instead of contrastive between the samples</p> <p>and demonstrate that these methods can be seen as contrastive between the dimensions of the embeddings instead of contrastive between the samples</p> <p>Even though they do not fit perfectly in our framework, we discuss how methods such as DINO, SimSiam, or MoCo can be linked to Lc and Lnc in supplementary section B.</p> <p>This means that penalizing all the terms of the Gram matrix (i.e., pairwise similarities) is the same as penalizing all of the terms of the Covariance matrix.</p> <p>However, if both rows and columns of K were L2 normalized, we would have Lnc = Lc + N \u2212 M .</p> <p>Since SimCLR relies on the cosine distance as a similarity measure between embeddings, we can effectively say that it uses normalized embeddings.</p> <p>Spectral Contrastive Loss projects the embeddings on a  ball of radius \u221a\u03bc, with \u03bc a tuned parameter,</p> <p>VICReg takes a similar approach where dimensions are centered, but their variance is regularized by the variance criterion.</p> <p>One of the main differences between normalizing embeddings or dimensions is that in the former case, embeddings are projected on a M \u2212 1 dimensional hypersphere, and in the latter, they are not constrained on a particular manifold; instead, their spread in the ambient space is limited.</p> <p>Nonetheless, a constraint on the norm of the embeddings also constrains the norm of the dimensions indirectly, and vice versa,</p> <p>Both correspond to collapsed representations and will thus not be attained in practice</p> <p>Considering the previous discussions, we thus argue that the main differences between sample-contrastive and dimension-contrastive methods come from the optimization process as well as the implementation details.</p> <p>The conclusion is that while some design choices negatively impact the optimization process on the embeddings, there are no easily visible differences in the representations which are used in practice.</p>","tags":["paper","dl_theory","computer_vision","ssl","self-distillation","representation_learning","hyperspherical"]},{"location":"100%20Reference%20notes/101%20Literature/OpenELM%20-%20An%20Efficient%20Language%20Model%20Family%20with%20Open-source%20Training%20and%20Inference%20Framework/","title":"OpenELM   An Efficient Language Model Family with Open source Training and Inference Framework","text":"Properties authors Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Dmitry Belenko, Peter Zatloukal, Mohammad Rastegari year 2024 url https://arxiv.org/abs/2404.14619 <p>Abstract</p> <p>The reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks. To this end, we release OpenELM, a state-of-the-art open language model. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy. For example, with a parameter budget of approximately one billion parameters, OpenELM exhibits a 2.36% improvement in accuracy compared to OLMo while requiring\u00a02\u00d7\u00a0fewer pre-training tokens. Diverging from prior practices that only provide model weights and inference code, and pre-train on private datasets, our release includes the complete framework for training and evaluation of the language model on publicly available datasets, including training logs, multiple checkpoints, and pre-training configurations. We also release code to convert models to MLX library for inference and fine-tuning on Apple devices. This comprehensive release aims to empower and strengthen the open research community, paving the way for future open research endeavors. Our source code along with pre-trained model weights and training recipes is available at \\url{this https URL}. Additionally, \\model models can be found on HuggingFace at: \\url{this https URL}.</p>","tags":["llm","foundation_models"]},{"location":"100%20Reference%20notes/101%20Literature/Optimal%20Brain%20Damage/","title":"Optimal Brain Damage","text":"Properties authors John Denker, Sara Solla, Yann LeCun year 1989 url https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html <p>Abstract</p> <p>We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.</p> <p>OBD Pruning Algorithm</p> <p>Use saliency measure based on Hessian (loss wrt parameters) to pick which parameters to prune. Finetune afterwards.</p>","tags":["paper","efficient_vision","efficient_dl","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Optimization%20Dynamics%20of%20Equivariant%20and%20Augmented%20Neural%20Networks/","title":"Optimization Dynamics of Equivariant and Augmented Neural Networks","text":"Properties authors Alex Flinth, Fredrik Ohlsson year 2023 url https://arxiv.org/abs/2303.13458 <p>Abstract</p> <p>We investigate the optimization of multilayer perceptrons on symmetric data. We compare the strategy of constraining the architecture to be equivariant to that of using augmentation. We show that, under natural assumptions on the loss and non-linearities, the sets of equivariant stationary points are identical for the two strategies, and that the set of equivariant layers is invariant under the gradient flow for augmented models. Finally, we show that stationary points may be unstable for augmented training although they are stable for the equivariant models.</p> <p>Main observations: 1. They show that if the augmented model is equivariantly initialized, it will remain equivariant during training (See Equivariance Initialization) 3. Compared to the equivariant approach, augmentation introduces no new equivariant stationary points, nor does it exclude existing ones. (See Multiple global minima) 4. The existence of a stable equivariant minimum is not guaranteed by augmentation. (See Multiple global minima)</p> <p>Regarding Equivariance Initialization in this work:</p> <p>We initialize \u03a6A with equivariant layers A0 \u2208 E by drawing matrices randomly from a standard Gaussian distribution, and then projecting them orthogonally onto E. We train the network on (finite) datasets D using gradient descent in three different ways. </p> <p>My intuition is that they do something like the isotropic convolution from Priors over Neural Network weights</p>","tags":["dl_theory","equivariance","optimization"]},{"location":"100%20Reference%20notes/101%20Literature/PART%20-%20Self-supervised%20Pretraining%20with%20Pairwise%20Relative%20Translations/","title":"PART   Self supervised Pretraining with Pairwise Relative Translations","text":"Properties authors Melika Ayoughi year 2024 url https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5257874 <p>Abstract</p> <p>The composition of objects and their parts, along with object-object positional relationships, provides a rich source of information for representation learning. Hence, spatial-aware pretext tasks have been actively explored in self-supervised learning. Existing works commonly start from a grid structure, where the goal of the pretext task involves predicting the absolute position index of patches within a fixed grid. However, grid-based approaches fall short of capturing the fluid and continuous nature of real-world object compositions. We introduce PART, a self-supervised learning approach that leverages continuous relative transformations between off-grid patches to overcome these limitations. By modeling how parts relate to each other in a continuous space, PART learns the relative composition of images\u2014an o\u21b5-grid structural relative positioning process that generalizes beyond occlusions and deformations. In tasks requiring precise spatial understanding such as object detection and time series prediction, PART outperforms strong grid-based methods like MAE and DropPos, while also maintaining competitive performance on global classification tasks with minimal hyperparameter tuning. By breaking free from grid constraints, PART opens up an exciting new trajectory for universal self-supervised pretraining across diverse datatypes\u2014from natural images to EEG signals\u2014with promising potential in video, medical imaging, and audio</p>","tags":["paper","foundation_models","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/PIN%20-%20Positional%20Insert%20Unlocks%20Object%20Localisation%20Abilities%20in%20VLMs/","title":"PIN   Positional Insert Unlocks Object Localisation Abilities in VLMs","text":"Properties authors Michael Dorkenwald, Nimrod Barazani, Cees G. M. Snoek, Yuki M. Asano year 2024 url http://arxiv.org/abs/2402.08657 <p>Abstract</p> <p>Vision-Language Models (VLMs), such as Flamingo and GPT-4V, have shown immense potential by integrating large language models with vision systems. Nevertheless, these models face challenges in the fundamental computer vision task of object localisation, due to their training on multimodal data containing mostly captions without explicit spatial grounding. While it is possible to construct custom, supervised training pipelines with bounding box annotations that integrate with VLMs, these result in specialized and hard-to-scale models. In this paper, we aim to explore the limits of caption-based VLMs and instead propose to tackle the challenge in a simpler manner by i) keeping the weights of a caption-based VLM frozen and ii) not using any supervised detection data. To this end, we introduce an input-agnostic Positional Insert (PIN), a learnable spatial prompt, containing a minimal set of parameters that are slid inside the frozen VLM, unlocking object localisation capabilities. Our PIN module is trained with a simple next-token prediction task on synthetic data without requiring the introduction of new output heads. Our experiments demonstrate strong zero-shot localisation performances on a variety of images, including Pascal VOC, COCO, LVIS, and diverse images like paintings or cartoons.</p>","tags":["paper","multimodal","object_localisation"]},{"location":"100%20Reference%20notes/101%20Literature/PIN%20-%20Positional%20Insert%20Unlocks%20Object%20Localisation%20Abilities%20in%20VLMs/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","multimodal","object_localisation"]},{"location":"100%20Reference%20notes/101%20Literature/Parameter%20Efficient%20Fine-tuning%20of%20Self-supervised%20ViTs%20without%20Catastrophic%20Forgetting/","title":"Parameter Efficient Fine tuning of Self supervised ViTs without Catastrophic Forgetting","text":"Properties authors Reza Akbarian Bafghi, Nidhin Harilal, Claire Monteleoni, Maziar Raissi year 2024 url https://arxiv.org/abs/2404.17245 <p>Abstract</p> <p>Artificial neural networks often suffer from catastrophic forgetting, where learning new concepts leads to a complete loss of previously acquired knowledge. We observe that this issue is particularly magnified in vision transformers (ViTs), where post-pre-training and fine-tuning on new tasks can significantly degrade the model's original general abilities. For instance, a DINO ViT-Base/16 pre-trained on ImageNet-1k loses over 70% accuracy on ImageNet-1k after just 10 iterations of fine-tuning on CIFAR-100. Overcoming this stability-plasticity dilemma is crucial for enabling ViTs to continuously learn and adapt to new domains while preserving their initial knowledge. In this work, we study two new parameter-efficient fine-tuning strategies: (1)~Block Expansion, and (2) Low-rank adaptation (LoRA). Our experiments reveal that using either Block Expansion or LoRA on self-supervised pre-trained ViTs surpass fully fine-tuned ViTs in new domains while offering significantly greater parameter efficiency. Notably, we find that Block Expansion experiences only a minimal performance drop in the pre-training domain, thereby effectively mitigating catastrophic forgetting in pre-trained ViTs.</p>","tags":["paper"]},{"location":"100%20Reference%20notes/101%20Literature/Parameter%20Efficient%20Fine-tuning%20of%20Self-supervised%20ViTs%20without%20Catastrophic%20Forgetting/#paper-results","title":"Paper Results","text":"Model N. params CIFAR-100 IN-1K Mean Standard Fine-tuning All 85.9 M 88.13 25.24 56.69 Top-3 21.3 M 84.56 74.15 79.36 Linear 76.9 K 80.57 76.11 78.34 LoRA \ud835\udc5f=4 301 K 87.91 66.82 77.37 \ud835\udc5f=8 448 K 88.27 65.99 77.13 \ud835\udc5f=16 743 K 87.84 65.06 76.45 Block Expansion \ud835\udc5d=1 7.2 M 82.72 75.75 79.24 \ud835\udc5d=2 14.3 M 86.70 75.54 81.12 \ud835\udc5d=3 21.3 M 88.58 74.61 81.60 \ud835\udc5d=4 28.4 M 89.09 72.28 80.69","tags":["paper"]},{"location":"100%20Reference%20notes/101%20Literature/Parameter%20Efficient%20Fine-tuning%20of%20Self-supervised%20ViTs%20without%20Catastrophic%20Forgetting/#observations","title":"Observations","text":"<ul> <li>Linear only fine-tuning does pretty well, kinda surprising.</li> <li>It's kind of suprising that LoRa Adapter do bad, but does it matter? What is the purpose of making LoRa resistant to catastrophic forgetting if the whole point of it is to be able to hot-swap modules depending on the task?</li> <li>Also worthy to point out that Block Expansion requires training parameters in the order of millions while LoRa only requires thousands. </li> </ul>","tags":["paper"]},{"location":"100%20Reference%20notes/101%20Literature/Parameter-Efficient%20Fine-Tuning%20for%20Pre-Trained%20Vision%20Models%20-%20A%20Survey/","title":"Parameter Efficient Fine Tuning for Pre Trained Vision Models   A Survey","text":"Properties authors Yi Xin, Siqi Luo, Haodi Zhou, Junlong Du, Xiaohong Liu, Yue Fan, Qing Li, Yuntao Du year 2024 url https://arxiv.org/abs/2402.02242 <p>Abstract</p> <p>Large-scale pre-trained vision models (PVMs) have shown great potential for adaptability across various downstream vision tasks. However, with state-of-the-art PVMs growing to billions or even trillions of parameters, the standard full fine-tuning paradigm is becoming unsustainable due to high computational and storage demands. In response, researchers are exploring parameter-efficient fine-tuning (PEFT), which seeks to exceed the performance of full fine-tuning with minimal parameter modifications. This survey provides a comprehensive overview and future directions for visual PEFT, offering a systematic review of the latest advancements. First, we provide a formal definition of PEFT and discuss model pre-training methods. We then categorize existing methods into three categories: addition-based, partial-based, and unified-based. Finally, we introduce the commonly used datasets and applications and suggest potential future research challenges. A comprehensive collection of resources is available at\u00a0this https URL.</p> <p></p>","tags":["paper","efficient_dl","efficient_vision","transformers","peft"]},{"location":"100%20Reference%20notes/101%20Literature/Patch-Wise%20Self-Supervised%20Visual%20Representation%20Learning%20-%20A%20Fine-Grained%20Approach/","title":"Patch Wise Self Supervised Visual Representation Learning   A Fine Grained Approach","text":"Properties authors Ali Javidani, Mohammad Amin Sadeghi, Babak Nadjar Araabi year 2024 url https://www.researchsquare.com/article/rs-4662935/v1 <p>Abstract</p> <p>Self-supervised visual representation learning traditionally focuses on image-level instance discrimination. Our study introduces an innovative, fine-grained dimension by integrating patch-level discrimination into these methodologies. This integration allows for the simultaneous analysis of local and global visual features, thereby enriching the quality of the learned representations. Initially, the original images undergo spatial augmentation. Subsequently, we employ a distinctive photometric patch-level augmentation, where each patch is individually augmented, independent from other patches within the same view. This approach generates a diverse training dataset with distinct color variations in each segment. The augmented images are then processed through a self-distillation learning framework, utilizing the Vision Transformer (ViT) as its backbone. The proposed method minimizes the representation distances across both image and patch levels to capture details from macro to micro perspectives. To this end, we present a simple yet effective patch-matching algorithm to find the corresponding patches across the augmented views. Thanks to the efficient structure of the patch-matching algorithm, our method reduces computational complexity compared to similar approaches. Consequently, we achieve an advanced understanding of the model without adding significant computational requirements. We have extensively pretrained our method on datasets of varied scales, such as Cifar10, ImageNet-100, and ImageNet-1K. It demonstrates superior performance over state-of-the-art self-supervised representation learning methods in image classification and downstream tasks, such as copy detection and image retrieval. The implementation of our method is accessible on GitHub.</p>","tags":["paper","vit","ssl","thesis","self-distillation"]},{"location":"100%20Reference%20notes/101%20Literature/Patch-Wise%20Self-Supervised%20Visual%20Representation%20Learning%20-%20A%20Fine-Grained%20Approach/#notes","title":"Notes","text":"<p>Zotero Link</p> <p></p>","tags":["paper","vit","ssl","thesis","self-distillation"]},{"location":"100%20Reference%20notes/101%20Literature/PatchRot%20-%20A%20Self-Supervised%20Technique%20for%20Training%20Vision%20Transformers/","title":"PatchRot   A Self Supervised Technique for Training Vision Transformers","text":"Properties authors Sachin Chhabra, Prabal Bijoy Dutta, Hemanth Venkateswara, Baoxin Li year 2022 url http://arxiv.org/abs/2210.15722 <p>Abstract</p> <p>Vision transformers require a huge amount of labeled data to outperform convolutional neural networks. However, labeling a huge dataset is a very expensive process. Self-supervised learning techniques alleviate this problem by learning features similar to supervised learning in an unsupervised way. In this paper, we propose a self-supervised technique PatchRot that is crafted for vision transformers. PatchRot rotates images and image patches and trains the network to predict the rotation angles. The network learns to extract both global and local features from an image. Our extensive experiments on different datasets showcase PatchRot training learns rich features which outperform supervised learning and compared baseline.</p>","tags":["paper","vit","thesis","ssl"]},{"location":"100%20Reference%20notes/101%20Literature/PatchRot%20-%20A%20Self-Supervised%20Technique%20for%20Training%20Vision%20Transformers/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","vit","thesis","ssl"]},{"location":"100%20Reference%20notes/101%20Literature/Position%20Prediction%20as%20an%20Effective%20Pretraining%20Strategy/","title":"Position Prediction as an Effective Pretraining Strategy","text":"Properties authors Shuangfei Zhai, Navdeep Jaitly, Jason Ramapuram, Dan Busbridge, Tatiana Likhomanenko, Joseph Yitan Cheng, Walter Talbott, Chen Huang, Hanlin Goh, Joshua Susskind year 2022 url https://arxiv.org/abs/2207.07611 <p>Abstract</p> <p>Transformers have gained increasing popularity in a wide range of applications, including Natural Language Processing (NLP), Computer Vision and Speech Recognition, because of their powerful representational capacity. However, harnessing this representational capacity effectively requires a large amount of data, strong regularization, or both, to mitigate overfitting. Recently, the power of the Transformer has been unlocked by self-supervised pretraining strategies based on masked autoencoders which rely on reconstructing masked inputs, directly, or contrastively from unmasked content. This pretraining strategy which has been used in BERT models in NLP, Wav2Vec models in Speech and, recently, in MAE models in Vision, forces the model to learn about relationships between the content in different parts of the input using autoencoding related objectives. In this paper, we propose a novel, but surprisingly simple alternative to content reconstruction~-- that of predicting locations from content, without providing positional information for it. Doing so requires the Transformer to understand the positional relationships between different parts of the input, from their content alone. This amounts to an efficient implementation where the pretext task is a classification problem among all possible positions for each input token. We experiment on both Vision and Speech benchmarks, where our approach brings improvements over strong supervised training baselines and is comparable to modern unsupervised/self-supervised pretraining methods. Our method also enables Transformers trained without position embeddings to outperform ones trained with full position information.</p>","tags":["paper","ssl","foundation_models"]},{"location":"100%20Reference%20notes/101%20Literature/Principal%20Components%20Enable%20A%20New%20Language%20of%20Images/","title":"Principal Components Enable A New Language of Images","text":"Properties authors Xin Wen, Bingchen Zhao, Ismail Elezi, Jiankang Deng, Xiaojuan Qi year 2025 url http://arxiv.org/abs/2503.08685 <p>Abstract</p> <p>We introduce a novel visual tokenization framework that embeds a provable PCA-like structure into the latent token space. While existing visual tokenizers primarily optimize for reconstruction fidelity, they often neglect the structural properties of the latent space -- a critical factor for both interpretability and downstream tasks. Our method generates a 1D causal token sequence for images, where each successive token contributes non-overlapping information with mathematically guaranteed decreasing explained variance, analogous to principal component analysis. This structural constraint ensures the tokenizer extracts the most salient visual features first, with each subsequent token adding diminishing yet complementary information. Additionally, we identified and resolved a semantic-spectrum coupling effect that causes the unwanted entanglement of high-level semantic content and low-level spectral details in the tokens by leveraging a diffusion decoder. Experiments demonstrate that our approach achieves state-of-the-art reconstruction performance and enables better interpretability to align with the human vision system. Moreover, auto-regressive models trained on our token sequences achieve performance comparable to current state-of-the-art methods while requiring fewer tokens for training and inference.</p>","tags":["paper","tokenization","vit","dl_theory","spectral"]},{"location":"100%20Reference%20notes/101%20Literature/Principal%20Components%20Enable%20A%20New%20Language%20of%20Images/#notes","title":"Notes","text":"<p>Zotero Link</p> <p></p> <p></p>","tags":["paper","tokenization","vit","dl_theory","spectral"]},{"location":"100%20Reference%20notes/101%20Literature/Progress%20measures%20for%20grokking%20via%20mechanistic%20interpretability/","title":"Progress measures for grokking via mechanistic interpretability","text":"Properties authors Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, Jacob Steinhardt year 2023 url https://arxiv.org/abs/2301.05217 <p>Abstract</p> <p>Neural networks often exhibit emergent behavior, where qualitatively new capabilities arise from scaling up the amount of parameters, training data, or training steps. One approach to understanding emergence is to find continuous \\textit{progress measures} that underlie the seemingly discontinuous qualitative changes. We argue that progress measures can be found via mechanistic interpretability: reverse-engineering learned behaviors into their individual components. As a case study, we investigate the recently-discovered phenomenon of ``grokking'' exhibited by small transformers trained on modular addition tasks. We fully reverse engineer the algorithm learned by these networks, which uses discrete Fourier transforms and trigonometric identities to convert addition to rotation about a circle. We confirm the algorithm by analyzing the activations and weights and by performing ablations in Fourier space. Based on this understanding, we define progress measures that allow us to study the dynamics of training and split training into three continuous phases: memorization, circuit formation, and cleanup. Our results show that grokking, rather than being a sudden shift, arises from the gradual amplification of structured mechanisms encoded in the weights, followed by the later removal of memorizing components.</p> <p>Related - Grokking</p>","tags":["paper","interpretability","mechinterp"]},{"location":"100%20Reference%20notes/101%20Literature/Provably%20Strict%20Generalisation%20Benefit%20for%20Equivariant%20Models/","title":"Provably Strict Generalisation Benefit for Equivariant Models","text":"Properties authors Bryn Elesedy, Sheheryar Zaidi year 2021 url https://arxiv.org/abs/2102.10333 <p>Abstract</p> <p>It is widely believed that engineering a model to be invariant/equivariant improves generalisation. Despite the growing popularity of this approach, a precise characterisation of the generalisation benefit is lacking. By considering the simplest case of linear models, this paper provides the first provably non-zero improvement in generalisation for invariant/equivariant models when the target distribution is invariant/equivariant with respect to a compact group. Moreover, our work reveals an interesting relationship between generalisation, the number of training examples and properties of the group action. Our results rest on an observation of the structure of function spaces under averaging operators which, along with its consequences for feature averaging, may be of independent interest.</p>","tags":["dl_theory","equivariance"]},{"location":"100%20Reference%20notes/101%20Literature/ProxylessNAS%20-%20Direct%20Neural%20Architecture%20Search%20on%20Target%20Task%20and%20Hardware/","title":"ProxylessNAS   Direct Neural Architecture Search on Target Task and Hardware","text":"Properties authors Han Cai, Ligeng Zhu, Song Han year 2019 url https://arxiv.org/abs/1812.00332 <p>Abstract</p> <p>Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g.\u00a0104\u00a0GPU hours) makes it difficult to \\emph{directly} search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize~\\emph{proxy} tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present \\emph{ProxylessNAS} that can \\emph{directly} learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08\\% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7\u00a0fewer parameters. On ImageNet, our model achieves 3.1\\% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7\u00a0faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.</p>","tags":["paper","efficient_dl","nas"]},{"location":"100%20Reference%20notes/101%20Literature/ProxylessNAS%20-%20Direct%20Neural%20Architecture%20Search%20on%20Target%20Task%20and%20Hardware/#notes","title":"Notes","text":"<ul> <li>To avoid measuring performance on the target device, they learn a latency model.<ol> <li>They take multiple measurements of a device with different architectures.</li> <li>They train a model to predict the latency given the architecture.</li> </ol> </li> </ul>","tags":["paper","efficient_dl","nas"]},{"location":"100%20Reference%20notes/101%20Literature/R-MAE%20-%20Regions%20Meet%20Masked%20Autoencoders/","title":"R MAE   Regions Meet Masked Autoencoders","text":"Properties authors Duy-Kien Nguyen, Vaibhav Aggarwal, Yanghao Li, Martin R. Oswald, Alexander Kirillov, Cees G. M. Snoek, Xinlei Chen year 2023 url https://arxiv.org/abs/2306.05411 <p>Abstract</p> <p>In this work, we explore regions as a potential visual analogue of words for self-supervised image representation learning. Inspired by Masked Autoencoding (MAE), a generative pre-training baseline, we propose masked region autoencoding to learn from groups of pixels or regions. Specifically, we design an architecture which efficiently addresses the one-to-many mapping between images and regions, while being highly effective especially with high-quality regions. When integrated with MAE, our approach (R-MAE) demonstrates consistent improvements across various pre-training datasets and downstream detection and segmentation benchmarks, with negligible computational overheads. Beyond the quantitative evaluation, our analysis indicates the models pre-trained with masked region autoencoding unlock the potential for interactive segmentation. The code is provided at\u00a0this https URL.</p>","tags":["paper","foundation_models"]},{"location":"100%20Reference%20notes/101%20Literature/R-MAE%20-%20Regions%20Meet%20Masked%20Autoencoders/#note","title":"Note","text":"<ul> <li> Note to self: Read in depth</li> </ul>","tags":["paper","foundation_models"]},{"location":"100%20Reference%20notes/101%20Literature/Rate%E2%80%93Distortion%E2%80%93Perception%20Trade-Off%20in%20Information%20Theory%2C%20Generative%20Models%2C%20and%20Intelligent%20Communications/","title":"Rate\u2013Distortion\u2013Perception Trade Off in Information Theory, Generative Models, and Intelligent Communications","text":"Properties authors Xueyan Niu, Bo Bai, Nian Guo, Weixi Zhang, Wei Han year 2025 url https://www.mdpi.com/1099-4300/27/4/373 <p>Abstract</p> <p>Traditional rate\u2013distortion (RD) theory examines the trade-off between the average length of the compressed representation of a source and the additive distortions of its reconstruction. The rate\u2013distortion\u2013perception (RDP) framework, which integrates the perceptual dimension into the RD paradigm, has garnered significant attention due to recent advancements in machine learning, where perceptual fidelity is assessed by the divergence between input and reconstruction distributions. In communication systems where downstream tasks involve generative modeling, high perceptual fidelity is essential, despite distortion constraints. However, while zero distortion implies perfect realism, the converse is not true, highlighting an imbalance in the significance of distortion and perceptual constraints. This article clarifies that incorporating perceptual constraints does not decrease the necessary rate; instead, under certain conditions, additional rate is required, even with the aid of common and private randomness, which are key elements in generative models. Consequently, we project an increase in expected traffic in intelligent communication networks with the consideration of perceptual quality. Nevertheless, a modest increase in rate can enable generative models to significantly enhance the perceptual quality of reconstructions. By exploring the synergies between generative modeling and communication through the lens of information-theoretic results, this article demonstrates the benefits of intelligent communication systems and advocates for the application of the RDP framework in advancing compression and semantic communication research.</p>","tags":["paper","information_theory","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Rate%E2%80%93Distortion%E2%80%93Perception%20Trade-Off%20in%20Information%20Theory%2C%20Generative%20Models%2C%20and%20Intelligent%20Communications/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","information_theory","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Refusal%20in%20Language%20Models%20Is%20Mediated%20by%20a%20Single%20Direction/","title":"Refusal in Language Models Is Mediated by a Single Direction","text":"Properties authors Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Rimsky, Wes Gurnee, Neel Nanda year 2024 url https://arxiv.org/abs/2406.11717 <p>Abstract</p> <p>Conversational large language models are fine-tuned for both instruction-following and safety, resulting in models that obey benign requests but refuse harmful ones. While this refusal behavior is widespread across chat models, its underlying mechanisms remain poorly understood. In this work, we show that refusal is mediated by a one-dimensional subspace, across 13 popular open-source chat models up to 72B parameters in size. Specifically, for each model, we find a single direction such that erasing this direction from the model's residual stream activations prevents it from refusing harmful instructions, while adding this direction elicits refusal on even harmless instructions. Leveraging this insight, we propose a novel white-box jailbreak method that surgically disables refusal with minimal effect on other capabilities. Finally, we mechanistically analyze how adversarial suffixes suppress propagation of the refusal-mediating direction. Our findings underscore the brittleness of current safety fine-tuning methods. More broadly, our work showcases how an understanding of model internals can be leveraged to develop practical methods for controlling model behavior.</p>","tags":["paper","transformers","mechinterp","interpretability"]},{"location":"100%20Reference%20notes/101%20Literature/Relaxed%20Octahedral%20Group%20Convolution%20for%20Learning%20Symmetry%20Breaking%20in%203D%20Physical%20Systems/","title":"Relaxed Octahedral Group Convolution for Learning Symmetry Breaking in 3D Physical Systems","text":"Properties authors Rui Wang, Robin Walters, Tess E Smidt year 2023 url https://arxiv.org/abs/2310.02299 <p>Abstract</p> <p>Deep equivariant models use symmetries to improve sample efficiency and generalization. However, the assumption of perfect symmetry in many of these models can sometimes be restrictive, especially when the data does not perfectly align with such symmetries. Thus, we introduce relaxed octahedral group convolution for modeling 3D physical systems in this paper. This flexible convolution technique provably allows the model to both maintain the highest level of equivariance that is consistent with data and discover the subtle symmetry-breaking factors in the physical systems. Empirical results validate that our approach can not only provide insights into the symmetry-breaking factors in phase transitions but also achieves superior performance in fluid super-resolution tasks.</p>","tags":["relaxed_equivariance","equivariance"]},{"location":"100%20Reference%20notes/101%20Literature/Relaxing%20Equivariance%20Constraints%20with%20Non-stationary%20Continuous%20Filters/","title":"Relaxing Equivariance Constraints with Non stationary Continuous Filters","text":"Properties authors David W. Romero <p>Abstract</p>","tags":["dl2","equivariance","partial_equivariance"]},{"location":"100%20Reference%20notes/101%20Literature/Rethinking%20Lossy%20Compression%20-%20The%20Rate-Distortion-Perception%20Tradeoff/","title":"Rethinking Lossy Compression   The Rate Distortion Perception Tradeoff","text":"Properties authors Yochai Blau, Tomer Michaeli year 2019 url https://arxiv.org/abs/1901.07821 <p>Abstract</p> <p>Lossy compression algorithms are typically designed and analyzed through the lens of Shannon\u2019s rate-distortion theory, where the goal is to achieve the lowest possible distortion (e.g., low MSE or high SSIM) at any given bit rate. However, in recent years, it has become increasingly accepted that \u201clow distortion\u201d is not a synonym for \u201chigh perceptual quality\u201d, and in fact optimization of one often comes at the expense of the other. In light of this understanding, it is natural to seek for a generalization of rate-distortion theory which takes perceptual quality into account. In this paper, we adopt the mathematical de\ufb01nition of perceptual quality recently proposed by Blau &amp; Michaeli (2018), and use it to study the three-way tradeoff between rate, distortion, and perception. We show that restricting the perceptual quality to be high, generally leads to an elevation of the rate-distortion curve, thus necessitating a sacri\ufb01ce in either rate or distortion. We prove several fundamental properties of this triple-tradeoff, calculate it in closed form for a Bernoulli source, and illustrate it visually on a toy MNIST example.</p>","tags":["paper","dl_theory","information_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Rethinking%20Lossy%20Compression%20-%20The%20Rate-Distortion-Perception%20Tradeoff/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","dl_theory","information_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Retrospective%20-%20EIE%20-%20Efficient%20Inference%20Engine%20onSparse%20and%20Compressed%20Neural%20Network/","title":"Retrospective   EIE   Efficient Inference Engine onSparse and Compressed Neural Network","text":"Properties authors Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A. Horowitz, William J. Dally year 2023 url https://arxiv.org/abs/2306.09552 <p>Abstract</p> <p>EIE proposed to accelerate pruned and compressed neural networks, exploiting weight sparsity, activation sparsity, and 4-bit weight-sharing in neural network accelerators. Since published in ISCA'16, it opened a new design space to accelerate pruned and sparse neural networks and spawned many algorithm-hardware co-designs for model compression and acceleration, both in academia and commercial AI chips. In retrospect, we review the background of this project, summarize the pros and cons, and discuss new opportunities where pruning, sparsity, and low precision can accelerate emerging deep learning workloads.</p>","tags":["paper","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/Revealing%20the%20Utilized%20Rank%20of%20Subspaces%20of%20Learning%20in%20Neural%20Networks/","title":"Revealing the Utilized Rank of Subspaces of Learning in Neural Networks","text":"Properties authors Isha Garg, Christian Koguchi, Eshan Verma, Daniel Ulbricht year 2024 url https://arxiv.org/abs/2407.04797 <p>Abstract</p> <p>In this work, we study how well the learned weights of a neural network utilize the space available to them. This notion is related to capacity, but additionally incorporates the interaction of the network architecture with the dataset. Most learned weights appear to be full rank, and are therefore not amenable to low rank decomposition. This deceptively implies that the weights are utilizing the entire space available to them. We propose a simple data-driven transformation that projects the weights onto the subspace where the data and the weight interact. This preserves the functional mapping of the layer and reveals its low rank structure. In our findings, we conclude that most models utilize a fraction of the available space. For instance, for ViTB-16 and ViTL-16 trained on ImageNet, the mean layer utilization is 35% and 20% respectively. Our transformation results in reducing the parameters to 50% and 25% respectively, while resulting in less than 0.2% accuracy drop after fine-tuning. We also show that self-supervised pre-training drives this utilization up to 70%, justifying its suitability for downstream tasks.</p>","tags":["paper","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Revisiting%20Feature%20Prediction%20for%20Learning%20Visual%20Representations%20from%20Video/","title":"Revisiting Feature Prediction for Learning Visual Representations from Video","text":"Properties authors Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mahmoud Assran, Nicolas Ballas year 2024 url http://arxiv.org/abs/2404.08471 <p>Abstract</p> <p>This paper explores feature prediction as a stand-alone objective for unsupervised learning from video and introduces V-JEPA, a collection of vision models trained solely using a feature prediction objective, without the use of pretrained image encoders, text, negative examples, reconstruction, or other sources of supervision. The models are trained on 2 million videos collected from public datasets and are evaluated on downstream image and video tasks. Our results show that learning by predicting video features leads to versatile visual representations that perform well on both motion and appearance-based tasks, without adaption of the model's parameters; e.g., using a frozen backbone. Our largest model, a ViT-H/16 trained only on videos, obtains 81.9% on Kinetics-400, 72.2% on Something-Something-v2, and 77.9% on ImageNet1K.</p>","tags":["paper","video","ssl"]},{"location":"100%20Reference%20notes/101%20Literature/Revisiting%20Feature%20Prediction%20for%20Learning%20Visual%20Representations%20from%20Video/#notes","title":"Notes","text":"<p>Zotero Link</p> <p>Feature prediction leads to versatile visual representations that perform well across downstream image and video tasks without adaption of the model\u2019s weights;</p> <p>In fact, the objective in equation (1) is similar to the loss of Assran et al. (2023) used for image pretraining, but we modify it to use an l1 regression, which we found to be more stable.</p> <p>Incredibly interesting. L1 regression because it was more stable. This is not very intuitive either! L1 is sparsity encouraging and would assume entries in the latents are laplace distributed instead of gaussian distributed.</p> <p>The hypothesis is that incorporating an exponential moving average to compute the representation of y ensures that the predictor evolves faster than the encoder and remains close to optimal, thereby preventing collapse.</p> <p>Masking a large continuous block that covers the full temporal dimension limits information leakage due to the spatial and temporal redundancy of videos, and results in a harder prediction task</p> <p>The encoder is parameterized using standard ViT networks, while the predictor is a narrow transformer implemented using 12 blocks with an embedding dimension of 384.</p>","tags":["paper","video","ssl"]},{"location":"100%20Reference%20notes/101%20Literature/Rewrite%20the%20Stars/","title":"Rewrite the Stars","text":"Properties authors Xu Ma, Xiyang Dai, Yue Bai, Yizhou Wang, Yun Fu year 2024 url https://arxiv.org/abs/2403.19967 <p>Abstract</p> <p>Recent studies have drawn attention to the untapped potential of the \"star operation\" (element-wise multiplication) in network design. While intuitive explanations abound, the foundational rationale behind its application remains largely unexplored. Our study attempts to reveal the star operation's ability to map inputs into high-dimensional, non-linear feature spaces -- akin to kernel tricks -- without widening the network. We further introduce StarNet, a simple yet powerful prototype, demonstrating impressive performance and low latency under compact network structure and efficient budget. Like stars in the sky, the star operation appears unremarkable but holds a vast universe of potential. Our work encourages further exploration across tasks, with codes available at\u00a0this https URL.</p>","tags":["dl_theory","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/Robust%20Self-Supervised%20Learning%20with%20Lie%20Groups/","title":"Robust Self Supervised Learning with Lie Groups","text":"Properties authors Mark Ibrahim, Diane Bouchacourt, Ari Morcos year 2022 url http://arxiv.org/abs/2210.13356 <p>Abstract</p> <p>Deep learning has led to remarkable advances in computer vision. Even so, today\u2019s best models are brittle when presented with variations that differ even slightly from those seen during training. Minor shifts in the pose, color, or illumination of an object can lead to catastrophic misclassi\ufb01cations. State-of-the art models struggle to understand how a set of variations can affect different objects. We propose a framework for instilling a notion of how objects vary in more realistic settings. Our approach applies the formalism of Lie groups to capture continuous transformations to improve models\u2019 robustness to distributional shifts. We apply our framework on top of state-of-the-art self-supervised learning (SSL) models, \ufb01nding that explicitly modeling transformations with Lie groups leads to substantial performance gains of greater than 10% for MAE on both known instances seen in typical poses now presented in new poses, and on unknown instances in any pose. We also apply our approach to ImageNet, \ufb01nding that the Lie operator improves performance by almost 4%. These results demonstrate the promise of learning transformations to improve model robustness1.</p>","tags":["paper","dl_theory","equivariance","ssl"]},{"location":"100%20Reference%20notes/101%20Literature/Robust%20Self-Supervised%20Learning%20with%20Lie%20Groups/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","dl_theory","equivariance","ssl"]},{"location":"100%20Reference%20notes/101%20Literature/Rotary%20Position%20Embedding%20for%20Vision%20Transformer/","title":"Rotary Position Embedding for Vision Transformer","text":"Properties authors Byeongho Heo, Song Park, Dongyoon Han, Sangdoo Yun year 2024 url https://arxiv.org/abs/2403.13298 <p>Abstract</p> <p>Rotary Position Embedding (RoPE) performs remarkably on language models, especially for length extrapolation of Transformers. However, the impacts of RoPE on computer vision domains have been underexplored, even though RoPE appears capable of enhancing Vision Transformer (ViT) performance in a way similar to the language domain. This study provides a comprehensive analysis of RoPE when applied to ViTs, utilizing practical implementations of RoPE for 2D vision data. The analysis reveals that RoPE demonstrates impressive extrapolation performance, i.e., maintaining precision while increasing image resolution at inference. It eventually leads to performance improvement for ImageNet-1k, COCO detection, and ADE-20k segmentation. We believe this study provides thorough guidelines to apply RoPE into ViT, promising improved backbone performance with minimal extra computational overhead. Our code and pre-trained models are available at\u00a0this https URL</p>","tags":["paper","efficient_dl","vit"]},{"location":"100%20Reference%20notes/101%20Literature/Round%20and%20Round%20We%20Go%21%20What%20makes%20Rotary%20Positional%20Encodings%20useful%3F/","title":"Round and Round We Go! What makes Rotary Positional Encodings useful?","text":"Properties authors Federico Barbero, Alex Vitvitskyi, Christos Perivolaropoulos, Razvan Pascanu, Petar Veli\u010dkovi\u0107 year 2024 url https://arxiv.org/abs/2410.06205 <p>Abstract</p> <p>Positional Encodings (PEs) are a critical component of Transformer-based Large Language Models (LLMs), providing the attention mechanism with important sequence-position information. One of the most popular types of encoding used today in LLMs are Rotary Positional Encodings (RoPE), that rotate the queries and keys based on their relative distance. A common belief is that RoPE is useful because it helps to decay token dependency as relative distance increases. In this work, we argue that this is unlikely to be the core reason. We study the internals of a trained Gemma 7B model to understand how RoPE is being used at a mechanical level. We find that Gemma learns to use RoPE to construct robust \"positional\" attention patterns by exploiting the highest frequencies. We also find that, in general, Gemma greatly prefers to use the lowest frequencies of RoPE, which we suspect are used to carry semantic information. We mathematically prove interesting behaviours of RoPE and conduct experiments to verify our findings, proposing a modification of RoPE that fixes some highlighted issues and improves performance. We believe that this work represents an interesting step in better understanding PEs in LLMs, which we believe holds crucial value for scaling LLMs to large sizes and context lengths.</p>","tags":["paper","transformers","posembed","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Round%20and%20Round%20We%20Go%21%20What%20makes%20Rotary%20Positional%20Encodings%20useful%3F/#notes","title":"Notes","text":"<ul> <li>Related </li> </ul>","tags":["paper","transformers","posembed","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/SAM-CLIP%20-%20Merging%20Vision%20Foundation%20Models%20towards%20Semantic%20and%20Spatial%20Understanding/","title":"SAM CLIP   Merging Vision Foundation Models towards Semantic and Spatial Understanding","text":"Properties authors Haoxiang Wang, Fartash Faghri, Raviteja Vemulapalli, Mehrdad Farajtabar, Sachin Mehta, Mohammad Rastegari, Oncel Tuzel, Hadi Pouransari, Pavan Kumar Anasosalu Vasu year 2024 url https://arxiv.org/abs/2310.15308 <p>Abstract</p> <p>The landscape of publicly available vision foundation models (VFMs), such as CLIP and Segment Anything Model (SAM), is expanding rapidly. VFMs are endowed with distinct capabilities stemming from their pre-training objectives. For instance, CLIP excels in semantic understanding, while SAM specializes in spatial understanding for segmentation. In this work, we introduce a simple recipe to efficiently merge VFMs into a unified model that absorbs their expertise. Our method integrates techniques of multi-task learning, continual learning, and distillation. Further, it demands significantly less computational cost compared to traditional multi-task training from scratch, and it only needs a small fraction of the pre-training datasets that were initially used to train individual models. By applying our method to SAM and CLIP, we obtain SAM-CLIP: a unified model that combines the capabilities of SAM and CLIP into a single vision transformer. Compared with deploying SAM and CLIP independently, our merged model, SAM-CLIP, reduces storage and compute costs for inference, making it well-suited for edge device applications. We show that SAM-CLIP not only retains the foundational strengths of SAM and CLIP, but also introduces synergistic functionalities, notably in zero-shot semantic segmentation, where SAM-CLIP establishes new state-of-the-art results on 5 benchmarks. It outperforms previous models that are specifically designed for this task by a large margin, including +6.8% and +5.9% mean IoU improvement on Pascal-VOC and COCO-Stuff datasets, respectively.</p>","tags":["paper","efficient_dl","efficient_vision"]},{"location":"100%20Reference%20notes/101%20Literature/Scaling%20%28Down%29%20CLIP%20-%20A%20Comprehensive%20Analysis%20of%20Data%2C%20Architecture%2C%20and%20Training%20Strategies/","title":"Scaling (Down) CLIP   A Comprehensive Analysis of Data, Architecture, and Training Strategies","text":"Properties authors Zichao Li, Cihang Xie, Ekin Dogus Cubuk year 2024 url https://arxiv.org/abs/2404.08197 <p>Abstract</p> <p>This paper investigates the performance of the Contrastive Language-Image Pre-training (CLIP) when scaled down to limited computation budgets. We explore CLIP along three dimensions: data, architecture, and training strategies. With regards to data, we demonstrate the significance of high-quality training data and show that a smaller dataset of high-quality data can outperform a larger dataset with lower quality. We also examine how model performance varies with different dataset sizes, suggesting that smaller ViT models are better suited for smaller datasets, while larger models perform better on larger datasets with fixed compute. Additionally, we provide guidance on when to choose a CNN-based architecture or a ViT-based architecture for CLIP training. We compare four CLIP training strategies - SLIP, FLIP, CLIP, and CLIP+Data Augmentation - and show that the choice of training strategy depends on the available compute resource. Our analysis reveals that CLIP+Data Augmentation can achieve comparable performance to CLIP using only half of the training data. This work provides practical insights into how to effectively train and deploy CLIP models, making them more accessible and affordable for practical use in various applications.</p>","tags":["efficient_dl","vit","cnn"]},{"location":"100%20Reference%20notes/101%20Literature/Scaling%20and%20Benchmarking%20Self-Supervised%20Visual%20Representation%20Learning/","title":"Scaling and Benchmarking Self Supervised Visual Representation Learning","text":"Properties authors Priya Goyal, Dhruv Mahajan, Abhinav Gupta, Ishan Misra year 2019 url http://arxiv.org/abs/1905.01235 <p>Abstract</p> <p>Self-supervised learning aims to learn representations from the data itself without explicit manual supervision. Existing efforts ignore a crucial aspect of self-supervised learning - the ability to scale to large amount of data because self-supervision requires no manual labels. In this work, we revisit this principle and scale two popular selfsupervised approaches to 100 million images. We show that by scaling on various axes (including data size and problem \u2018hardness\u2019), one can largely match or even exceed the performance of supervised pre-training on a variety of tasks such as object detection, surface normal estimation (3D) and visual navigation using reinforcement learning. Scaling these methods also provides many interesting insights into the limitations of current self-supervised techniques and evaluations. We conclude that current self-supervised methods are not \u2018hard\u2019 enough to take full advantage of large scale data and do not seem to learn effective high level semantic representations. We also introduce an extensive benchmark across 9 different datasets and tasks. We believe that such a benchmark along with comparable evaluation settings is necessary to make meaningful progress. Code is at: https://github.com/facebookresearch/ fair_self_supervision_benchmark.</p>","tags":["paper","ssl","representation_learning"]},{"location":"100%20Reference%20notes/101%20Literature/Scaling%20and%20Benchmarking%20Self-Supervised%20Visual%20Representation%20Learning/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","ssl","representation_learning"]},{"location":"100%20Reference%20notes/101%20Literature/Searching%20for%20Efficient%20Linear%20Layers%20over%20a%20Continuous%20Space%20of%20Structured%20Matrices/","title":"Searching for Efficient Linear Layers over a Continuous Space of Structured Matrices","text":"Properties authors Andres Potapczynski, Shikai Qiu, Marc Finzi, Christopher Ferri, Zixi Chen, Micah Goldblum, Bayan Bruss, Christopher De Sa, Andrew Gordon Wilson year 2024 url http://arxiv.org/abs/2410.02117 <p>Abstract</p> <p>Dense linear layers are the dominant computational bottleneck in large neural networks, presenting a critical need for more efficient alternatives. Previous efforts focused on a small number of hand-crafted structured matrices and neglected to investigate whether these structures can surpass dense layers in terms of computeoptimal scaling laws when both the model size and training examples are optimally allocated. In this work, we present a unifying framework that enables searching among all linear operators expressible via an Einstein summation. This framework encompasses many previously proposed structures, such as low-rank, Kronecker, Tensor-Train, Block Tensor-Train (BTT), and Monarch, along with many novel structures. To analyze the framework, we develop a taxonomy of all such operators based on their computational and algebraic properties and show that differences in the compute-optimal scaling laws are mostly governed by a small number of variables that we introduce. Namely, a small \u03c9 (which measures parameter sharing) and large \u03c8 (which measures the rank) reliably led to better scaling laws. Guided by the insight that full-rank structures that maximize parameters per unit of compute perform the best, we propose BTT-MoE, a novel Mixture-of-Experts (MoE) architecture obtained by sparsifying computation in the BTT structure. In contrast to the standard sparse MoE for each entire feed-forward network, BTT-MoE learns an MoE in every single linear layer of the model, including the projection matrices in the attention blocks. We find BTT-MoE provides a substantial compute-efficiency gain over dense layers and standard MoE.</p>","tags":["paper","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Searching%20for%20Efficient%20Linear%20Layers%20over%20a%20Continuous%20Space%20of%20Structured%20Matrices/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Segment%20Anything/","title":"Segment Anything","text":"Properties authors Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, Ross Girshick year 2023 url https://arxiv.org/abs/2304.02643 <p>Abstract</p> <p>We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at\u00a0this https URL\u00a0to foster research into foundation models for computer vision.</p>","tags":["paper","segmentation","computer_vision","foundation_models"]},{"location":"100%20Reference%20notes/101%20Literature/Self-Guided%20Diffusion%20Models/","title":"Self Guided Diffusion Models","text":"Properties authors Vincent Tao Hu, David W. Zhang, Yuki M. Asano, Gertjan J. Burghouts, Cees G. M. Snoek year 2023 url https://ieeexplore.ieee.org/document/10203989/ <p>Abstract</p> <p>Diffusion models have demonstrated remarkable progress in image generation quality, especially when guidance is used to control the generative process. However, guidance requires a large amount of image-annotation pairs for training and is thus dependent on their availability and correctness. In this paper, we eliminate the need for such annotation by instead exploiting the flexibility of self-supervision signals to design a framework for self-guided diffusion models. By leveraging a feature extraction function and a selfannotation function, our method provides guidance signals at various image granularities: from the level of holistic images to object boxes and even segmentation masks. Our experiments on single-label and multi-label image datasets demonstrate that self-labeled guidance always outperforms diffusion models without guidance and may even surpass guidance based on ground-truth labels. When equipped with self-supervised box or mask proposals, our method further generates visually diverse yet semantically consistent images, without the need for any class, box, or segment label annotation. Self-guided diffusion is simple, flexible and expected to profit from deployment at scale.</p>","tags":["paper","computer_vision","diffusion"]},{"location":"100%20Reference%20notes/101%20Literature/Self-Guided%20Diffusion%20Models/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","computer_vision","diffusion"]},{"location":"100%20Reference%20notes/101%20Literature/Self-Supervised%20Detection%20of%20Perfect%20and%20Partial%20Input-Dependent%20Symmetries/","title":"Self Supervised Detection of Perfect and Partial Input Dependent Symmetries","text":"Properties authors David W. Romero, Alonso Urbano","tags":["dl2","equivariance","partial_equivariance","inductive_bias"]},{"location":"100%20Reference%20notes/101%20Literature/Self-Supervised%20Learning%20from%20Images%20with%20a%20Joint-Embedding%20Predictive%20Architecture/","title":"Self Supervised Learning from Images with a Joint Embedding Predictive Architecture","text":"Properties authors Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, Nicolas Ballas year 2023 url https://arxiv.org/abs/2301.08243 <p>Abstract</p> <p>This paper demonstrates an approach for learning highly semantic image representations without relying on hand-crafted data-augmentations. We introduce the Image-based Joint-Embedding Predictive Architecture (I-JEPA), a non-generative approach for self-supervised learning from images. The idea behind I-JEPA is simple: from a single context block, predict the representations of various target blocks in the same image. A core design choice to guide I-JEPA towards producing semantic representations is the masking strategy; specifically, it is crucial to (a) sample target blocks with sufficiently large scale (semantic), and to (b) use a sufficiently informative (spatially distributed) context block. Empirically, when combined with Vision Transformers, we find I-JEPA to be highly scalable. For instance, we train a ViT-Huge/14 on ImageNet using 16 A100 GPUs in under 72 hours to achieve strong downstream performance across a wide range of tasks, from linear classification to object counting and depth prediction.</p>","tags":["paper","ssl","computer_vision","foundation_models"]},{"location":"100%20Reference%20notes/101%20Literature/Self-Supervised%20Learning%20of%20Object%20Parts%20for%20Semantic%20Segmentation/","title":"Self Supervised Learning of Object Parts for Semantic Segmentation","text":"Properties authors Adrian Ziegler, Yuki M. Asano year 2022 url https://ieeexplore.ieee.org/document/9879393/ <p>Abstract</p> <p>Progress in self-supervised learning has brought strong image representation learning methods. Yet so far, it has mostly focused on image-level learning. In turn, tasks such as unsupervised image segmentation have not benefited from this trend as they require spatially-diverse representations. However, learning dense representations is challenging, as in the unsupervised context it is not clear how to guide the model to learn representations that correspond to various potential object categories. In this paper, we argue that self-supervised learning of object parts is a solution to this issue. Object parts are generalizable: they are a-priori independent of an object definition, but can be grouped to form objects a-posteriori. To this end, we leverage the recently proposed Vision Transformer\u2019s capability of attending to objects and combine it with a spatially dense clustering task for fine-tuning the spatial tokens. Our method surpasses the state-of-the-art on three semantic segmentation benchmarks by 3%-17%, showing that our representations are versatile under various object definitions. Finally, we extend this to fully unsupervised segmentation \u2013 which refrains completely from using label information even at test-time \u2013 and demonstrate that a simple method for automatically merging discovered object parts based on community detection yields substantial gains.</p>","tags":["paper","vit","clustering","ssl","dense","patch_level"]},{"location":"100%20Reference%20notes/101%20Literature/Self-Supervised%20Learning%20of%20Object%20Parts%20for%20Semantic%20Segmentation/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","vit","clustering","ssl","dense","patch_level"]},{"location":"100%20Reference%20notes/101%20Literature/Self-supervised%20learning%20of%20Split%20Invariant%20Equivariant%20representations/","title":"Self supervised learning of Split Invariant Equivariant representations","text":"Properties authors Quentin Garrido, Laurent Najman, Yann Lecun year 2023 url http://arxiv.org/abs/2302.10283 <p>Abstract</p> <p>Recent progress has been made towards learning invariant or equivariant representations with selfsupervised learning. While invariant methods are evaluated on large scale datasets, equivariant ones are evaluated in smaller, more controlled, settings. We aim at bridging the gap between the two in order to learn more diverse representations that are suitable for a wide range of tasks. We start by introducing a dataset called 3DIEBench, consisting of renderings from 3D models over 55 classes and more than 2.5 million images where we have full control on the transformations applied to the objects. We further introduce a predictor architecture based on hypernetworks to learn equivariant representations with no possible collapse to invariance. We introduce SIE (Split InvariantEquivariant) which combines the hypernetworkbased predictor with representations split in two parts, one invariant, the other equivariant, to learn richer representations. We demonstrate significant performance gains over existing methods on equivariance related tasks from both a qualitative and quantitative point of view. We further analyze our introduced predictor and show how it steers the learned latent space. We hope that both our introduced dataset and approach will enable learning richer representations without supervision in more complex scenarios. Code and data are available at https://github.com/facebookresearch/SIE.</p>","tags":["paper","dl_theory","ssl","equivariance","representation_learning"]},{"location":"100%20Reference%20notes/101%20Literature/Self-supervised%20learning%20of%20Split%20Invariant%20Equivariant%20representations/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","dl_theory","ssl","equivariance","representation_learning"]},{"location":"100%20Reference%20notes/101%20Literature/Self-supervised%20learning%20of%20intertwined%20content%20and%20positional%20features%20for%20object%20detection/","title":"Self supervised learning of intertwined content and positional features for object detection","text":"Properties authors Kang-Jun Liu, Masanori Suganuma, Takayuki Okatani year 2025 url https://openreview.net/forum?id=nf4v09zw6O <p>Abstract</p> <p>We present a novel self-supervised feature learning method using Vision Transformers (ViT) as the backbone, specifically designed for object detection and instance segmentation. Our approach addresses the challenge of extracting features that capture both class and positional information, which are crucial for these tasks. The method introduces two key components: (1) a positional encoding tied to the cropping process in contrastive learning, which utilizes a novel vector field representation for positional embeddings; and (2) masking and prediction, similar to conventional Masked Image Modeling (MIM), applied in parallel to both content and positional embeddings of image patches. These components enable the effective learning of intertwined content and positional features. We evaluate our method against state-of-the-art approaches, pre-training on ImageNet-1K and fine-tuning on downstream tasks. Our method outperforms the state-of-the-art SSL methods on the COCO object detection benchmark, achieving significant improvements with fewer pre-training epochs. These results suggest that better integration of positional information into self-supervised learning can improve performance on dense prediction tasks.</p>","tags":["paper","preprint","vit","ssl"]},{"location":"100%20Reference%20notes/101%20Literature/Self-supervised%20learning%20of%20intertwined%20content%20and%20positional%20features%20for%20object%20detection/#notes","title":"Notes","text":"<p>Zotero Link</p> <p>The first is the use of positional encoding tied to the cropping process in contrastive learning; see Fig. 1. In conventional SSL (Caron et al., 2021; Chen et al., 2021; Caron et al., 2024), the position encoding is not aligned with the cropping, meaning the same position embeddings are applied whether processing the full image or a cropped sub-image. We propose representing positional encoding as a vector field with the same dimensions as the input image, which is then cropped in the same manner as the image and sampled on a regular grid, yielding a set of position embeddings of the patches. They are subsequently combined with the content embeddings of their corresponding image patches.</p> <p>our method also applies it to position embeddings.</p> <p>This position and scale augmentation is expected to reduce excessive reliance on absolute positional information within the image and alleviate biases associated with the object scale distribution in ImageNet.</p> <p>is worth noting that some existing methods also incorporate position prediction; however, they predict precise positions using location indicators (Wang et al., 2024; Caron et al., 2024) or at the pixel level (He et al., 2022). In contrast, our method predicts positional information within the feature (embedding) space, representing a fundamentally different approach.</p>","tags":["paper","preprint","vit","ssl"]},{"location":"100%20Reference%20notes/101%20Literature/SimPLR%20-%20A%20Simple%20and%20Plain%20Transformer%20for%20Scaling-Efficient%20Object%20Detection%20and%20Segmentation/","title":"SimPLR   A Simple and Plain Transformer for Scaling Efficient Object Detection and Segmentation","text":"Properties authors Duy-Kien Nguyen, Martin R. Oswald, Cees G. M. Snoek year 2024 url https://arxiv.org/abs/2310.05920 <p>Abstract</p> <p>The ability to detect objects in images at varying scales has played a pivotal role in the design of modern object detectors. Despite considerable progress in removing hand-crafted components and simplifying the architecture with transformers, multi-scale feature maps and/or pyramid design remain a key factor for their empirical success. In this paper, we show that this reliance on either feature pyramids or an hierarchical backbone is unnecessary and a transformer-based detector with scale-aware attention enables the plain detector 'SimPLR' whose backbone and detection head are both non-hierarchical and operate on single-scale features. We find through our experiments that SimPLR with scale-aware attention is plain and simple, yet competitive with multi-scale vision transformer alternatives. Compared to the multi-scale and single-scale state-of-the-art, our model scales much better with bigger capacity (self-supervised) models and more pre-training data, allowing us to report a consistently better accuracy and faster runtime for object detection, instance segmentation as well as panoptic segmentation. Code will be released.</p>","tags":["paper","object_detection","computer_vision","vit"]},{"location":"100%20Reference%20notes/101%20Literature/SimPLR%20-%20A%20Simple%20and%20Plain%20Transformer%20for%20Scaling-Efficient%20Object%20Detection%20and%20Segmentation/#notes","title":"Notes","text":"<p>\u201cDespite enabling plain-backbone detectors, feature pyramids are still an important factor in ViTDet to detect objects at various scales\u201d (Nguyen et al., 2024, p. 4)</p> <p>Not really an issue as far as I understand, but in the spirit of less inductive biases it makes sense. Feature pyramids intuitively hardcode scale information.</p> <p>\u201cMost recently, Lin et al. [35] introduce the transformer-based detector, PlainDETR, which also removes the multi-scale input. However, it still relies on multi-scale features to generate the object proposals for its decoder.\u201d (Nguyen et al., 2024, p. 4)</p> <p>Don't quite understand this, does this still allow arbitrary vits? - [ ] Read PlainDETR \ud83d\udd3d </p>","tags":["paper","object_detection","computer_vision","vit"]},{"location":"100%20Reference%20notes/101%20Literature/Simplifying%20DINO%20via%20Coding%20Rate%20Regularization/","title":"Simplifying DINO via Coding Rate Regularization","text":"Properties authors Ziyang Wu, Jingyuan Zhang, Druv Pai, XuDong Wang, Chandan Singh, Jianwei Yang, Jianfeng Gao, Yi Ma year 2025 url http://arxiv.org/abs/2502.10385 <p>Abstract</p> <p>DINO and DINOv2 are two model families being widely used to learn representations from unlabeled imagery data at large scales. Their learned representations often enable state-of-theart performance for downstream tasks, such as image classification and segmentation. However, they employ many empirically motivated design choices and their training pipelines are highly complex and unstable \u2014 many hyperparameters need to be carefully tuned to ensure that the representations do not collapse \u2014 which poses considerable difficulty to improving them or adapting them to new domains. In this work, we posit that we can remove most such-motivated idiosyncrasies in the pre-training pipelines, and only need to add an explicit coding rate term in the loss function to avoid collapse of the representations. As a result, we obtain highly simplified variants of the DINO and DINOv2 which we call SimDINO and SimDINOv2, respectively. Remarkably, these simplified models are more robust to different design choices, such as network architecture and hyperparameters, and they learn even higher-quality representations, measured by performance on downstream tasks, offering a Pareto improvement over the corresponding DINO and DINOv2 models. This work highlights the potential of using simplifying design principles to improve the empirical practice of deep learning.</p>","tags":["paper","information_theory","ssl","dl_theory","vit","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/Simplifying%20DINO%20via%20Coding%20Rate%20Regularization/#notes","title":"Notes","text":"<p>Zotero Link</p> <p>\u201cTo explain why DINO does not collapse, we wish to highlight the centering operation in (3), which computes batch statistics during its EMA update, hence using negative samples and implicitly pushing different samples\u2019 features apart, even though the precise conceptual mechanism by which this occurs is not clear\u201d (Wu et al., 2025, p. 3)</p> <p>\u201cTo see why this works, consider if some patches are masked, and the model is able to predict masked patches using their unmasked neighbors, then from each patch the model can extract strong information about the semantics of nearby patches\u201d (Wu et al., 2025, p. 4)</p> <p>\u201cNamely, we dispense with the DINO/iBOT heads, the Sinkhorn-Knopp centering, and the softmaxes, and compute the Euclidean distance-based loss directly on normalized features.\u201d (Wu et al., 2025, p. 5)</p> <p>It's interesting that SK Centering can be replaced also.</p> <p>\u201cDue to the explicit coding rate regularization, it is possible to train SimDINO without self-distillation. To validate this, we train ViT-S models on ImageNet-1k by setting the teacher network to be the student network at each iteration, effectively removing the EMA operation. Results are presented in Table 8. We can see that the original DINO collapses under this setup for reasons discussed in Appendix B, while SimDINO is able to yield non-trivial performance. It is worth noting that compared to training with full self-distillation, this variant primarily lags behind in terms of k-NN performance while the gap in linear probe is significantly smaller.\u201d (Wu et al., 2025, p. 16)</p> <p>{ width=\"500\" }</p> <p>This is, I think, one of the most important advantages of using the CRR formulation over DINO's.  Training doesn't actually collapse without self-distillation.</p> <p>\u201cOn the theoretical side, our simplified framework provides an entry point for studying the geometric properties of the global optima of self-supervised learning losses.\u201d (Wu et al., 2025, p. 8)</p>","tags":["paper","information_theory","ssl","dl_theory","vit","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/Simultaneous%20linear%20connectivity%20of%20neural%20networks%20modulo%20permutation/","title":"Simultaneous linear connectivity of neural networks modulo permutation","text":"Properties authors Ekansh Sharma, Devin Kwok, Tom Denton, Daniel M. Roy, David Rolnick, Gintare Karolina Dziugaite year 2024 url https://arxiv.org/abs/2404.06498 <p>Abstract</p> <p>Neural networks typically exhibit permutation symmetries which contribute to the non-convexity of the networks' loss landscapes, since linearly interpolating between two permuted versions of a trained network tends to encounter a high loss barrier. Recent work has argued that permutation symmetries are the only sources of non-convexity, meaning there are essentially no such barriers between trained networks if they are permuted appropriately. In this work, we refine these arguments into three distinct claims of increasing strength. We show that existing evidence only supports \"weak linear connectivity\"-that for each pair of networks belonging to a set of SGD solutions, there exist (multiple) permutations that linearly connect it with the other networks. In contrast, the claim \"strong linear connectivity\"-that for each network, there exists one permutation that simultaneously connects it with the other networks-is both intuitively and practically more desirable. This stronger claim would imply that the loss landscape is convex after accounting for permutation, and enable linear interpolation between three or more independently trained models without increased loss. In this work, we introduce an intermediate claim-that for certain sequences of networks, there exists one permutation that simultaneously aligns matching pairs of networks from these sequences. Specifically, we discover that a single permutation aligns sequences of iteratively trained as well as iteratively pruned networks, meaning that two networks exhibit low loss barriers at each step of their optimization and sparsification trajectories respectively. Finally, we provide the first evidence that strong linear connectivity may be possible under certain conditions, by showing that barriers decrease with increasing network width when interpolating among three networks.</p>","tags":["dl_theory","linear_connectivity","network_permutation_symmetries"]},{"location":"100%20Reference%20notes/101%20Literature/Slicing%20Vision%20Transformer%20for%20Flexible%20Inference/","title":"Slicing Vision Transformer for Flexible Inference","text":"Properties authors Yitian Zhang, Huseyin Coskun, Xu Ma, Huan Wang, Ke Ma,  Xi,  Chen, Derek Hao Hu, Yun Fu year 2024 url http://arxiv.org/abs/2412.04786 <p>Abstract</p> <p>Vision Transformers (ViT) is known for its scalability. In this work, we target to scale down a ViT to fit in an environment with dynamic-changing resource constraints. We observe that smaller ViTs are intrinsically the sub-networks of a larger ViT with different widths. Thus, we propose a general framework, named Scala, to enable a single network to represent multiple smaller ViTs with flexible inference capability, which aligns with the inherent design of ViT to vary from widths. Concretely, Scala activates several subnets during training, introduces Isolated Activation to disentangle the smallest sub-network from other subnets, and leverages Scale Coordination to ensure each sub-network receives simplified, steady, and accurate learning objectives. Comprehensive empirical validations on different tasks demonstrate that with only one-shot training, Scala learns slimmable representation without modifying the original ViT structure and matches the performance of Separate Training. Compared with the prior art, Scala achieves an average improvement of 1.6% on ImageNet-1K with fewer parameters. Code is available at here.</p>","tags":["paper"]},{"location":"100%20Reference%20notes/101%20Literature/Slicing%20Vision%20Transformer%20for%20Flexible%20Inference/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper"]},{"location":"100%20Reference%20notes/101%20Literature/Stand-Alone%20Self-Attention%20in%20Vision%20Models/","title":"Stand Alone Self Attention in Vision Models","text":"Properties authors Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, Jonathon Shlens year 2019 <p>Abstract</p> <p>Convolutions are a fundamental building block of modern computer vision systems. Recent approaches have argued for going beyond convolutions in order to capture long-range dependencies. These efforts focus on augmenting convolutional models with content-based interactions, such as self-attention and non-local means, to achieve gains on a number of vision tasks. The natural question that arises is whether attention can be a stand-alone primitive for vision models instead of serving as just an augmentation on top of convolutions. In developing and testing a pure self-attention vision model, we verify that self-attention can indeed be an effective stand-alone layer. A simple procedure of replacing all instances of spatial convolutions with a form of self-attention applied to ResNet model produces a fully self-attentional model that outperforms the baseline on ImageNet classification with 12% fewer FLOPS and 29% fewer parameters. On COCO object detection, a pure self-attention model matches the mAP of a baseline RetinaNet while having 39% fewer FLOPS and 34% fewer parameters. Detailed ablation studies demonstrate that self-attention is especially impactful when used in later layers. These results establish that stand-alone self-attention is an important addition to the vision practitioner's toolbox.</p>","tags":["vit","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/Stochastic%20positional%20embeddings%20improve%20masked%20image%20modeling/","title":"Stochastic positional embeddings improve masked image modeling","text":"Properties authors Amir Bar, Florian Bordes, Assaf Shocher, Mahmoud Assran, Pascal Vincent, Nicolas Ballas, Trevor Darrell, Amir Globerson, Yann LeCun year 2024 url http://arxiv.org/abs/2308.00566 <p>Abstract</p> <p>Masked Image Modeling (MIM) is a promising self-supervised learning approach that enables learning from unlabeled images. Despite its recent success, learning good representations through MIM remains challenging because it requires predicting the right semantic content in accurate locations. For example, given an incomplete picture of a dog, we can guess that there is a tail, but we cannot determine its exact location. In this work, we propose to incorporate location uncertainty into MIM by using stochastic positional embeddings (StoP). Specifically, we condition the model on stochastic masked token positions drawn from a Gaussian distribution. StoP reduces overfitting to location features and guides the model toward learning features that are more robust to location uncertainties. Quantitatively, StoP improves downstream MIM performance on a variety of downstream tasks, including \\(+1.7\\%\\) on ImageNet linear probing using ViT-B, and \\(+2.5\\%\\) for ViT-H using \\(1\\%\\) of the data.</p>","tags":["paper","ssl","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Stochastic%20positional%20embeddings%20improve%20masked%20image%20modeling/#notes","title":"Notes","text":"<p>Zotero Link</p> <p>we condition the model on stochastic masked token positions drawn from a Gaussian distribution.</p> <p>Without explicitly modeling this location uncertainty, existing MIM models like MAE and I-JEPA might overfit on semantic content in arbitrary locations (e.g, the tail location).</p> <p>we model the position of every masked token</p> <p>as a random variable with a Gaussian distribution where its mean is the position of the patch, and the covariance matrix is learned.</p>","tags":["paper","ssl","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Structuring%20Representation%20Geometry%20with%20Rotationally%20Equivariant%20Contrastive%20Learning/","title":"Structuring Representation Geometry with Rotationally Equivariant Contrastive Learning","text":"Properties authors Sharut Gupta, Joshua Robinson, Derek Lim, Soledad Villar, Stefanie Jegelka year 2023 url http://arxiv.org/abs/2306.13924 <p>Abstract</p> <p>Self-supervised learning converts raw perceptual data such as images to a compact space where simple Euclidean distances measure meaningful variations in data. In this paper, we extend this formulation by adding additional geometric structure to the embedding space by enforcing transformations of input space to correspond to simple (i.e., linear) transformations of embedding space. Specifically, in the contrastive learning setting, we introduce an equivariance objesctive and theoretically prove that its minima forces augmentations on input space to correspond to rotations on the spherical embedding space. We show that merely combining our equivariant loss with a non-collapse term results in non-trivial representations, without requiring invariance to data augmentations. Optimal performance is achieved by also encouraging approximate invariance, where input augmentations correspond to small rotations. Our method, Care: Contrastive Augmentation-induced Rotational Equivariance, leads to improved performance on downstream tasks, and ensures sensitivity in embedding space to important variations in data (e.g., color) that standard contrastive methods do not achieve. Code is available at https://github.com/Sharut/CARE.</p>","tags":["paper","equivariance","ssl","contrastive_learning"]},{"location":"100%20Reference%20notes/101%20Literature/Structuring%20Representation%20Geometry%20with%20Rotationally%20Equivariant%20Contrastive%20Learning/#notes","title":"Notes","text":"<p>Zotero Link</p> <p>In this paper, we extend this formulation by adding additional geometric structure to the embedding space by enforcing transformations of input space to correspond to simple (i.e., linear) transformations of embedding space.</p> <p>We show that merely combining our equivariant loss with a non-collapse term results in non-trivial representations, without requiring invariance to data augmentations.</p> <p>Encouraging sensitivity is especially important in contrastive learning, as it is known to learn shortcuts that forget features that are not needed to solve the pretraining task</p> <p>As noted by Wang and Isola [2020], the contrastive training mechanism balances invariance to augmentations with a competing objective: uniformly distributing embeddings over the sphere, which rules out trivial solutions such as constant functions.</p>","tags":["paper","equivariance","ssl","contrastive_learning"]},{"location":"100%20Reference%20notes/101%20Literature/Surgical%20Fine-Tuning%20Improves%20Adaptation%20to%20Distribution%20Shifts/","title":"Surgical Fine Tuning Improves Adaptation to Distribution Shifts","text":"Properties authors Yoonho Lee, Annie S. Chen, Fahim Tajwar, Huaxiu Yao, Percy Liang, Chelsea Finn, Ananya Kumar year 2022 url https://arxiv.org/abs/2210.11466 <p>Abstract</p> <p>A common approach to transfer learning under distribution shift is to fine-tune the last few layers of a pre-trained model, preserving learned features while also adapting to the new task. This paper shows that in such settings, selectively fine-tuning a subset of layers (which we term surgical fine-tuning) matches or outperforms commonly used fine-tuning approaches. Moreover, the type of distribution shift influences which subset is more effective to tune: for example, for image corruptions, fine-tuning only the first few layers works best. We validate our findings systematically across seven real-world data tasks spanning three types of distribution shifts. Theoretically, we prove that for two-layer neural networks in an idealized setting, first-layer tuning can outperform fine-tuning all layers. Intuitively, fine-tuning more parameters on a small target dataset can cause information learned during pre-training to be forgotten, and the relevant information depends on the type of shift.</p> <p>Notes: - Paper mentions that it depends on what kind of distribution shift the choice of layers (subset of parameters) to finetune. - They provide an automatic procedure to select those layers that beats full finetuning but is suboptimal when compared to expert/surgical finetuning. Suggest future work in this regard.</p>","tags":["paper","peft","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/Surgical-DINO%20-%20Adapter%20Learning%20of%20Foundation%20Models%20for%20Depth%20Estimation%20in%20Endoscopic%20Surgery/","title":"Surgical DINO   Adapter Learning of Foundation Models for Depth Estimation in Endoscopic Surgery","text":"Properties authors Beilei Cui, Mobarakol Islam, Long Bai, Hongliang Ren year 2024 url https://arxiv.org/abs/2401.06013 <p>Abstract</p> <p>Purpose: Depth estimation in robotic surgery is vital in 3D reconstruction, surgical navigation and augmented reality visualization. Although the foundation model exhibits outstanding performance in many vision tasks, including depth estimation (e.g., DINOv2), recent works observed its limitations in medical and surgical domain-specific applications. This work presents a low-ranked adaptation (LoRA) of the foundation model for surgical depth estimation. Methods: We design a foundation model-based depth estimation method, referred to as Surgical-DINO, a low-rank adaptation of the DINOv2 for depth estimation in endoscopic surgery. We build LoRA layers and integrate them into DINO to adapt with surgery-specific domain knowledge instead of conventional fine-tuning. During training, we freeze the DINO image encoder, which shows excellent visual representation capacity, and only optimize the LoRA layers and depth decoder to integrate features from the surgical scene. Results: Our model is extensively validated on a MICCAI challenge dataset of SCARED, which is collected from da Vinci Xi endoscope surgery. We empirically show that Surgical-DINO significantly outperforms all the state-of-the-art models in endoscopic depth estimation tasks. The analysis with ablation studies has shown evidence of the remarkable effect of our LoRA layers and adaptation. Conclusion: Surgical-DINO shed some light on the successful adaptation of the foundation models into the surgical domain for depth estimation. There is clear evidence in the results that zero-shot prediction on pre-trained weights in computer vision datasets or naive fine-tuning is not sufficient to use the foundation model in the surgical domain directly. Code is available at\u00a0this https URL.</p> <p>References: - LoRA - Low-Rank Adaptation of Large Language Models</p> <p>Keywords: - LoRa Adapter</p>","tags":["paper","efficient_dl","efficient_vision","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/Symmetries%20in%20Overparametrized%20Neural%20Networks%20-%20A%20Mean-Field%20View/","title":"Symmetries in Overparametrized Neural Networks   A Mean Field View","text":"Properties authors Javier Maass Martinez, Joaquin Fontbona year 2024 url https://arxiv.org/abs/2405.19995 <p>Abstract</p> <p>We develop a Mean-Field (MF) view of the learning dynamics of overparametrized Artificial Neural Networks (NN) under data symmetric in law wrt the action of a general compact group\u00a0G. We consider for this a class of generalized shallow NNs given by an ensemble of\u00a0N\u00a0multi-layer units, jointly trained using stochastic gradient descent (SGD) and possibly symmetry-leveraging (SL) techniques, such as Data Augmentation (DA), Feature Averaging (FA) or Equivariant Architectures (EA). We introduce the notions of weakly and strongly invariant laws (WI and SI) on the parameter space of each single unit, corresponding, respectively, to\u00a0G-invariant distributions, and to distributions supported on parameters fixed by the group action (which encode EA). This allows us to define symmetric models compatible with taking\u00a0N\u2192\u221e\u00a0and give an interpretation of the asymptotic dynamics of DA, FA and EA in terms of Wasserstein Gradient Flows describing their MF limits. When activations respect the group action, we show that, for symmetric data, DA, FA and freely-trained models obey the exact same MF dynamic, which stays in the space of WI laws and minimizes therein the population risk. We also give a counterexample to the general attainability of an optimum over SI laws. Despite this, quite remarkably, we show that the set of SI laws is also preserved by the MF dynamics even when freely trained. This sharply contrasts the finite-N\u00a0setting, in which EAs are generally not preserved by unconstrained SGD. We illustrate the validity of our findings as\u00a0N\u00a0gets larger in a teacher-student experimental setting, training a student NN to learn from a WI, SI or arbitrary teacher model through various SL schemes. We last deduce a data-driven heuristic to discover the largest subspace of parameters supporting SI distributions for a problem, that could be used for designing EA with minimal generalization error.</p>","tags":["dl_theory","equivariance"]},{"location":"100%20Reference%20notes/101%20Literature/Talaria%20-%20Interactively%20Optimizing%20Machine%20Learning%20Models%20for%20Efficient%20Inference/","title":"Talaria   Interactively Optimizing Machine Learning Models for Efficient Inference","text":"Properties authors Fred Hohman, Chaoqun Wang, Jinmook Lee, Jochen G\u00f6rtler, Dominik Moritz, Jeffrey P Bigham, Zhile Ren, Cecile Foret, Qi Shan, Ziaoyi Zhang year 2024 url https://arxiv.org/abs/2404.03085 <p>Abstract</p> <p>On-device machine learning (ML) moves computation from the cloud to personal devices, protecting user privacy and enabling intelligent user experiences. However, fitting models on devices with limited resources presents a major technical challenge: practitioners need to optimize models and balance hardware metrics such as model size, latency, and power. To help practitioners create efficient ML models, we designed and developed Talaria: a model visualization and optimization system. Talaria enables practitioners to compile models to hardware, interactively visualize model statistics, and simulate optimizations to test the impact on inference metrics. Since its internal deployment two years ago, we have evaluated Talaria using three methodologies: (1) a log analysis highlighting its growth of 800+ practitioners submitting 3,600+ models; (2) a usability survey with 26 users assessing the utility of 20 Talaria features; and (3) a qualitative interview with the 7 most active users about their experience using Talaria.</p>","tags":["efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/The%20Empirical%20Impact%20of%20Neural%20Parameter%20Symmetries%2C%20or%20Lack%20Thereof/","title":"The Empirical Impact of Neural Parameter Symmetries, or Lack Thereof","text":"Properties authors Derek Lim, Moe Putterman, Robin Walters, Haggai Maron, Stefanie Jegelka year 2024 url https://arxiv.org/abs/2405.20231 <p>Abstract</p> <p>Many algorithms and observed phenomena in deep learning appear to be affected by parameter symmetries -- transformations of neural network parameters that do not change the underlying neural network function. These include linear mode connectivity, model merging, Bayesian neural network inference, metanetworks, and several other characteristics of optimization or loss-landscapes. However, theoretical analysis of the relationship between parameter space symmetries and these phenomena is difficult. In this work, we empirically investigate the impact of neural parameter symmetries by introducing new neural network architectures that have reduced parameter space symmetries. We develop two methods, with some provable guarantees, of modifying standard neural networks to reduce parameter space symmetries. With these new methods, we conduct a comprehensive experimental study consisting of multiple tasks aimed at assessing the effect of removing parameter symmetries. Our experiments reveal several interesting observations on the empirical impact of parameter symmetries; for instance, we observe linear mode connectivity between our networks without alignment of weight spaces, and we find that our networks allow for faster and more effective Bayesian neural network training.</p>","tags":["equivariance","relaxed_equivariance","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/The%20Forward-Forward%20Algorithm%20-%20Some%20Preliminary%20Investigations/","title":"The Forward Forward Algorithm   Some Preliminary Investigations","text":"Properties authors Geoffrey Hinton year 2022 url http://arxiv.org/abs/2212.13345 <p>Abstract</p> <p>The aim of this paper is to introduce a new learning procedure for neural networks and to demonstrate that it works well enough on a few small problems to be worth further investigation. The Forward-Forward algorithm replaces the forward and backward passes of backpropagation by two forward passes, one with positive (i.e. real) data and the other with negative data which could be generated by the network itself. Each layer has its own objective function which is simply to have high goodness for positive data and low goodness for negative data. The sum of the squared activities in a layer can be used as the goodness but there are many other possibilities, including minus the sum of the squared activities. If the positive and negative passes could be separated in time, the negative passes could be done of\ufb02ine, which would make the learning much simpler in the positive pass and allow video to be pipelined through the network without ever storing activities or stopping to propagate derivatives.</p>","tags":["paper","dl_theory","optimization"]},{"location":"100%20Reference%20notes/101%20Literature/The%20Forward-Forward%20Algorithm%20-%20Some%20Preliminary%20Investigations/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","dl_theory","optimization"]},{"location":"100%20Reference%20notes/101%20Literature/The%20Hidden%20Uniform%20Cluster%20Prior%20in%20Self-Supervised%20Learning/","title":"The Hidden Uniform Cluster Prior in Self Supervised Learning","text":"Properties authors Mahmoud Assran, Randall Balestriero, Quentin Duval, Florian Bordes, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Nicolas Ballas year 2022 url http://arxiv.org/abs/2210.07277 <p>Abstract</p> <p>A successful paradigm in representation learning is to perform self-supervised pretraining using tasks based on mini-batch statistics (e.g., SimCLR, VICReg, SwAV, MSN). We show that in the formulation of all these methods is an overlooked prior to learn features that enable uniform clustering of the data. While this prior has led to remarkably semantic representations when pretraining on class-balanced data, such as ImageNet, we demonstrate that it can hamper performance when pretraining on class-imbalanced data. By moving away from conventional uniformity priors and instead preferring power-law distributed feature clusters, we show that one can improve the quality of the learned representations on real-world class-imbalanced datasets. To demonstrate this, we develop an extension of the Masked Siamese Networks (MSN) method to support the use of arbitrary features priors.</p>","tags":["paper","dl_theory","ssl"]},{"location":"100%20Reference%20notes/101%20Literature/The%20Hidden%20Uniform%20Cluster%20Prior%20in%20Self-Supervised%20Learning/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","dl_theory","ssl"]},{"location":"100%20Reference%20notes/101%20Literature/The%20Lie%20derivative%20for%20measuring%20learned%20equivariance/","title":"The Lie derivative for measuring learned equivariance","text":"Properties authors Nate Gruver, Marc Finzi, Micah Goldblum, Andrew Gordon Wilson year 2022 url https://arxiv.org/abs/2210.02984 <p>Abstract</p> <p>The Lie derivative is introduced, a method for measuring equivariance with strong mathematical foundations and minimal hyperparameters that finds that transformers can be more equivariant than convolutional neural networks after training, and that as models get larger and more accurate they tend to display more equivariance, regardless of architecture.</p>","tags":["equivariance"]},{"location":"100%20Reference%20notes/101%20Literature/The%20Lie%20derivative%20for%20measuring%20learned%20equivariance/#notes","title":"Notes","text":"","tags":["equivariance"]},{"location":"100%20Reference%20notes/101%20Literature/The%20Unreasonable%20Ineffectiveness%20of%20the%20Deeper%20Layers/","title":"The Unreasonable Ineffectiveness of the Deeper Layers","text":"Properties authors Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, Daniel A. Roberts year 2024 url https://arxiv.org/abs/2403.17887 <p>Abstract</p> <p>We empirically study a simple layer-pruning strategy for popular families of open-weight pretrained LLMs, finding minimal degradation of performance on different question-answering benchmarks until after a large fraction (up to half) of the layers are removed. To prune these models, we identify the optimal block of layers to prune by considering similarity across layers; then, to \"heal\" the damage, we perform a small amount of finetuning. In particular, we use parameter-efficient finetuning (PEFT) methods, specifically quantization and Low Rank Adapters (QLoRA), such that each of our experiments can be performed on a single A100 GPU. From a practical perspective, these results suggest that layer pruning methods can complement other PEFT strategies to further reduce computational resources of finetuning on the one hand, and can improve the memory and latency of inference on the other hand. From a scientific perspective, the robustness of these LLMs to the deletion of layers implies either that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge.</p>","tags":["transformers","efficient_dl","pruning","quantization"]},{"location":"100%20Reference%20notes/101%20Literature/Three%20things%20everyone%20should%20know%20about%20Vision%20Transformers/","title":"Three things everyone should know about Vision Transformers","text":"Properties authors Hugo Touvron, Matthieu Cord, Alaaeldin El-Nouby, Jakob Verbeek, Herv\u00e9 Jegou year 2022 url https://arxiv.org/abs/2203.09795 <p>Abstract</p> <p>After their initial success in natural language processing, transformer architectures have rapidly gained traction in computer vision, providing state-of-the-art results for tasks such as image classification, detection, segmentation, and video analysis. We offer three insights based on simple and easy to implement variants of vision transformers. (1) The residual layers of vision transformers, which are usually processed sequentially, can to some extent be processed efficiently in parallel without noticeably affecting the accuracy. (2) Fine-tuning the weights of the attention layers is sufficient to adapt vision transformers to a higher resolution and to other classification tasks. This saves compute, reduces the peak memory consumption at fine-tuning time, and allows sharing the majority of weights across tasks. (3) Adding MLP-based patch pre-processing layers improves Bert-like self-supervised training based on patch masking. We evaluate the impact of these design choices using the ImageNet-1k dataset, and confirm our findings on the ImageNet-v2 test set. Transfer performance is measured across six smaller datasets.</p>","tags":["paper","computer_vision","vit"]},{"location":"100%20Reference%20notes/101%20Literature/Three%20things%20everyone%20should%20know%20about%20Vision%20Transformers/#notes","title":"Notes","text":"","tags":["paper","computer_vision","vit"]},{"location":"100%20Reference%20notes/101%20Literature/TiC-CLIP%20-%20Continual%20Training%20of%20CLIP%20models/","title":"TiC CLIP   Continual Training of CLIP models","text":"Properties authors Saurabh Garg, Mehrdad Farajtabar, Hadi Pouransari, Raviteja Vemulapalli, Sachin Mehta, Oncel Tuzel, Vaishaal Shankar, Fartash Faghri year 2024 url https://arxiv.org/abs/2310.16226 <p>Abstract</p> <p>Keeping large foundation models up to date on latest data is inherently expensive. To avoid the prohibitive costs of constantly retraining, it is imperative to continually train these models. This problem is exacerbated by the lack of any large scale continual learning benchmarks or baselines. We introduce the first set of web-scale Time-Continual (TiC) benchmarks for training vision-language models: TiC-DataComp, TiC-YFCC, and TiC-Redcaps. TiC-DataComp, our largest dataset, contains over 12.7B timestamped image-text pairs spanning 9 years (2014-2022). We first use our benchmarks to curate various dynamic evaluations to measure temporal robustness of existing models. We show OpenAI's CLIP (trained on data up to 2020) loses\u00a0\u22488%\u00a0zero-shot accuracy on our curated retrieval task from 2021-2022 compared with more recently trained models in OpenCLIP repository. We then study how to efficiently train models on time-continuous data. We demonstrate that a simple rehearsal-based approach that continues training from the last checkpoint and replays old data reduces compute by\u00a02.5\u00d7\u00a0when compared to the standard practice of retraining from scratch. Code is available at\u00a0this https URL.</p>","tags":["paper","continual_learning","multimodal"]},{"location":"100%20Reference%20notes/101%20Literature/To%20Compress%20or%20Not%20to%20Compress-%20Self-Supervised%20Learning%20and%20Information%20Theory%20-%20A%20Review/","title":"To Compress or Not to Compress  Self Supervised Learning and Information Theory   A Review","text":"Properties authors Ravid Shwartz-Ziv, Yann LeCun year 2023 url http://arxiv.org/abs/2304.09355 <p>Abstract</p> <p>Deep neural networks excel in supervised learning tasks but are constrained by the need for extensive labeled data. Self-supervised learning emerges as a promising alternative, allowing models to learn without explicit labels. Information theory, and notably the information bottleneck principle, has been pivotal in shaping deep neural networks. This principle focuses on optimizing the trade-off between compression and preserving relevant information, providing a foundation for efficient network design in supervised contexts. However, its precise role and adaptation in self-supervised learning remain unclear. In this work, we scrutinize various self-supervised learning approaches from an informationtheoretic perspective, introducing a unified framework that encapsulates the self-supervised information-theoretic learning problem. We weave together existing research into a cohesive narrative, delve into contemporary self-supervised methodologies, and spotlight potential research avenues and inherent challenges. Additionally, we discuss the empirical evaluation of information-theoretic quantities and their estimation methods. Overall, this paper furnishes an exhaustive review of the intersection of information theory, self-supervised learning, and deep neural networks.</p>","tags":["paper","information_theory","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/To%20Compress%20or%20Not%20to%20Compress-%20Self-Supervised%20Learning%20and%20Information%20Theory%20-%20A%20Review/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","information_theory","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Toward%20a%20Geometrical%20Understanding%20of%20Self-supervised%20Contrastive%20Learning/","title":"Toward a Geometrical Understanding of Self supervised Contrastive Learning","text":"Properties authors Romain Cosentino, Anirvan Sengupta, Salman Avestimehr, Mahdi Soltanolkotabi, Antonio Ortega, Ted Willke, Mariano Tepper year 2022 url http://arxiv.org/abs/2205.06926 <p>Abstract</p> <p>Self-supervised learning (SSL) is currently one of the premier techniques to create data representations that are actionable for transfer learning in the absence of human annotations. Despite their success, the underlying geometry of these representations remains elusive, which obfuscates the quest for more robust, trustworthy, and interpretable models. In particular, mainstream SSL techniques rely on a speci\ufb01c deep neural network architecture with two cascaded neural networks: the encoder and the projector. When used for transfer learning, the projector is discarded since empirical results show that its representation generalizes more poorly than the encoder\u2019s. In this paper, we investigate the representation induced by the encoder and how the strength of the data augmentation policies as well as the width and depth of the projector a\ufb00ect its representation. We discover a non-trivial relation between the encoder, the projector, and the data augmentation strength: with increasingly larger augmentation policies, the projector, rather than the encoder, is more strongly driven to become invariant to the augmentations. It does so by eliminating crucial information about the data by learning to project it into a low-dimensional space, a noisy estimate of the data manifold tangent plane in the encoder representation. This analysis is substantiated through a geometrical perspective with theoretical and empirical results.</p>","tags":["paper","equivariance","dl_theory","ssl"]},{"location":"100%20Reference%20notes/101%20Literature/Toward%20a%20Geometrical%20Understanding%20of%20Self-supervised%20Contrastive%20Learning/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","equivariance","dl_theory","ssl"]},{"location":"100%20Reference%20notes/101%20Literature/Towards%20Understanding%20the%20Spectral%20Bias%20of%20Deep%20Learning/","title":"Towards Understanding the Spectral Bias of Deep Learning","text":"Properties authors Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, Quanquan Gu year 2020 url http://arxiv.org/abs/1912.01198 <p>Abstract</p> <p>An intriguing phenomenon observed during training neural networks is the spectral bias, which states that neural networks are biased towards learning less complex functions. The priority of learning functions with low complexity might be at the core of explaining generalization ability of neural network, and certain e\ufb00orts have been made to provide theoretical explanation for spectral bias. However, there is still no satisfying theoretical result justifying the underlying mechanism of spectral bias. In this paper, we give a comprehensive and rigorous explanation for spectral bias and relate it with the neural tangent kernel function proposed in recent work. We prove that the training process of neural networks can be decomposed along di\ufb00erent directions de\ufb01ned by the eigenfunctions of the neural tangent kernel, where each direction has its own convergence rate and the rate is determined by the corresponding eigenvalue. We then provide a case study when the input data is uniformly distributed over the unit sphere, and show that lower degree spherical harmonics are easier to be learned by over-parameterized neural networks. Finally, we provide numerical experiments to demonstrate the correctness of our theory. Our experimental results also show that our theory can tolerate certain model misspeci\ufb01cation in terms of the input data distribution.</p>","tags":["paper","dl_theory","spectral"]},{"location":"100%20Reference%20notes/101%20Literature/Towards%20Understanding%20the%20Spectral%20Bias%20of%20Deep%20Learning/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","dl_theory","spectral"]},{"location":"100%20Reference%20notes/101%20Literature/Towards%20a%20Definition%20of%20Disentangled%20Representations/","title":"Towards a Definition of Disentangled Representations","text":"Properties authors Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, Alexander Lerchner year 2018 url http://arxiv.org/abs/1812.02230 <p>Abstract</p> <p>How can intelligent agents solve a diverse set of tasks in a data-efficient manner? The disentangled representation learning approach posits that such an agent would benefit from separating out (disentangling) the underlying structure of the world into disjoint parts of its representation. However, there is no generally agreed-upon definition of disentangling, not least because it is unclear how to formalise the notion of world structure beyond toy datasets with a known ground truth generative process. Here we propose that a principled solution to characterising disentangled representations can be found by focusing on the transformation properties of the world. In particular, we suggest that those transformations that change only some properties of the underlying world state, while leaving all other properties invariant, are what gives exploitable structure to any kind of data. Similar ideas have already been successfully applied in physics, where the study of symmetry transformations has revolutionised the understanding of the world structure. By connecting symmetry transformations to vector representations using the formalism of group and representation theory we arrive at the first formal definition of disentangled representations. Our new definition is in agreement with many of the current intuitions about disentangling, while also providing principled resolutions to a number of previous points of contention. While this work focuses on formally defining disentangling - as opposed to solving the learning problem - we believe that the shift in perspective to studying data transformations can stimulate the development of better representation learning algorithms.</p>","tags":["paper","disentanglement","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Towards%20a%20Definition%20of%20Disentangled%20Representations/#notes","title":"Notes","text":"<p>Zotero Link</p> <p>focusing on the transformation properties of the world</p> <p>these approaches can be used to define a set of constraints on the decomposition of a vector space into independent subspaces to ensure that the vector space is reflective of the underlying structure of the corresponding symmetry group.</p> <p>However, it is important to note that we define disentangled representations with respect to a particular decomposition of a symmetry group into subgroups. This aspect of our definition will be discussed in more detail later in this section.</p> <p>A representation disentangled by the definition above will have the compositional property often found to be desirable.</p> <p>This is the basis of the theory of actionable information, which holds that a complete theory of visual perception needs to treat image acquisition as an active process of information gathering [69].</p> <p>To date, most work on disentangling has relied on comparison with human intuition for evaluation, though a number of metrics have been proposed in the case where the ground truth factors are known [27, 65, 51, 47, 55, 72].</p> <p>Explicitness Explicitness measures whether the values of all of the data generative factors can be decoded from the representation using a linear transformation. This is the strongest requirement of the three, since it encompasses two points: that a disentangled representation captures information about all the data generative factors, and that this information is linearly decodable.</p>","tags":["paper","disentanglement","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Training%20quantized%20nets%20-%20A%20deeper%20understanding/","title":"Training quantized nets   A deeper understanding","text":"Properties authors Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, Tom Goldstein year 2017 url https://arxiv.org/abs/1706.02379 <p>Abstract</p> <p>Currently, deep neural networks are deployed on low-power portable devices by first training a full-precision model using powerful hardware, and then deriving a corresponding low-precision model for efficient inference on such systems. However, training models directly with coarsely quantized weights is a key step towards learning on embedded platforms that have limited computing resources, memory capacity, and power consumption. Numerous recent publications have studied methods for training quantized networks, but these studies have mostly been empirical. In this work, we investigate training methods for quantized neural networks from a theoretical viewpoint. We first explore accuracy guarantees for training methods under convexity assumptions. We then look at the behavior of these algorithms for non-convex problems, and show that training algorithms that exploit high-precision representations have an important greedy search phase that purely quantized training methods lack, which explains the difficulty of training using low-precision arithmetic.</p>","tags":["paper","quantization"]},{"location":"100%20Reference%20notes/101%20Literature/Training%20quantized%20nets%20-%20A%20deeper%20understanding/#notes","title":"Notes","text":"<ul> <li> Read paper</li> </ul>","tags":["paper","quantization"]},{"location":"100%20Reference%20notes/101%20Literature/Understanding%20Deep%20Learning%20-%20Chapter%2010/","title":"Understanding Deep Learning   Chapter 10","text":"Properties authors Simon J.D. Prince year 2023 url https://udlbook.github.io/udlbook/","tags":["textbook","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Understanding%20Deep%20Learning%20-%20Chapter%2020/","title":"Understanding Deep Learning   Chapter 20","text":"Properties authors Simon J.D. Prince year 2023 url https://udlbook.github.io/udlbook/","tags":["textbook","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Understanding%20Deep%20Learning%20-%20Chapter%2020/#chapter-20-why-does-deep-learning-work","title":"Chapter 20: Why does deep learning work?","text":"<p>Contents</p> <ul> <li>20.1 The case against deep learning </li> <li>20.2 Factors that influence fitting performance </li> <li>20.3 Properties of loss functions</li> <li>20.4 Factors that determine generalization</li> <li>20.5 Do we need so many parameters? </li> <li>20.6 Do networks have to be deep?</li> <li>20.7 Summary</li> </ul>","tags":["textbook","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Understanding%20symmetries%20in%20deep%20networks/","title":"Understanding symmetries in deep networks","text":"Properties authors Vijay Badrinarayanan, Bamdev Mishra, Roberto Cipolla year 2015 url https://arxiv.org/abs/1511.01029 <p>Abstract</p> <p>Recent works have highlighted scale invariance or symmetry present in the weight space of a typical deep network and the adverse effect it has on the Euclidean gradient based stochastic gradient descent optimization. In this work, we show that a commonly used deep network, which uses convolution, batch normalization, reLU, max-pooling, and sub-sampling pipeline, possess more complex forms of symmetry arising from scaling-based reparameterization of the network weights. We propose to tackle the issue of the weight space symmetry by constraining the filters to lie on the unit-norm manifold. Consequently, training the network boils down to using stochastic gradient descent updates on the unit-norm manifold. Our empirical evidence based on the MNIST dataset shows that the proposed updates improve the test performance beyond what is achieved with batch normalization and without sacrificing the computational efficiency of the weight updates.</p> <p>,</p>","tags":["dl_theory","dl2"]},{"location":"100%20Reference%20notes/101%20Literature/Unlocking%20Slot%20Attention%20by%20Changing%20Optimal%20Transport%20Costs/","title":"Unlocking Slot Attention by Changing Optimal Transport Costs","text":"Properties authors Yan Zhang, David W. Zhang, Simon Lacoste-Julien, Cees G. M. Snoek, Gertjan J. Burghouts year 2023 url http://arxiv.org/abs/2301.13197 <p>Abstract</p> <p>Slot attention is a powerful method for objectcentric modeling in images and videos. However, its set-equivariance limits its ability to handle videos with a dynamic number of objects because it cannot break ties. To overcome this limitation, we first establish a connection between slot attention and optimal transport. Based on this new perspective we propose MESH (Minimize Entropy of Sinkhorn): a cross-attention module that combines the tiebreaking properties of unregularized optimal transport with the speed of regularized optimal transport. We evaluate slot attention using MESH on multiple object-centric learning benchmarks and find significant improvements over slot attention in every setting.</p>","tags":["paper","computer_vision","video"]},{"location":"100%20Reference%20notes/101%20Literature/Unlocking%20Slot%20Attention%20by%20Changing%20Optimal%20Transport%20Costs/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","computer_vision","video"]},{"location":"100%20Reference%20notes/101%20Literature/Unsupervised%20Learning%20of%20Visual%20Features%20by%20Contrasting%20Cluster%20Assignments/","title":"Unsupervised Learning of Visual Features by Contrasting Cluster Assignments","text":"Properties authors Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, Armand Joulin year 2021 url http://arxiv.org/abs/2006.09882 <p>Abstract</p> <p>Unsupervised image representations have signi\ufb01cantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Speci\ufb01cally, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or \u201cviews\u201d) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a \u201cswapped\u201d prediction mechanism where we predict the code of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory ef\ufb01cient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements. We validate our \ufb01ndings by achieving 75.3% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.</p>","tags":["paper","vit"]},{"location":"100%20Reference%20notes/101%20Literature/Unsupervised%20Learning%20of%20Visual%20Features%20by%20Contrasting%20Cluster%20Assignments/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","vit"]},{"location":"100%20Reference%20notes/101%20Literature/Unsupervised%20Visual%20Representation%20Learning%20by%20Context%20Prediction/","title":"Unsupervised Visual Representation Learning by Context Prediction","text":"Properties authors Carl Doersch, Abhinav Gupta, Alexei A. Efros year 2016 url https://arxiv.org/abs/1505.05192 <p>Abstract</p> <p>This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the R-CNN framework and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.</p>","tags":["paper","thesis","ssl"]},{"location":"100%20Reference%20notes/101%20Literature/Unsupervised%20Visual%20Representation%20Learning%20by%20Context%20Prediction/#notes","title":"Notes","text":"<p>{ width=\"400\" }</p> <p>{ width=\"400\" }</p>","tags":["paper","thesis","ssl"]},{"location":"100%20Reference%20notes/101%20Literature/Using%20Degeneracy%20in%20the%20Loss%20Landscape%20for%20Mechanistic%20Interpretability/","title":"Using Degeneracy in the Loss Landscape for Mechanistic Interpretability","text":"Properties authors Lucius Bushnaq, Jake Mendel, Stefan Heimersheim, Dan Braun, Nicholas Goldowsky-Dill, Kaarel H\u00e4nni, Cindy Wu, Marius Hobbhahn year 2024 url https://arxiv.org/abs/2405.10927 <p>Abstract</p> <p>Mechanistic Interpretability aims to reverse engineer the algorithms implemented by neural networks by studying their weights and activations. An obstacle to reverse engineering neural networks is that many of the parameters inside a network are not involved in the computation being implemented by the network. These degenerate parameters may obfuscate internal structure. Singular learning theory teaches us that neural network parameterizations are biased towards being more degenerate, and parameterizations with more degeneracy are likely to generalize further. We identify 3 ways that network parameters can be degenerate: linear dependence between activations in a layer; linear dependence between gradients passed back to a layer; ReLUs which fire on the same subset of datapoints. We also present a heuristic argument that modular networks are likely to be more degenerate, and we develop a metric for identifying modules in a network that is based on this argument. We propose that if we can represent a neural network in a way that is invariant to reparameterizations that exploit the degeneracies, then this representation is likely to be more interpretable, and we provide some evidence that such a representation is likely to have sparser interactions. We introduce the Interaction Basis, a tractable technique to obtain a representation that is invariant to degeneracies from linear dependence of activations or Jacobians.</p>","tags":["paper","dl_theory","mechinterp","optimization"]},{"location":"100%20Reference%20notes/101%20Literature/VICRegL%20-%20Self-Supervised%20Learning%20of%20Local%20Visual%20Features/","title":"VICRegL   Self Supervised Learning of Local Visual Features","text":"Properties authors Adrien Bardes, Jean Ponce, Yann LeCun year 2022 url http://arxiv.org/abs/2210.01571 <p>Abstract</p> <p>Most recent self-supervised methods for learning image representations focus on either producing a global feature with invariance properties, or producing a set of local features. The former works best for classification tasks while the latter is best for detection and segmentation tasks. This paper explores the fundamental trade-off between learning local and global features. A new method called VICRegL is proposed that learns good global and local features simultaneously, yielding excellent performance on detection and segmentation tasks while maintaining good performance on classification tasks. Concretely, two identical branches of a standard convolutional net architecture are fed two differently distorted versions of the same image. The VICReg criterion is applied to pairs of global feature vectors. Simultaneously, the VICReg criterion is applied to pairs of local feature vectors occurring before the last pooling layer. Two local feature vectors are attracted to each other if their l2-distance is below a threshold or if their relative locations are consistent with a known geometric transformation between the two input images. We demonstrate strong performance on linear classification and segmentation transfer tasks. Code and pretrained models are publicly available at: https://github.com/facebookresearch/VICRegL</p>","tags":["paper","ssl","regularization","dense_ssl","vit"]},{"location":"100%20Reference%20notes/101%20Literature/VICRegL%20-%20Self-Supervised%20Learning%20of%20Local%20Visual%20Features/#notes","title":"Notes","text":"<p>Zotero Link</p> <p></p> <p>\u201cWe argue that more complex reasoning systems should be structured in a hierarchical way, by learning at several scales.\u201d (Bardes et al., 2022, p. 2)</p> <p>Interesting, a bit of pushback to the less-inductive-bias-better line of work.</p> <p>\u201cwe propose VICRegL, a method that learn features at a global scale, and that additionally uses spatial information, and matches feature vectors that are either pooled from close-by regions in the original input image, or close in the embedding space, therefore learning features at a local scale.\u201d (Bardes et al., 2022, p. 2)</p> <p>I understand the close-in-input-space but why would you want to pull features already close in the embedding space?  </p> <p>I think the word \u201cpull\u201d might be carrying a bit more meaning.</p> <p>\u201cIn practice, the global VICReg criterion [Bardes et al., 2022] is applied to pairs of feature vectors, before and after the final pooling layer of a convolutional network, thus learning local and global features at the same time.\u201d (Bardes et al., 2022, p. 2)</p> <p>That seems computationally expensive.</p> <p>Okay, the subsample top y pairs</p> <p>\u201cStudy of the importance between feature-based and location-based local criteria. VICRegL matches feature vectors according to a location-based criterion Ls of Eq. (2) and a feature-based criterion Ld of Eq. (3). Table 3 study the importance of these criterion. Baseline in the table means that no local criterion is used, and is simply VICReg. The location-based criterion gives the best improvement by +2.9 mIoU over the baseline, compared to only +1.7 mIoU for the feature-based criterion, but it is the combination of the two that significantly improves over the baseline by +4.3 mIoU, which shows that using both the learned distance in the embedding space in combination with the actual distance in the pixel space produces local features with the best quality\u201d (Bardes et al., 2022, p. 9)</p> <p>I'm still confused as to why the make-close-features-close objective should help, it seems another reason to collapse.</p> <p>\u201cWe report in Table 11, the running time of VICRegL in comparison with VICReg, for pretraining a ResNet-50 backbone with a batch size of 2048. The introduction of the local criterion comes with an additional computation overhead, mainly do to computing the covariance matrices of every feature vector in the output feature maps. These computations induces a moderate additional computational burden both in terms of time and memory usage.\u201d (Bardes et al., 2022, p. 17)</p> <p></p>","tags":["paper","ssl","regularization","dense_ssl","vit"]},{"location":"100%20Reference%20notes/101%20Literature/Variance%20Covariance%20Regularization%20Enforces%20Pairwise%20Independence%20in%20Self-Supervised%20Representations/","title":"Variance Covariance Regularization Enforces Pairwise Independence in Self Supervised Representations","text":"Properties authors Gr\u00e9goire Mialon, Randall Balestriero, Yann LeCun year 2024 url http://arxiv.org/abs/2209.14905 <p>Abstract</p> <p>Self-Supervised Learning (SSL) methods such as VICReg, Barlow Twins or W-MSE avoid collapse of their joint embedding architectures by constraining or regularizing the covariance matrix of their projector\u2019s output. This study highlights important properties of such strategy, which we coin Variance-Covariance regularization (VCReg). More precisely, we show that VCReg combined to a MLP projector enforces pairwise independence between the features of the learned representation. This result emerges by bridging VCReg applied on the projector\u2019s output to kernel independence criteria applied on the projector\u2019s input. We empirically validate our findings where (i) we put in evidence which projector\u2019s characteristics favor pairwise independence, (ii) we demonstrate pairwise independence to be beneficial for out-of-domain generalization, (iii) we demonstrate that the scope of VCReg goes beyond SSL by using it to solve Independent Component Analysis. This provides the first theoretical motivation and explanation of MLP projectors in SSL.</p>","tags":["paper","dl_theory","ssl","regularization","variance","representation_learning"]},{"location":"100%20Reference%20notes/101%20Literature/Variance%20Covariance%20Regularization%20Enforces%20Pairwise%20Independence%20in%20Self-Supervised%20Representations/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","dl_theory","ssl","regularization","variance","representation_learning"]},{"location":"100%20Reference%20notes/101%20Literature/ViDT%20-%20An%20Efficient%20and%20Effective%20Fully%20Transformer-based%20Object%20Detector/","title":"ViDT   An Efficient and Effective Fully Transformer based Object Detector","text":"Properties authors Hwanjun Song, Deqing Sun, Sanghyuk Chun, Varun Jampani, Dongyoon Han, Byeongho Heo, Wonjae Kim, Ming-Hsuan Yang year 2021 url https://arxiv.org/abs/2110.03921 <p>Abstract</p> <p>Transformers are transforming the landscape of computer vision, especially for recognition tasks. Detection transformers are the first fully end-to-end learning systems for object detection, while vision transformers are the first fully transformer-based architecture for image classification. In this paper, we integrate Vision and Detection Transformers (ViDT) to build an effective and efficient object detector. ViDT introduces a reconfigured attention module to extend the recent Swin Transformer to be a standalone object detector, followed by a computationally efficient transformer decoder that exploits multi-scale features and auxiliary techniques essential to boost the detection performance without much increase in computational load. Extensive evaluation results on the Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP and latency trade-off among existing fully transformer-based object detectors, and achieves 49.2AP owing to its high scalability for large models. We will release the code and trained models at\u00a0this https URL</p>","tags":["paper","object_detection","vit","computer_vision"]},{"location":"100%20Reference%20notes/101%20Literature/ViTAR%20-%20Vision%20Transformer%20with%20Any%20Resolution/","title":"ViTAR   Vision Transformer with Any Resolution","text":"Properties authors Qihang Fan, Quanzeng You, Xiaotian Han, Yongfei Liu, Yunzhe Tao, Huaibo Huang, Ran He, Hongxia Yang year 2024 url http://arxiv.org/abs/2403.18361 <p>Abstract</p> <p>This paper tackles a significant challenge faced by Vision Transformers (ViTs): their constrained scalability across different image resolutions. Typically, ViTs experience a performance decline when processing resolutions different from those seen during training. Our work introduces two key innovations to address this issue. Firstly, we propose a novel module for dynamic resolution adjustment, designed with a single Transformer block, specifically to achieve highly efficient incremental token integration. Secondly, we introduce fuzzy positional encoding in the Vision Transformer to provide consistent positional awareness across multiple resolutions, thereby preventing overfitting to any single training resolution. Our resulting model, ViTAR (Vision Transformer with Any Resolution), demonstrates impressive adaptability, achieving 83.3% top-1 accuracy at a 1120x1120 resolution and 80.4% accuracy at a 4032x4032 resolution, all while reducing computational costs. ViTAR also shows strong performance in downstream tasks such as instance and semantic segmentation and can easily combined with self-supervised learning techniques like Masked AutoEncoder. Our work provides a cost-effective solution for enhancing the resolution scalability of ViTs, paving the way for more versatile and efficient high-resolution image processing.</p>","tags":["paper","vit","computer_vision","efficient_vision"]},{"location":"100%20Reference%20notes/101%20Literature/ViTAR%20-%20Vision%20Transformer%20with%20Any%20Resolution/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","vit","computer_vision","efficient_vision"]},{"location":"100%20Reference%20notes/101%20Literature/Vision%20Mamba%20-%20Efficient%20Visual%20Representation%20Learning%20with%20Bidirectional%20State%20Space%20Model/","title":"Vision Mamba   Efficient Visual Representation Learning with Bidirectional State Space Model","text":"Properties authors Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, Xinggang Wang year 2024 url https://arxiv.org/abs/2401.09417 <p>Abstract</p> <p>Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation &amp; memory efficiency. For example, Vim is 2.8\u00d7\u00a0faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248\u00d71248. The results demonstrate that Vim is capable of overcoming the computation &amp; memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models. Code is available at\u00a0this https URL.</p>","tags":["transformers","mamba","ssm","efficient_dl"]},{"location":"100%20Reference%20notes/101%20Literature/Vision%20Transformer%20with%20Deformable%20Attention/","title":"Vision Transformer with Deformable Attention","text":"Properties authors Zhuofan Xia, Xuran Pan, Shiji Song, Li Erran Li, Gao Huang year 2022 url http://arxiv.org/abs/2201.00520 <p>Abstract</p> <p>Transformers have recently shown superior performances on various vision tasks. The large, sometimes even global, receptive \ufb01eld endows Transformer models with higher representation power over their CNN counterparts. Nevertheless, simply enlarging receptive \ufb01eld also gives rise to several concerns. On the one hand, using dense attention e.g., in ViT, leads to excessive memory and computational cost, and features can be in\ufb02uenced by irrelevant parts which are beyond the region of interests. On the other hand, the sparse attention adopted in PVT or Swin Transformer is data agnostic and may limit the ability to model long range relations. To mitigate these issues, we propose a novel deformable self-attention module, where the positions of key and value pairs in self-attention are selected in a data-dependent way. This \ufb02exible scheme enables the self-attention module to focus on relevant regions and capture more informative features. On this basis, we present Deformable Attention Transformer, a general backbone model with deformable attention for both image classi\ufb01cation and dense prediction tasks. Extensive experiments show that our models achieve consistently improved results on comprehensive benchmarks. Code is available at https://github.com/LeapLabTHU/DAT.</p>","tags":["paper","transformers","computer_vision"]},{"location":"100%20Reference%20notes/101%20Literature/Vision%20Transformer%20with%20Deformable%20Attention/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","transformers","computer_vision"]},{"location":"100%20Reference%20notes/101%20Literature/Vision%20Transformers%20Need%20Registers/","title":"Vision Transformers Need Registers","text":"Properties authors Timoth\u00e9e Darcet, Maxime Oquab, Julien Mairal, Piotr Bojanowski year 2023 url https://arxiv.org/pdf/2309.16588 <p>Abstract</p> <p>Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing.</p>","tags":["paper","vit","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/Vision%20Transformers%20Need%20Registers/#note","title":"Note","text":"<ul> <li>note to myself:<ul> <li> Read paper in depth #personal \ud83d\udd3c </li> </ul> </li> </ul>","tags":["paper","vit","dl_theory"]},{"location":"100%20Reference%20notes/101%20Literature/What%20Do%20Self-Supervised%20Vision%20Transformers%20Learn%3F/","title":"What Do Self Supervised Vision Transformers Learn?","text":"Properties authors Namuk Park, Wonjae Kim, Byeongho Heo, Taekyung Kim, Sangdoo Yun year 2023 url https://arxiv.org/abs/2305.00729 <p>Abstract</p> <p>We present a comparative study on how and why contrastive learning (CL) and masked image modeling (MIM) differ in their representations and in their performance of downstream tasks. In particular, we demonstrate that self-supervised Vision Transformers (ViTs) have the following properties: (1) CL trains self-attentions to capture longer-range global patterns than MIM, such as the shape of an object, especially in the later layers of the ViT architecture. This CL property helps ViTs linearly separate images in their representation spaces. However, it also makes the self-attentions collapse into homogeneity for all query tokens and heads. Such homogeneity of self-attention reduces the diversity of representations, worsening scalability and dense prediction performance. (2) CL utilizes the low-frequency signals of the representations, but MIM utilizes high-frequencies. Since low- and high-frequency information respectively represent shapes and textures, CL is more shape-oriented and MIM more texture-oriented. (3) CL plays a crucial role in the later layers, while MIM mainly focuses on the early layers. Upon these analyses, we find that CL and MIM can complement each other and observe that even the simplest harmonization can help leverage the advantages of both methods. The code is available at\u00a0this https URL.</p>","tags":["paper","dl_theory","vit","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/What%20Do%20Self-Supervised%20Vision%20Transformers%20Learn%3F/#notes","title":"Notes","text":"","tags":["paper","dl_theory","vit","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/nGPT%20-%20Normalized%20Transformer%20with%20Representation%20Learning%20on%20the%20Hypersphere/","title":"nGPT   Normalized Transformer with Representation Learning on the Hypersphere","text":"Properties authors Ilya Loshchilov, Cheng-Ping Hsieh, Simeng Sun, Boris Ginsburg year 2025 url http://arxiv.org/abs/2410.01131 <p>Abstract</p> <p>We propose a novel neural network architecture, the normalized Transformer (nGPT) with representation learning on the hypersphere. In nGPT, all vectors forming the embeddings, MLP, attention matrices and hidden states are unit norm normalized. The input stream of tokens travels on the surface of a hypersphere, with each layer contributing a displacement towards the target output predictions. These displacements are de\ufb01ned by the MLP and attention blocks, whose vector components also reside on the same hypersphere. Experiments show that nGPT learns much faster, reducing the number of training steps required to achieve the same accuracy by a factor of 4 to 20, depending on the sequence length.</p>","tags":["paper","dl_theory","non_euclidean","transformers"]},{"location":"100%20Reference%20notes/101%20Literature/nGPT%20-%20Normalized%20Transformer%20with%20Representation%20Learning%20on%20the%20Hypersphere/#notes","title":"Notes","text":"<p>Zotero Link</p>","tags":["paper","dl_theory","non_euclidean","transformers"]},{"location":"100%20Reference%20notes/102%20Authors/Adrien%20Bardes/","title":"Adrien Bardes","text":"Properties affiliation FAIR, INRIA"},{"location":"100%20Reference%20notes/102%20Authors/Albert%20Gu/","title":"Albert Gu","text":"Properties affiliation Carnegie Mellon University"},{"location":"100%20Reference%20notes/102%20Authors/Alex%20Flinth/","title":"Alex Flinth","text":"Properties affiliation Umea University"},{"location":"100%20Reference%20notes/102%20Authors/Alex%20Vitvitskyi/","title":"Alex Vitvitskyi","text":"Properties affiliation Google DeepMind"},{"location":"100%20Reference%20notes/102%20Authors/Alexander%20Kirillov/","title":"Alexander Kirillov","text":"Properties affiliation OpenAI, FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Alexey%20Dosovitskiy/","title":"Alexey Dosovitskiy","text":"Properties affiliation Google"},{"location":"100%20Reference%20notes/102%20Authors/Amir%20Zamir/","title":"Amir Zamir","text":"Properties affiliation EPFL"},{"location":"100%20Reference%20notes/102%20Authors/Ananya%20Kumar/","title":"Ananya Kumar","text":"Properties affiliation Stanford"},{"location":"100%20Reference%20notes/102%20Authors/Andreas%20Loukas/","title":"Andreas Loukas","text":"Properties affiliation EPFL"},{"location":"100%20Reference%20notes/102%20Authors/Andreas%20Savakis/","title":"Andreas Savakis","text":"Properties affiliation Rochester Institute of Technology"},{"location":"100%20Reference%20notes/102%20Authors/Andrew%20Gordon%20Wilson/","title":"Andrew Gordon Wilson","text":"Properties affiliation New York University"},{"location":"100%20Reference%20notes/102%20Authors/Angela%20Fan/","title":"Angela Fan","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Annie%20S.%20Chen/","title":"Annie S. Chen","text":"Properties affiliation Stanford"},{"location":"100%20Reference%20notes/102%20Authors/Antonio%20Orvieto/","title":"Antonio Orvieto","text":"Properties affiliation Max Planck Institute for Intelligent Systems"},{"location":"100%20Reference%20notes/102%20Authors/Ardavan%20Pedram/","title":"Ardavan Pedram","text":"Properties affiliation Stanford, Samsung"},{"location":"100%20Reference%20notes/102%20Authors/Armand%20Joulin/","title":"Armand Joulin","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Attila%20Lengyel/","title":"Attila Lengyel","text":"Properties affiliation TU Delft"},{"location":"100%20Reference%20notes/102%20Authors/Binxu%20Wang/","title":"Binxu Wang","text":"Properties affiliation Harvard, Kempner Institute"},{"location":"100%20Reference%20notes/102%20Authors/Boris%20Ginsburg/","title":"Boris Ginsburg","text":"Properties affiliation NVIDIA"},{"location":"100%20Reference%20notes/102%20Authors/Boshi%20Wang/","title":"Boshi Wang","text":"Properties affiliation The Ohio State University"},{"location":"100%20Reference%20notes/102%20Authors/Byeongho%20Heo/","title":"Byeongho Heo","text":"Properties affiliation Naver AI Lab"},{"location":"100%20Reference%20notes/102%20Authors/Caglar%20Gulcehre/","title":"Caglar Gulcehre","text":"Properties affiliation CLAIRE, EPFL"},{"location":"100%20Reference%20notes/102%20Authors/Carmen%20Amo%20Alonso/","title":"Carmen Amo Alonso","text":"Properties affiliation ETH Zurich"},{"location":"100%20Reference%20notes/102%20Authors/Cees%20G.%20M.%20Snoek/","title":"Cees G. M. Snoek","text":"Properties affiliation University of Amsterdam, VIS Lab File year tags authors MoSiC - Optimal-Transport Motion Trajectory for Dense Self-Supervised Learning 2025 <ul><li>paper</li><li>ssl</li><li>dense_ssl</li><li>computer_vision</li></ul> <ul><li>Mohammadreza Salehi</li><li>Shashanka Venkataramanan</li><li>Ioana Simion</li><li>Efstratios Gavves</li><li>Cees G. M. Snoek</li><li>Yuki M. Asano</li></ul> NeoBabel - A Multilingual Open Tower for Visual Generation 2025 <ul><li>paper</li><li>multimodal</li><li>llm</li></ul> <ul><li>Mohammad Mahdi Derakhshani</li><li>Dheeraj Varghese</li><li>Marzieh Fadaee</li><li>Cees G. M. Snoek</li></ul> KV Cache Steering for Inducing Reasoning in Small Language Models 2025 <ul><li>paper</li><li>llm</li><li>efficient_dl</li><li>reasoning</li></ul> <ul><li>[[Max Belitsky|Max Belitsky]]</li><li>Dawid J. Kopiczko</li><li>Michael Dorkenwald</li><li>[[M. Jehanzeb Mirza|M. Jehanzeb Mirza]]</li><li>Cees G. M. Snoek</li><li>Yuki M. Asano</li></ul> Lost in Time - A New Temporal Benchmark for VideoLLMs 2025 <ul><li>paper</li><li>llm</li><li>video</li><li>temporal</li></ul> <ul><li>[[Daniel Cores|Daniel Cores]]</li><li>Michael Dorkenwald</li><li>[[Manuel Mucientes|Manuel Mucientes]]</li><li>Cees G. M. Snoek</li><li>Yuki M. Asano</li></ul> An Image is Worth More Than 16x16 Patches - Exploring Transformers on Individual Pixels 2024 <ul><li>paper</li><li>dl_theory</li><li>vit</li><li>computer_vision</li></ul> <ul><li>Duy-Kien Nguyen</li><li>Mahmoud Assran</li><li>[[Unnat Jain|Unnat Jain]]</li><li>Martin R. Oswald</li><li>Cees G. M. Snoek</li><li>Xinlei Chen</li></ul> SimPLR - A Simple and Plain Transformer for Scaling-Efficient Object Detection and Segmentation 2024 <ul><li>paper</li><li>object_detection</li><li>computer_vision</li><li>vit</li></ul> <ul><li>Duy-Kien Nguyen</li><li>Martin R. Oswald</li><li>Cees G. M. Snoek</li></ul> Low-Resource Vision Challenges for Foundation Models 2024 <ul><li>paper</li><li>efficient_dl</li><li>foundation_models</li><li>computer_vision</li></ul> <ul><li>[[Yunhua Zhang|Yunhua Zhang]]</li><li>[[Hazel Doughty|Hazel Doughty]]</li><li>Cees G. M. Snoek</li></ul> PIN - Positional Insert Unlocks Object Localisation Abilities in VLMs 2024 <ul><li>paper</li><li>multimodal</li><li>object_localisation</li></ul> <ul><li>Michael Dorkenwald</li><li>[[Nimrod Barazani|Nimrod Barazani]]</li><li>Cees G. M. Snoek</li><li>Yuki M. Asano</li></ul> R-MAE - Regions Meet Masked Autoencoders 2023 <ul><li>paper</li><li>foundation_models</li></ul> <ul><li>Duy-Kien Nguyen</li><li>Vaibhav Aggarwal</li><li>Yanghao Li</li><li>Martin R. Oswald</li><li>Alexander Kirillov</li><li>Cees G. M. Snoek</li><li>Xinlei Chen</li></ul> Learning Unseen Modality Interaction 2023 <ul><li>paper</li><li>multimodal</li><li>llm</li></ul> <ul><li>[[Yunhua Zhang|Yunhua Zhang]]</li><li>[[Hazel Doughty|Hazel Doughty]]</li><li>Cees G. M. Snoek</li></ul> Self-Guided Diffusion Models 2023 <ul><li>paper</li><li>computer_vision</li><li>diffusion</li></ul> <ul><li>[[Vincent Tao Hu|Vincent Tao Hu]]</li><li>David W. Zhang</li><li>Yuki M. Asano</li><li>Gertjan J. Burghouts</li><li>Cees G. M. Snoek</li></ul> Unlocking Slot Attention by Changing Optimal Transport Costs 2023 <ul><li>paper</li><li>computer_vision</li><li>video</li></ul> <ul><li>[[Yan Zhang|Yan Zhang]]</li><li>David W. Zhang</li><li>[[Simon Lacoste-Julien|Simon Lacoste-Julien]]</li><li>Cees G. M. Snoek</li><li>Gertjan J. Burghouts</li></ul> BoxeR - Box-Attention for 2D and 3D Transformers 2021 <ul><li>paper</li><li>transformers</li><li>object_detection</li></ul> <ul><li>Duy-Kien Nguyen</li><li>[[Jihong Ju|Jihong Ju]]</li><li>[[Olaf Booij|Olaf Booij]]</li><li>Martin R. Oswald</li><li>Cees G. M. Snoek</li></ul>"},{"location":"100%20Reference%20notes/102%20Authors/Chelsea%20Finn/","title":"Chelsea Finn","text":"Properties affiliation Stanford"},{"location":"100%20Reference%20notes/102%20Authors/Cheng-Ping%20Hsieh/","title":"Cheng Ping Hsieh","text":"Properties affiliation NVIDIA"},{"location":"100%20Reference%20notes/102%20Authors/Chong%20Wang/","title":"Chong Wang","text":"Properties affiliation Apple, Princeton University"},{"location":"100%20Reference%20notes/102%20Authors/Christopher%20Olah/","title":"Christopher Olah","text":"Properties affiliation Anthropic"},{"location":"100%20Reference%20notes/102%20Authors/Christos%20Perivolaropoulos/","title":"Christos Perivolaropoulos","text":"Properties affiliation Google DeepMind"},{"location":"100%20Reference%20notes/102%20Authors/Daniel%20M.%20Roy/","title":"Daniel M. Roy","text":"Properties affiliation Vector Institute"},{"location":"100%20Reference%20notes/102%20Authors/Daniel%20Ulbricht/","title":"Daniel Ulbricht","text":"Properties affiliation Apple"},{"location":"100%20Reference%20notes/102%20Authors/David%20M.%20Knigge/","title":"David M. Knigge","text":"Properties affiliation University of Amsterdam"},{"location":"100%20Reference%20notes/102%20Authors/David%20W.%20Romero/","title":"David W. Romero","text":"Properties affiliation Vrije Universiteit Amsterdam"},{"location":"100%20Reference%20notes/102%20Authors/David%20W.%20Zhang/","title":"David W. Zhang","text":"Properties affiliation University of Amsterdam, FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Dheeraj%20Varghese/","title":"Dheeraj Varghese","text":"Properties affiliation University of Amsterdam, VIS Lab"},{"location":"100%20Reference%20notes/102%20Authors/Diane%20Larlus/","title":"Diane Larlus","text":"Properties affiliation Naver Labs Europe"},{"location":"100%20Reference%20notes/102%20Authors/Donghyun%20Kim/","title":"Donghyun Kim","text":"Properties affiliation Naver Cloud AI"},{"location":"100%20Reference%20notes/102%20Authors/Dongyoon%20Han/","title":"Dongyoon Han","text":"Properties affiliation Naver AI Lab"},{"location":"100%20Reference%20notes/102%20Authors/Duy-Kien%20Nguyen/","title":"Duy Kien Nguyen","text":"Properties affiliation University of Amsterdam"},{"location":"100%20Reference%20notes/102%20Authors/Edward%20J.%20Hu/","title":"Edward J. Hu","text":"Properties affiliation Microsoft"},{"location":"100%20Reference%20notes/102%20Authors/Edward%20Z.%20Yang/","title":"Edward Z. Yang","text":"Properties affiliation FAIR, Stanford, MIT, PyTorch <p>Notes: - Has a pretty cool YouTube channel where he shares (bi-weekly) PyTorch meetings     - For me, it's a nice source to get more involved with PyTorch compiler-ish libraries/tools like [[ExecuTorch|ExecuTorch]], [[torch.export|torch.export]]     - Also it is interesting to see the interaction between engineers</p>"},{"location":"100%20Reference%20notes/102%20Authors/Efstratios%20Gavves/","title":"Efstratios Gavves","text":"Properties affiliation University of Amsterdam"},{"location":"100%20Reference%20notes/102%20Authors/Eric%20Mintun/","title":"Eric Mintun","text":"Properties affiliation FAIR, UC Santa Barbara"},{"location":"100%20Reference%20notes/102%20Authors/Erik%20J.%20Bekkers/","title":"Erik J. Bekkers","text":"Properties affiliation University of Amsterdam"},{"location":"100%20Reference%20notes/102%20Authors/Eshan%20Verma/","title":"Eshan Verma","text":"Properties affiliation Apple"},{"location":"100%20Reference%20notes/102%20Authors/Fahim%20Tajwar/","title":"Fahim Tajwar","text":"Properties affiliation Stanford"},{"location":"100%20Reference%20notes/102%20Authors/Fartash%20Faghri/","title":"Fartash Faghri","text":"Properties affiliation Apple"},{"location":"100%20Reference%20notes/102%20Authors/Federico%20Barbero/","title":"Federico Barbero","text":"Properties affiliation University of Oxford"},{"location":"100%20Reference%20notes/102%20Authors/Florian%20Bordes/","title":"Florian Bordes","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Francisco%20Massa/","title":"Francisco Massa","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Fred%20Hohman/","title":"Fred Hohman","text":"Properties affiliation Apple"},{"location":"100%20Reference%20notes/102%20Authors/Furu%20Wei/","title":"Furu Wei","text":"Properties affiliation Microsoft"},{"location":"100%20Reference%20notes/102%20Authors/Gabriel%20Synnaeve/","title":"Gabriel Synnaeve","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Geoffrey%20Hinton/","title":"Geoffrey Hinton","text":"Properties affiliation Google"},{"location":"100%20Reference%20notes/102%20Authors/Gertjan%20Burghouts/","title":"Gertjan Burghouts","text":"Properties affiliation TNO"},{"location":"100%20Reference%20notes/102%20Authors/Gertjan%20J.%20Burghouts/","title":"Gertjan J. Burghouts","text":"Properties affiliation TNO"},{"location":"100%20Reference%20notes/102%20Authors/Gintare%20Karolina%20Dziugaite/","title":"Gintare Karolina Dziugaite","text":"Properties affiliation Google DeepMind"},{"location":"100%20Reference%20notes/102%20Authors/Giovanni%20Chierchia/","title":"Giovanni Chierchia","text":"Properties affiliation Univ Gustave Eiffel"},{"location":"100%20Reference%20notes/102%20Authors/Gr%C3%A9goire%20Mialon/","title":"Gr\u00e9goire Mialon","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Hadi%20Pouransari/","title":"Hadi Pouransari","text":"Properties affiliation Apple"},{"location":"100%20Reference%20notes/102%20Authors/Han%20Cai/","title":"Han Cai","text":"Properties affiliation MIT, Shanghai Jiao Tong University"},{"location":"100%20Reference%20notes/102%20Authors/Hanzi%20Mao/","title":"Hanzi Mao","text":"Properties affiliation FAIR, NVIDIA"},{"location":"100%20Reference%20notes/102%20Authors/Haoxiang%20Wang/","title":"Haoxiang Wang","text":"Properties affiliation Apple, University of Illinois at Urbana-Champaign"},{"location":"100%20Reference%20notes/102%20Authors/Herv%C3%A9%20Jegou/","title":"Herv\u00e9 Jegou","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Huaxiu%20Yao/","title":"Huaxiu Yao","text":"Properties affiliation Stanford"},{"location":"100%20Reference%20notes/102%20Authors/Hugo%20Touvron/","title":"Hugo Touvron","text":"Properties affiliation FAIR, Sorbonne University"},{"location":"100%20Reference%20notes/102%20Authors/Huizi%20Mao/","title":"Huizi Mao","text":"Properties affiliation NVIDIA"},{"location":"100%20Reference%20notes/102%20Authors/Ilya%20Loshchilov/","title":"Ilya Loshchilov","text":"Properties affiliation NVIDIA"},{"location":"100%20Reference%20notes/102%20Authors/Ioana%20Simion/","title":"Ioana Simion","text":"Properties affiliation University of Amsterdam"},{"location":"100%20Reference%20notes/102%20Authors/Isha%20Garg/","title":"Isha Garg","text":"Properties affiliation Purdue University, Apple"},{"location":"100%20Reference%20notes/102%20Authors/Ishan%20Misra/","title":"Ishan Misra","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Jan%20E.%20Gerken/","title":"Jan E. Gerken","text":"Properties affiliation Chalmers University of Technology"},{"location":"100%20Reference%20notes/102%20Authors/Javier%20Maass%20Martinez/","title":"Javier Maass Martinez","text":"Properties affiliation University of Chile"},{"location":"100%20Reference%20notes/102%20Authors/Jean%20Ponce/","title":"Jean Ponce","text":"Properties affiliation INRIA, New York University"},{"location":"100%20Reference%20notes/102%20Authors/Jean-Baptiste%20Cordonnier/","title":"Jean Baptiste Cordonnier","text":"Properties affiliation EPFL"},{"location":"100%20Reference%20notes/102%20Authors/Jean-Fran%C3%A7ois%20Bercher/","title":"Jean Fran\u00e7ois Bercher","text":"Properties affiliation Univ Gustave Eiffel"},{"location":"100%20Reference%20notes/102%20Authors/Jeff%20Pool/","title":"Jeff Pool","text":"Properties affiliation NVIDIA"},{"location":"100%20Reference%20notes/102%20Authors/Jesse%20Cai/","title":"Jesse Cai","text":"Properties affiliation Meta, UCLA, PyTorch"},{"location":"100%20Reference%20notes/102%20Authors/Jing%20Pu/","title":"Jing Pu","text":"Properties affiliation Google, Stanford"},{"location":"100%20Reference%20notes/102%20Authors/Joaquin%20Fontbona/","title":"Joaquin Fontbona","text":"Properties affiliation University of Chile"},{"location":"100%20Reference%20notes/102%20Authors/Joaquin%20Vanschoren/","title":"Joaquin Vanschoren","text":"Properties affiliation TU Eindhoven"},{"location":"100%20Reference%20notes/102%20Authors/John%20Denker/","title":"John Denker","text":"Properties affiliation Nokia Bell Labs"},{"location":"100%20Reference%20notes/102%20Authors/John%20Tran/","title":"John Tran","text":"Properties affiliation NVIDIA"},{"location":"100%20Reference%20notes/102%20Authors/Julien%20Mairal/","title":"Julien Mairal","text":"Properties affiliation INRIA"},{"location":"100%20Reference%20notes/102%20Authors/Juliette%20Marrie/","title":"Juliette Marrie","text":"Properties affiliation Naver Labs Europe, INRIA"},{"location":"100%20Reference%20notes/102%20Authors/Kaiming%20He/","title":"Kaiming He","text":"Properties affiliation FAIR, MIT"},{"location":"100%20Reference%20notes/102%20Authors/Kamyar%20Azizzadenesheli/","title":"Kamyar Azizzadenesheli","text":"Properties affiliation NVIDIA, Purdue University"},{"location":"100%20Reference%20notes/102%20Authors/Kaushik%20Roy/","title":"Kaushik Roy","text":"Properties affiliation Purdue University"},{"location":"100%20Reference%20notes/102%20Authors/Laurent%20Najman/","title":"Laurent Najman","text":"Properties affiliation Univ Gustave Eiffel"},{"location":"100%20Reference%20notes/102%20Authors/Lawrence%20Chan/","title":"Lawrence Chan","text":"Properties affiliation UC Berkeley"},{"location":"100%20Reference%20notes/102%20Authors/Lucas%20Beyer/","title":"Lucas Beyer","text":"Properties affiliation Google, OpenAI"},{"location":"100%20Reference%20notes/102%20Authors/Lucia%20Donatelli/","title":"Lucia Donatelli","text":"Properties affiliation Vrije Universiteit Amsterdam"},{"location":"100%20Reference%20notes/102%20Authors/Lucius%20Bushnaq/","title":"Lucius Bushnaq","text":"Properties affiliation Apollo Research"},{"location":"100%20Reference%20notes/102%20Authors/Maciej%20Wo%C5%82czyk/","title":"Maciej Wo\u0142czyk","text":"Properties affiliation IDEAS NCBR"},{"location":"100%20Reference%20notes/102%20Authors/Mahdi%20Soltanolkotabi/","title":"Mahdi Soltanolkotabi","text":"Properties affiliation University of Southern California"},{"location":"100%20Reference%20notes/102%20Authors/Mahmoud%20Assran/","title":"Mahmoud Assran","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Marc%20Finzi/","title":"Marc Finzi","text":"Properties affiliation New York University"},{"location":"100%20Reference%20notes/102%20Authors/Mark%20A.%20Horowitz/","title":"Mark A. Horowitz","text":"Properties affiliation Stanford"},{"location":"100%20Reference%20notes/102%20Authors/Mark%20Ibrahim/","title":"Mark Ibrahim","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Martin%20Jaggi/","title":"Martin Jaggi","text":"Properties affiliation EPFL"},{"location":"100%20Reference%20notes/102%20Authors/Martin%20R.%20Oswald/","title":"Martin R. Oswald","text":"Properties affiliation University of Amsterdam"},{"location":"100%20Reference%20notes/102%20Authors/Marzieh%20Fadaee/","title":"Marzieh Fadaee","text":"Properties affiliation Cohere"},{"location":"100%20Reference%20notes/102%20Authors/Mathilde%20Caron/","title":"Mathilde Caron","text":"Properties affiliation FAIR, INRIA"},{"location":"100%20Reference%20notes/102%20Authors/Max%20Belistky/","title":"Max Belistky","text":"Properties affiliation University of Amsterdam"},{"location":"100%20Reference%20notes/102%20Authors/Maxime%20Oquab/","title":"Maxime Oquab","text":"Properties affiliation FAIR, INRIA"},{"location":"100%20Reference%20notes/102%20Authors/Mehrdad%20Farajtabar/","title":"Mehrdad Farajtabar","text":"Properties affiliation Apple"},{"location":"100%20Reference%20notes/102%20Authors/Micah%20Goldblum/","title":"Micah Goldblum","text":"Properties affiliation New York University"},{"location":"100%20Reference%20notes/102%20Authors/Michael%20Arbel/","title":"Michael Arbel","text":"Properties affiliation INRIA"},{"location":"100%20Reference%20notes/102%20Authors/Michael%20Dorkenwald/","title":"Michael Dorkenwald","text":"Properties affiliation University of Amsterdam, VIS Lab"},{"location":"100%20Reference%20notes/102%20Authors/Mohammad%20Mahdi%20Derakhshani/","title":"Mohammad Mahdi Derakhshani","text":"Properties affiliation University of Amsterdam, VIS Lab"},{"location":"100%20Reference%20notes/102%20Authors/Mohammad%20Rastegari/","title":"Mohammad Rastegari","text":"Properties affiliation Apple"},{"location":"100%20Reference%20notes/102%20Authors/Mohammadreza%20Salehi/","title":"Mohammadreza Salehi","text":"Properties affiliation University of Amsterdam"},{"location":"100%20Reference%20notes/102%20Authors/Namuk%20Park/","title":"Namuk Park","text":"Properties affiliation Naver AI Lab, Prescient Design, Genentech"},{"location":"100%20Reference%20notes/102%20Authors/Navin%20Ranjan/","title":"Navin Ranjan","text":"Properties affiliation Rochester Institute of Technology"},{"location":"100%20Reference%20notes/102%20Authors/Neel%20Nanda/","title":"Neel Nanda","text":"Properties affiliation Google DeepMind, Anthropic"},{"location":"100%20Reference%20notes/102%20Authors/Neil%20Houlsby/","title":"Neil Houlsby","text":"Properties affiliation Anthropic, Google"},{"location":"100%20Reference%20notes/102%20Authors/Nicolas%20Carion/","title":"Nicolas Carion","text":"Properties affiliation New York University"},{"location":"100%20Reference%20notes/102%20Authors/Nicolas%20Michel/","title":"Nicolas Michel","text":"Properties affiliation Univ Gustave Eiffel"},{"location":"100%20Reference%20notes/102%20Authors/Nicolas%20Usunier/","title":"Nicolas Usunier","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Olivier%20J.%20H%C3%A9naff/","title":"Olivier J. H\u00e9naff","text":"Properties affiliation Google DeepMind"},{"location":"100%20Reference%20notes/102%20Authors/Oncel%20Tuzel/","title":"Oncel Tuzel","text":"Properties affiliation Apple"},{"location":"100%20Reference%20notes/102%20Authors/Pascal%20Vincent/","title":"Pascal Vincent","text":"Properties affiliation FAIR, Mila Quebec AI Institute"},{"location":"100%20Reference%20notes/102%20Authors/Patrick%20Forr%C3%A9/","title":"Patrick Forr\u00e9","text":"Properties affiliation University of Amsterdam"},{"location":"100%20Reference%20notes/102%20Authors/Pavan%20Kumar%20Anasosalu%20Vasu/","title":"Pavan Kumar Anasosalu Vasu","text":"Properties affiliation Apple"},{"location":"100%20Reference%20notes/102%20Authors/Percy%20Liang/","title":"Percy Liang","text":"Properties affiliation Stanford"},{"location":"100%20Reference%20notes/102%20Authors/Petar%20Veli%C4%8Dkovi%C4%87/","title":"Petar Veli\u010dkovi\u0107","text":"Properties affiliation Google DeepMind"},{"location":"100%20Reference%20notes/102%20Authors/Phillip%20Isola/","title":"Phillip Isola","text":"Properties affiliation MIT"},{"location":"100%20Reference%20notes/102%20Authors/Piotr%20Bojanowski/","title":"Piotr Bojanowski","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Piotr%20Doll%C3%A1r/","title":"Piotr Doll\u00e1r","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Quentin%20Garrido/","title":"Quentin Garrido","text":"Properties affiliation FAIR, Univ Gustave Eiffel"},{"location":"100%20Reference%20notes/102%20Authors/Randall%20Balestriero/","title":"Randall Balestriero","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Ravid%20Shwartz-Ziv/","title":"Ravid Shwartz Ziv","text":"Properties affiliation New York University"},{"location":"100%20Reference%20notes/102%20Authors/Raviteja%20Vemulapalli/","title":"Raviteja Vemulapalli","text":"Properties affiliation Apple"},{"location":"100%20Reference%20notes/102%20Authors/Razvan%20Pascanu/","title":"Razvan Pascanu","text":"Properties affiliation Google DeepMind"},{"location":"100%20Reference%20notes/102%20Authors/Robin%20Walters/","title":"Robin Walters","text":"Properties affiliation Northeastern University"},{"location":"100%20Reference%20notes/102%20Authors/Romain%20Cosentino/","title":"Romain Cosentino","text":"Properties affiliation University of Southern California"},{"location":"100%20Reference%20notes/102%20Authors/Romain%20Negrel/","title":"Romain Negrel","text":"Properties affiliation Univ Gustave Eiffel"},{"location":"100%20Reference%20notes/102%20Authors/Rose%20Yu/","title":"Rose Yu","text":"Properties affiliation UC San Diego"},{"location":"100%20Reference%20notes/102%20Authors/Ross%20Girshick/","title":"Ross Girshick","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Rui%20Wang/","title":"Rui Wang","text":"Properties affiliation MIT, UC San Diego"},{"location":"100%20Reference%20notes/102%20Authors/Ruoming%20Pang/","title":"Ruoming Pang","text":"Properties affiliation Apple, Princeton University"},{"location":"100%20Reference%20notes/102%20Authors/Sachin%20Mehta/","title":"Sachin Mehta","text":"Properties affiliation Apple"},{"location":"100%20Reference%20notes/102%20Authors/Saining%20Xie/","title":"Saining Xie","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Sangdoo%20Yun/","title":"Sangdoo Yun","text":"Properties affiliation Naver AI Lab"},{"location":"100%20Reference%20notes/102%20Authors/Sanghyuk%20Chun/","title":"Sanghyuk Chun","text":"Properties affiliation Naver AI Lab"},{"location":"100%20Reference%20notes/102%20Authors/Sara%20Solla/","title":"Sara Solla","text":"Properties affiliation Northwestern University"},{"location":"100%20Reference%20notes/102%20Authors/Sergey%20Zagoruyko/","title":"Sergey Zagoruyko","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Shaohan%20Huang/","title":"Shaohan Huang","text":"Properties affiliation Microsoft"},{"location":"100%20Reference%20notes/102%20Authors/Shashanka%20Venkataramanan/","title":"Shashanka Venkataramanan","text":"Properties affiliation INRIA"},{"location":"100%20Reference%20notes/102%20Authors/Sibylle%20Hess/","title":"Sibylle Hess","text":"Properties affiliation TU Eindhoven"},{"location":"100%20Reference%20notes/102%20Authors/Simeng%20Sun/","title":"Simeng Sun","text":"Properties affiliation NVIDIA"},{"location":"100%20Reference%20notes/102%20Authors/Simon%20J.D.%20Prince/","title":"Simon J.D. Prince","text":"Properties affiliation University of Bath"},{"location":"100%20Reference%20notes/102%20Authors/Skander%20Moalla/","title":"Skander Moalla","text":"Properties affiliation CLAIRE, EPFL"},{"location":"100%20Reference%20notes/102%20Authors/Soham%20De/","title":"Soham De","text":"Properties affiliation Google DeepMind, University of Maryland"},{"location":"100%20Reference%20notes/102%20Authors/Song%20Han/","title":"Song Han","text":"Properties affiliation MIT"},{"location":"100%20Reference%20notes/102%20Authors/Song%20Park/","title":"Song Park","text":"Properties affiliation Naver AI Lab"},{"location":"100%20Reference%20notes/102%20Authors/Songkuk%20Kim/","title":"Songkuk Kim","text":"Properties affiliation Yonsei University"},{"location":"100%20Reference%20notes/102%20Authors/Sonia%20Joseph/","title":"Sonia Joseph","text":"Properties affiliation Meta"},{"location":"100%20Reference%20notes/102%20Authors/Sourya%20Basu/","title":"Sourya Basu","text":"Properties affiliation University of Illinois at Urbana-Champaign, IBM Research"},{"location":"100%20Reference%20notes/102%20Authors/St%C3%A9phane%20d%27Ascoli/","title":"St\u00e9phane d'Ascoli","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Sukjun%20Hwang/","title":"Sukjun Hwang","text":"Properties affiliation Carnegie Mellon University"},{"location":"100%20Reference%20notes/102%20Authors/Taekyung%20Kim/","title":"Taekyung Kim","text":"Properties affiliation Naver AI Lab"},{"location":"100%20Reference%20notes/102%20Authors/Tete%20Xiao/","title":"Tete Xiao","text":"Properties affiliation FAIR <p>Associations: FAIR, UC Berkeley</p>"},{"location":"100%20Reference%20notes/102%20Authors/Thomas%20Fel/","title":"Thomas Fel","text":"Properties affiliation Kempner Institute, Harvard"},{"location":"100%20Reference%20notes/102%20Authors/Thomas%20Kipf/","title":"Thomas Kipf","text":"Properties affiliation University of Amsterdam, Google DeepMind"},{"location":"100%20Reference%20notes/102%20Authors/Tim%20R.%20Davidson/","title":"Tim R. Davidson","text":"Properties affiliation University of Amsterdam, EPFL"},{"location":"100%20Reference%20notes/102%20Authors/Tom%20Goldstein/","title":"Tom Goldstein","text":"Properties affiliation University of Maryland"},{"location":"100%20Reference%20notes/102%20Authors/Tom%20Gunter/","title":"Tom Gunter","text":"Properties affiliation Apple, University of Oxford"},{"location":"100%20Reference%20notes/102%20Authors/Tom%20Lieberum/","title":"Tom Lieberum","text":"Properties affiliation University of Amsterdam, Google DeepMind"},{"location":"100%20Reference%20notes/102%20Authors/Tongzhou%20Wang/","title":"Tongzhou Wang","text":"Properties affiliation MIT"},{"location":"100%20Reference%20notes/102%20Authors/Vaibhav%20Aggarwal/","title":"Vaibhav Aggarwal","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Valentinos%20Pariza/","title":"Valentinos Pariza","text":"Properties affiliation FunAI Lab"},{"location":"100%20Reference%20notes/102%20Authors/Walter%20Simoncini/","title":"Walter Simoncini","text":"Properties affiliation University of Amsterdam, QUVA Lab"},{"location":"100%20Reference%20notes/102%20Authors/Wei%20Lu/","title":"Wei Lu","text":"Properties affiliation Xiaomi AI Lab"},{"location":"100%20Reference%20notes/102%20Authors/William%20J.%20Dally/","title":"William J. Dally","text":"Properties affiliation Stanford, NVIDIA"},{"location":"100%20Reference%20notes/102%20Authors/Wonjae%20Kim/","title":"Wonjae Kim","text":"Properties affiliation Naver AI Lab"},{"location":"100%20Reference%20notes/102%20Authors/Xiang%20Yue/","title":"Xiang Yue","text":"Properties affiliation Carnegie Mellon University"},{"location":"100%20Reference%20notes/102%20Authors/Xingyu%20Liu/","title":"Xingyu Liu","text":"Properties affiliation Carnegie Mellon University"},{"location":"100%20Reference%20notes/102%20Authors/Xinlei%20Chen/","title":"Xinlei Chen","text":"Properties affiliation FAIR, Zhejiang University, Carnegie Mellon University, Zhejiang University"},{"location":"100%20Reference%20notes/102%20Authors/Xiuying%20Wei/","title":"Xiuying Wei","text":"Properties affiliation EPFL, CLAIRE"},{"location":"100%20Reference%20notes/102%20Authors/Xu%20Ma/","title":"Xu Ma","text":"Properties affiliation Northeastern University"},{"location":"100%20Reference%20notes/102%20Authors/Xun%20Wu/","title":"Xun Wu","text":"Properties affiliation Microsoft, Tsinghua University"},{"location":"100%20Reference%20notes/102%20Authors/Yanghao%20Li/","title":"Yanghao Li","text":"Properties affiliation FAIR, Apple"},{"location":"100%20Reference%20notes/102%20Authors/Yann%20LeCun/","title":"Yann LeCun","text":"Properties affiliation FAIR, New York University"},{"location":"100%20Reference%20notes/102%20Authors/Yelong%20Shen/","title":"Yelong Shen","text":"Properties affiliation Microsoft"},{"location":"100%20Reference%20notes/102%20Authors/Yi%20Ma/","title":"Yi Ma","text":"Properties affiliation UC Berkeley, HKU"},{"location":"100%20Reference%20notes/102%20Authors/Yoonho%20Lee/","title":"Yoonho Lee","text":"Properties affiliation Stanford"},{"location":"100%20Reference%20notes/102%20Authors/Yubei%20Chen/","title":"Yubei Chen","text":"Properties affiliation FAIR, New York University"},{"location":"100%20Reference%20notes/102%20Authors/Yuki%20M.%20Asano/","title":"Yuki M. Asano","text":"Properties affiliation University of Amsterdam, University of Technology Nuremberg"},{"location":"100%20Reference%20notes/102%20Authors/Zeyuan%20Allen-Zhu/","title":"Zeyuan Allen Zhu","text":"Properties affiliation FAIR"},{"location":"100%20Reference%20notes/102%20Authors/Zhuoyang%20Zhang/","title":"Zhuoyang Zhang","text":"Properties affiliation NVIDIA, Tsinghua University"},{"location":"100%20Reference%20notes/102%20Authors/Ziaoyi%20Zhang/","title":"Ziaoyi Zhang","text":"Properties affiliation Apple"},{"location":"100%20Reference%20notes/102%20Authors/Zirui%20Wang/","title":"Zirui Wang","text":"Properties affiliation Apple, Google, Carnegie Mellon University"},{"location":"100%20Reference%20notes/102%20Authors/Ziyang%20Wu/","title":"Ziyang Wu","text":"Properties affiliation UC Berkeley"},{"location":"100%20Reference%20notes/103%20Affiliations/CLAIRE/","title":"CLAIRE","text":""},{"location":"100%20Reference%20notes/103%20Affiliations/CLAIRE/#three-essential-pilars-of-the-lab","title":"Three essential pilars of the lab","text":"<p>Efficient deep learning algorithms</p> <ul> <li>Efficient RL</li> <li>Sample efficient learning algorithms</li> <li>Model Recycling</li> <li>Efficient sequence models</li> </ul> <p>Robust, safe and responsible algorithms</p> <ul> <li>RLHF/Alignment</li> <li>Uncertainty aware/Bayesian algorithms</li> <li>Offline RL</li> <li>Active learning/Human in the loop algorithms</li> <li>Better evaluations</li> </ul> <p>Improving reasoning: Moving from system 1 to system 2 level thinking</p> <ul> <li>Improving reasoning</li> <li>Creativity</li> <li>Deliberation</li> <li>Causality</li> <li>Imagination</li> <li>Planning</li> </ul>"},{"location":"100%20Reference%20notes/103%20Affiliations/CLAIRE/#notes","title":"Notes","text":"<ul> <li>omg, this is amazing</li> <li> Note to self: Look at CLAIRE's research \u23eb </li> </ul>"},{"location":"100%20Reference%20notes/103%20Affiliations/FAIR/","title":"FAIR","text":"<p>Related: FAIR</p>"},{"location":"100%20Reference%20notes/103%20Affiliations/Naver%20Labs%20Europe/","title":"Naver Labs Europe","text":"<p>Related to Naver AI Lab</p>"},{"location":"100%20Reference%20notes/104%20Other/EPFL-CS439%20-%20Optimization%20for%20Machine%20Learning/","title":"EPFL CS439   Optimization for Machine Learning","text":"Properties authors Martin Jaggi, Nicolas Flammarion year 2024 url https://github.com/epfml/OptML_course/tree/master <p>Abstract</p> <p>This course teaches an overview of modern mathematical optimization methods, for applications in machine learning and data science. In particular, scalability of algorithms to large datasets will be discussed in theory and in implementation.</p> <p>Topics</p> <p>Convexity, Gradient Methods, Proximal algorithms, Subgradient Methods, Stochastic and Online Variants of mentioned methods, Coordinate Descent, Frank-Wolfe, Accelerated Methods, Primal-Dual context and certificates, Lagrange and Fenchel Duality, Second-Order Methods including Quasi-Newton Methods, Derivative-Free Optimization.</p>","tags":["course","optimization"]},{"location":"100%20Reference%20notes/104%20Other/GPU%20mode%20-%20Sparsity/","title":"GPU mode   Sparsity","text":"Properties authors Jesse Cai year 2024 url https://github.com/gpu-mode/lectures/blob/main/lecture_011/sparsity.pptx","tags":["lecture","presentation"]},{"location":"100%20Reference%20notes/104%20Other/GPU%20mode%20-%20Sparsity/#notes","title":"Notes","text":"<ul> <li> #todo take notes</li> </ul>","tags":["lecture","presentation"]},{"location":"100%20Reference%20notes/104%20Other/Introducing%20Apple%E2%80%99s%20On-Device%20and%20Server%20Foundation%20Models/","title":"Introducing Apple\u2019s On Device and Server Foundation Models","text":"Properties year 2024 url https://machinelearning.apple.com/research/introducing-apple-foundation-models","tags":["efficient_dl"]},{"location":"100%20Reference%20notes/104%20Other/Introducing%20Apple%E2%80%99s%20On-Device%20and%20Server%20Foundation%20Models/#pre-training","title":"## Pre-Training","text":"<p>Our foundation models are trained on\u00a0Apple's AXLearn framework, an open-source project we released in 2023. It builds on top of JAX and XLA, and allows us to train the models with high efficiency and scalability on various training hardware and cloud platforms, including TPUs and both cloud and on-premise GPUs. We used a combination of data parallelism, tensor parallelism, sequence parallelism, and Fully Sharded Data Parallel (FSDP) to scale training along multiple dimensions such as data, model, and sequence length.</p>","tags":["efficient_dl"]},{"location":"100%20Reference%20notes/104%20Other/Introducing%20Apple%E2%80%99s%20On-Device%20and%20Server%20Foundation%20Models/#optimization","title":"Optimization","text":"<p>In addition to ensuring our generative models are highly capable, we have used a range of innovative techniques to optimize them on-device and on our private cloud for speed and efficiency. We have applied an extensive set of optimizations for both first token and extended token inference performance.</p> <p>Both the on-device and server models use grouped-query-attention. We use shared input and output vocab embedding tables to reduce memory requirements and inference cost. These shared embedding tensors are mapped without duplications. The on-device model uses a vocab size of 49K, while the server model uses a vocab size of 100K, which includes additional language and technical tokens.</p> <p>For on-device inference, we use low-Bit Palettization, a critical optimization technique that achieves the necessary memory, power, and performance requirements. To maintain model quality, we developed a new framework using LoRA adapters that incorporates a mixed 2-bit and 4-bit configuration strategy \u2014 averaging 3.5 bits-per-weight \u2014 to achieve the same accuracy as the uncompressed models.</p> <p>Additionally, we use an interactive model latency and power analysis tool,\u00a0Talaria, to better guide the bit rate selection for each operation. We also utilize activation quantization and embedding quantization, and have developed an approach to enable efficient Key-Value (KV) cache update on our neural engines.</p> <p>References: Talaria - Interactively Optimizing Machine Learning Models for Efficient Inference Notes: - Might be useful to look at KV Cache hardware-dependency</p> <p>With this set of optimizations, on iPhone 15 Pro we are able to reach time-to-first-token latency of about 0.6 millisecond per prompt token, and a generation rate of 30 tokens per second. Notably, this performance is attained before employing token speculation techniques, from which we see further enhancement on the token generation rate.</p>","tags":["efficient_dl"]},{"location":"100%20Reference%20notes/104%20Other/Introducing%20Apple%E2%80%99s%20On-Device%20and%20Server%20Foundation%20Models/#model-adaptation","title":"Model Adaptation","text":"<p>Our foundation models are fine-tuned for users\u2019 everyday activities, and can dynamically specialize themselves on-the-fly for the task at hand. We utilize adapters, small neural network modules that can be plugged into various layers of the pre-trained model, to fine-tune our models for specific tasks. For our models we adapt the attention matrices, the attention projection matrix, and the fully connected layers in the point-wise feedforward networks for a suitable set of the decoding layers of the transformer architecture.</p> <p>Notes: - How do you adapt the attention matrices? Is it like a bias? `A[i][j] += lora[i][j] - Attention projection matrix I suppose referes to the projection matrices \\(W_Q, W_K, W_V\\)</p> <p>By fine-tuning only the adapter layers, the original parameters of the base pre-trained model remain unchanged, preserving the general knowledge of the model while tailoring the adapter layers to support specific tasks.</p> <p>We represent the values of the adapter parameters using 16 bits, and for the ~3 billion parameter on-device model, the parameters for a rank 16 adapter typically require 10s of megabytes. The adapter models can be dynamically loaded, temporarily cached in memory, and swapped \u2014 giving our foundation model the ability to specialize itself on the fly for the task at hand while efficiently managing memory and guaranteeing the operating system's responsiveness.</p> <p>To facilitate the training of the adapters, we created an efficient infrastructure that allows us to rapidly retrain, test, and deploy adapters when either the base model or the training data gets updated. The adapter parameters are initialized using\u00a0the accuracy-recovery adapter introduced in the Optimization section.</p>","tags":["efficient_dl"]},{"location":"100%20Reference%20notes/104%20Other/Introduction%20to%20Quantization%20on%20PyTorch/","title":"Introduction to Quantization on PyTorch","text":"Properties authors Raghuraman Krishnamoorthi, James Reed, Min Ni, Chris Gottbrath, Seth Weidman year 2020 url https://pytorch.org/blog/introduction-to-quantization-on-pytorch/","tags":["website","efficient_dl","quantization"]},{"location":"100%20Reference%20notes/104%20Other/Introduction%20to%20Quantization%20on%20PyTorch/#notes","title":"Notes","text":"<p>Quantization aware training is typically only used in CNN models when post training static or dynamic quantization doesn\u2019t yield sufficient accuracy. This can occur with models that are highly optimized to achieve small size (such as Mobilenet).</p> <p>Currently, operator coverage is limited and may restrict the choices listed in the table below: The table below provides a guideline.</p> Model Type Preferred scheme Why LSTM/RNN Dynamic Quantization Throughput dominated by compute/memory bandwidth for weights BERT/Transformer Dynamic Quantization Throughput dominated by compute/memory bandwidth for weights CNN Static Quantization Throughput limited by memory bandwidth for activations CNN Quantization Aware Training In the case where accuracy can't be achieved with static quantization <p>Does the Transformer row apply also for vision transformers? Since the number of tokens is quite large.</p> Model Float Latency (ms) Quantized Latency (ms) Inference Performance Gain Device Notes BERT 581 313 1.8x Xeon-D2191 (1.6GHz) Batch size = 1, Maximum sequence length= 128, Single thread, x86-64, Dynamic quantization Resnet-50 214 103 2x Xeon-D2191 (1.6GHz) Single thread, x86-64, Static quantization Mobilenet-v2 97 17 5.7x Samsung S9 Static quantization, Floating point numbers are based on Caffe2 run-time and are not optimized <p>So I should expect something around ~2x latency improvement with dynamic quantization</p>","tags":["website","efficient_dl","quantization"]},{"location":"100%20Reference%20notes/104%20Other/Let%27s%20talk%20about%20the%20Python%20Dispatcher/","title":"Let's talk about the Python Dispatcher","text":"Properties authors Edward Z. Yang year 2020 url http://blog.ezyang.com/2020/09/lets-talk-about-the-pytorch-dispatcher/","tags":["blog"]},{"location":"100%20Reference%20notes/104%20Other/MIT-65940%20-%20TinyML%20and%20Efficient%20Deep%20Learning%20Computing/","title":"MIT 65940   TinyML and Efficient Deep Learning Computing","text":"Properties authors Song Han year 2023 url https://hanlab.mit.edu/courses/2023-fall-65940","tags":["course"]},{"location":"100%20Reference%20notes/104%20Other/Optimizing%20Vision%20Transformer%20Model%20for%20Deployment/","title":"Optimizing Vision Transformer Model for Deployment","text":"Properties authors Jeff Tang, Geeta Chauhan year 2021 url https://pytorch.org/tutorials/beginner/vt_tutorial.html","tags":["website"]},{"location":"100%20Reference%20notes/104%20Other/PyTorch%20-%20ExecuTorch%20-%20Export%20IR%20Specification/","title":"PyTorch   ExecuTorch   Export IR Specification","text":"Properties authors PyTorch - Functionalization in PyTorch - Everything you need to know year 2024 url https://pytorch.org/executorch/main/ir-exir.html <p>The Exported IR is a specification that consists of the following parts:</p> <ol> <li>A definition of computation graph model.</li> <li>Set of operators allowed in the graph.</li> </ol> <p>A dialect also provides further constraints meant for a specific purpose or stage in some compilation phase. Some dialects are: - aten dialect - edge dialect - backend dialect</p> <p>Executorch compilation first exports to aten, then to edge and finally to backend.</p>","tags":["paper"]},{"location":"100%20Reference%20notes/104%20Other/PyTorch%20-%20ExecuTorch%20-%20Export%20IR%20Specification/#aten-dialect","title":"Aten Dialect","text":"<ul> <li>PyTorch Functionalization is performed, removing any tensor aliases and mutations, and allowing for more flexible graph transformations to be made.</li> </ul>","tags":["paper"]},{"location":"100%20Reference%20notes/104%20Other/PyTorch%20-%20ExecuTorch%20-%20How%20ExecuTorch%20works%3F/","title":"PyTorch   ExecuTorch   How ExecuTorch works?","text":"Properties authors PyTorch Quantization for TensorRT year 2024 url https://pytorch.org/executorch/main/intro-how-it-works","tags":["pytorch","compilers","efficient_dl","documentation"]},{"location":"100%20Reference%20notes/104%20Other/PyTorch%20-%20ExecuTorch%20-%20How%20ExecuTorch%20works%3F/#what-are-the-steps-to-run-a-model-with-executorch","title":"What are the steps to run a model with ExecuTorch?","text":"","tags":["pytorch","compilers","efficient_dl","documentation"]},{"location":"100%20Reference%20notes/104%20Other/PyTorch%20-%20ExecuTorch%20-%20How%20ExecuTorch%20works%3F/#1-export-the-model","title":"1. Export the model","text":"<ul> <li>Capture the pytorch program as a graph </li> </ul>","tags":["pytorch","compilers","efficient_dl","documentation"]},{"location":"100%20Reference%20notes/104%20Other/PyTorch%20-%20ExecuTorch%20-%20How%20ExecuTorch%20works%3F/#2-compile-the-exported-model-to-an-executorch-program","title":"2. Compile the exported model to an ExecuTorch program","text":"<p>Captured Graph -&gt; ExecuTorch program</p> <p>Possible Optimizations: - Compressing the model (e.g., quantization)  - Lowering subgraphs to on-device specialized hardware accelerators to improve latency. - memory planning, i.e. to efficiently plan the location of intermediate tensors to reduce the runtime memory footprint.</p>","tags":["pytorch","compilers","efficient_dl","documentation"]},{"location":"100%20Reference%20notes/104%20Other/PyTorch%20-%20ExecuTorch%20-%20How%20ExecuTorch%20works%3F/#3-run-the-executorch-program-to-a-target-device","title":"3. Run the ExecuTorch program to a target device","text":"<ul> <li>Light runtime with memory planning for fast inference :)</li> </ul>","tags":["pytorch","compilers","efficient_dl","documentation"]},{"location":"100%20Reference%20notes/104%20Other/PyTorch%20-%20ExecuTorch%20-%20How%20ExecuTorch%20works%3F/#key-benefits","title":"Key Benefits","text":"<ul> <li>Export that is robust and powerful</li> <li>Operator Standardization</li> <li>Standardization for compiler interfaces (aka delegates) and the OSS ecosystem</li> <li>First-party SDK and toolchain</li> <li>Ease of customization</li> <li>Low overhead runtime and execution</li> </ul>","tags":["pytorch","compilers","efficient_dl","documentation"]},{"location":"100%20Reference%20notes/104%20Other/PyTorch%20-%20ExecuTorch%20-%20Quantization%20Overview/","title":"PyTorch   ExecuTorch   Quantization Overview","text":"Properties authors PyTorch Quantization for TensorRT year 2024 url https://pytorch.org/executorch/main/quantization-overview.html <p>{ width=\"400\" }</p> <p>Quantization is usually tied to execution backends that have quantized operators implemented. Thus each backend is opinionated about how the model should be quantized, expressed in a backend specific\u00a0<code>Quantizer</code>\u00a0class.</p>","tags":["documentation"]},{"location":"100%20Reference%20notes/104%20Other/PyTorch%20-%20Functionalization%20in%20PyTorch%20-%20Everything%20you%20need%20to%20know/","title":"PyTorch   Functionalization in PyTorch   Everything you need to know","text":"Properties authors Brian Hirsh year 2023 url https://dev-discuss.pytorch.org/t/functionalization-in-pytorch-everything-you-wanted-to-know/965 <p>Given a program/function of PyTorch operators, functionalization will return a new function, that: 1. Has the same semantics as the old function 2. Has no mutations in it</p> <p>Exposed in functorch API.</p> <p>Functionalization operates at the level of our ATen API.</p> <p>Why? - Compilers don't like mutations: Graph partitioning is harder if nodes have side effects, etc.</p> <p>Notes: - PyTorch Functionalization</p>","tags":["documentation"]},{"location":"100%20Reference%20notes/104%20Other/PyTorch%20-%20PyTorch%202%20Export%20Post%20Training%20Quantization/","title":"PyTorch   PyTorch 2 Export Post Training Quantization","text":"Properties authors Jerry Zhang year 2024 url https://pytorch.org/tutorials/prototype/pt2e_quant_ptq.html <p>Uses <code>prepare_pt2e</code> and <code>convert_pt2e</code>.</p> <pre><code>float_model(Python)                          Example Input\n    \\                                              /\n     \\                                            /\n\u2014-------------------------------------------------------\n|                        export                        |\n\u2014-------------------------------------------------------\n                            |\n                    FX Graph in ATen     Backend Specific Quantizer\n                            |                       /\n\u2014--------------------------------------------------------\n|                     prepare_pt2e                      |\n\u2014--------------------------------------------------------\n                            |\n                     Calibrate/Train\n                            |\n\u2014--------------------------------------------------------\n|                    convert_pt2e                       |\n\u2014--------------------------------------------------------\n                            |\n                    Quantized Model\n                            |\n\u2014--------------------------------------------------------\n|                       Lowering                        |\n\u2014--------------------------------------------------------\n                            |\n        Executorch, Inductor or &lt;Other Backends&gt;\n</code></pre>","tags":["documentation"]},{"location":"100%20Reference%20notes/104%20Other/PyTorch%20-%20Quantization/","title":"PyTorch   Quantization","text":"Properties authors PyTorch Quantization for TensorRT year 2024 url https://pytorch.org/docs/main/quantization.html#prototype-pytorch-2-export-quantization","tags":["documentation"]},{"location":"100%20Reference%20notes/104%20Other/PyTorch%20-%20Quantization/#backendhardware-support","title":"Backend/Hardware Support","text":"Hardware Kernel Library Eager Mode Quantization FX Graph Mode Quantization Quantization Mode Support server CPU fbgemm/onednn Supported All Supported mobile CPU qnnpack/xnnpack server GPU TensorRT (early prototype) Not support this it requires a graph Supported Static Quantization <p>Today, PyTorch supports the following backends for running quantized operators efficiently:</p> <ul> <li>x86 CPUs with AVX2 support or higher (without AVX2 some operations have inefficient implementations), via\u00a0x86\u00a0optimized by\u00a0fbgemm\u00a0and\u00a0onednn\u00a0(see the details at\u00a0RFC)</li> <li>ARM CPUs (typically found in mobile/embedded devices), via\u00a0qnnpack</li> <li>(early prototype) support for NVidia GPU via\u00a0TensorRT\u00a0through\u00a0fx2trt\u00a0(to be open sourced)</li> </ul> <p>Note: - This is a bit old, as fx2trt is already available in torch-tensorrt. However, there </p>","tags":["documentation"]},{"location":"100%20Reference%20notes/104%20Other/PyTorch%20Compilers%20-%20What%20makes%20PyTorch%20beloved%20makes%20it%20hard%20to%20compile/","title":"PyTorch Compilers   What makes PyTorch beloved makes it hard to compile","text":"Properties authors Peng Wu year 2022 url https://chips-compilers-mlsys-22.github.io/assets/slides/PyTorch%20Compilers%20(Compiler%20&amp;%20Chips%20Symposium%202022).pdf <p>Multiple pytorch compilers - TorchScript (torch.jit.script, torch.jit.trace)     - supports python subset     - full graph capture = Ahead-of-Time (AOT) Compilation     - executed by TS interpreter - nnc, nvfuser - torch.fx - torch.package, torch.deploy - torch-mlir - TorchDynamo, TorchInductor     - TorchDynamo captures partial graphs (if strict=False), and falls-back to eager.</p> <p>What makes TorchDynamo graph capture sound and out-of-the-box? - Partial graph capture: Ability to skip unwanted parts of eager - Guarded graphs: Ability to check if captured graph is valid for execution     - Note: Basically, it inserts assertions/runtime checks to see that the partial graph is sound at runtime, if not, it jit recompiles. - Just-in-time recapture: recapture a graph if captured graph is invalid for execution</p> <p>Dynamo workflow - Captures FX Graph - Sends FX Graph to compiler hook to compile (which can be another compiler like TRT or torchscript)</p> <p>{ width=\"800\" }</p> <p>Note: tbh this seems like an arbitrary separation, because torchdynamo also is meant for inference (torch.export), but this is probably because this tutorial is 2 years old</p>","tags":["presentation"]},{"location":"100%20Reference%20notes/104%20Other/PyTorch%20Conference%202024%20-%20Fast%20Sparse%20Vision%20Transformers%20with%20minimal%20accuracy%20loss/","title":"PyTorch Conference 2024   Fast Sparse Vision Transformers with minimal accuracy loss","text":"Properties authors Jesse Cai year 2024 url https://static.sched.com/hosted_files/pytorch2024/c6/Sparsifying%20ViT%20lightning%20talk%20slides.pdf?_gl=119zah9b_gcl_auMTk3MjgxODE5OC4xNzI3MjU4NDM2FPAU*MTk3MjgxODE5OC4xNzI3MjU4NDM2 <p>Nice, it is on <code>torchao</code></p> <p></p> <p></p> <p></p> <p> Notes: - Don't quite understand what does Core or AO mean in this context, but at least <code>torch.compile</code> is acknowledged :p</p>","tags":["presentation"]},{"location":"100%20Reference%20notes/104%20Other/PyTorch%20Conference%202024%20-%20What%E2%80%99s%20new%20in%20torch.export%3F/","title":"PyTorch Conference 2024   What\u2019s new in torch.export?","text":"Properties authors Avik Chaudhuri year 2024 url https://static.sched.com/hosted_files/pytorch2024/6b/What%E2%80%99s%20new%20in%20torch.export_.pptx.pdf?_gl=11s5cwnu_gcl_au*MTk3MjgxODE5OC4xNzI3MjU4NDM2","tags":["presentation"]},{"location":"100%20Reference%20notes/104%20Other/PyTorch%20Conference%202024%20-%20What%E2%80%99s%20new%20in%20torch.export%3F/#recap-what-is-torchexport-and-why","title":"[Recap] What is torch.export and why?","text":"<ul> <li>\"Sound\", whole-graph capture of pytorch models</li> <li>Emits \"IR\": backend-agnostic</li> <li>For easier backend-specific lowering (trt, etc)</li> <li>For python-free environments</li> </ul>","tags":["presentation"]},{"location":"100%20Reference%20notes/104%20Other/PyTorch%20Conference%202024%20-%20What%E2%80%99s%20new%20in%20torch.export%3F/#composable-apis","title":"Composable APIs","text":"<ul> <li>Useful: torch.export.export_for_inference </li> </ul>","tags":["presentation"]},{"location":"100%20Reference%20notes/104%20Other/PyTorch%20Conference%202024/","title":"PyTorch Conference 2024","text":"Properties year 2024 <p>Some interesting talks for #efficient_dl : - PyTorch Conference 2024 - What\u2019s new in torch.export? - PyTorch Conference 2024 - Fast Sparse Vision Transformers with minimal accuracy loss</p>","tags":["conference"]},{"location":"100%20Reference%20notes/104%20Other/PyTorch%20Eager%20Mode%20Quantization%20TensorRT%20Acceleration/","title":"PyTorch Eager Mode Quantization TensorRT Acceleration","text":"Properties authors Lei Mao year 2024 url https://leimao.github.io/blog/PyTorch-Eager-Mode-Quantization-TensorRT-Acceleration/ <p>Abstract</p> <p>The TensorRT acceleration for the quantized PyTorch model from the PyTorch eager mode quantization interface involves three steps:</p> <ol> <li>Perform PyTorch eager mode quantization on the floating-point PyTorch model in PyTorch and export the quantized PyTorch model to ONNX.</li> <li>Fix the quantized ONNX model graph so that it can be parsed by the TensorRT parser.</li> <li>Build the quantized ONNX model to a TensorRT engine, profile the performance, and verify the accuracy.&gt; 1</li> </ol> <p>The source code for this post can be found on GitHub .</p>","tags":["website","paper"]},{"location":"100%20Reference%20notes/104%20Other/PyTorch%20internals/","title":"PyTorch internals","text":"Properties authors Edward Z. Yang year 2019 url http://blog.ezyang.com/2019/05/pytorch-internals/ <p>Depending on tensor metadata (if it's CUDA, or sparse, etc) it's dispatched to different implementations () { width=\"500\" }</p>","tags":["blog"]},{"location":"100%20Reference%20notes/104%20Other/Quantized%20Transfer%20Learning%20for%20Computer%20Vision%20Tutorial/","title":"Quantized Transfer Learning for Computer Vision Tutorial","text":"Properties authors Zafar Takhirov url https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html","tags":["website"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%2010/","title":"Reinforcement Learning   An Introduction   Chapter 10","text":"Properties authors Richard S. Sutton, Andrew G. Barton year 2018","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%2010/#10-on-policy-control-with-approximation","title":"10 On-Policy Control with Approximation","text":"<p>Now that we know how to learn value functions, we can tackle the control problem by learning q-value functions instead and using a \\(\\epsilon\\)-greedy policy over those.</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%2010/#101-episodic-semi-gradient-control","title":"10.1 Episodic Semi-gradient Control","text":"<p>Equation 10.1: General gradient-descent update for action-value prediction</p> \\[ \\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha \\left[U_t - \\hat{q}(S_t, A_t, \\mathbf{w}_t) \\right] \\nabla \\hat{q}(S_t, A_t, \\mathbf{w}_t) \\tag{10.1} \\] <p>Equation 10.2: Episodic semi-gradient one-step SARSA</p> \\[ \\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha \\left[R_{t+1} + \\gamma \\hat{q}(S_{t+1}, A_{t+1}, \\mathbf{w}_t) - \\hat{q}(S_t, A_t, \\mathbf{w}_t) \\right] \\nabla \\hat{q}(S_t, A_t, \\mathbf{w}_t) \\tag{10.2} \\] <p>{ width=\"700\" }</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%2011/","title":"Reinforcement Learning   An Introduction   Chapter 11","text":"Properties authors Richard S. Sutton, Andrew G. Barton year 2018","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%2011/#11-off-policy-methods-with-approximation","title":"11 Off-policy Methods with Approximation","text":"<p>Off-policy case is a bit tricky, previously Reinforcement Learning - An Introduction - Chapter 5 we adjusted our target \\(G_t\\) with the importance sampling ratio so that its expectation was \\(v_{\\pi}\\) and not \\(v_b\\). </p> <p>However, for semi-gradient methods with function approximation, another factor comes in: By following the behavior policy, the distribution of updates is also biased! </p> <p>There are 2 solutions: 1. Importance sampling - Corrects the distribution of updates 2. True gradient methods - To avoid relying on the distribution for stability</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%2011/#111-semi-gradient-methods","title":"11.1 Semi-gradient Methods","text":"<p>Equation 11.1: Per-step importance sampling ratio</p> \\[ \\rho_t \\doteq \\rho_{t:T-1} = \\frac{\\pi(A_t \\mid S_t)}{b(A_t \\mid S_t)} \\] What does a high/low importance sampling ratio imply about the action taken? <ul> <li>High: The action was taken rarely under the behavior policy</li> <li>Low: The action was taken often under the behavior policy</li> </ul> <p>Equation 11.2: Update rule for semi-gradient off-policy TD(0)</p> \\[ \\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha \\rho_t \\delta_t \\nabla \\hat{v}(S_t, \\mathbf{w}_t) \\tag{11.2} \\] <p>Equation 11.3: Definition of \\(\\delta_t\\) for semi-gradient off-policy TD(0)</p> \\[ \\delta_t \\doteq R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}_t) - \\hat{v}(S_t, \\mathbf{w}_t) \\tag{11.3} \\] <p>Equation 11.5: Update rule for semi-gradient off-policy SARSA</p> \\[ \\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha \\rho_t \\delta_t \\nabla \\hat{q}(S_t, A_t, \\mathbf{w}_t) \\tag{11.5} \\] <p>Equation 11.6: Definition of \\(\\delta_t\\) for semi-gradient off-policy SARSA</p> \\[ \\delta_t \\doteq R_{t+1} + \\gamma \\sum_a \\pi(a \\mid S_{t+1}) \\hat{q}(S_{t+1}, a, \\mathbf{w}_t) - \\hat{q}(S_t, A_t, \\mathbf{w}_t) \\tag{11.6} \\] <p>Also: - eq 11.6: n-step version of semi-gradient Sarsa - eq 11.7: semi-gradient version of n-step tree-backup algorithm</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%2011/#112-examples-of-off-policy-divergence","title":"11.2 Examples of Off-policy Divergence","text":"<p>Baird's counterexample: Shows that even the simplest  bootstrapping (DP, TD) with function approximation can diverge if updates aren't done under on-policy distribution.</p> <p>Another way to try to prevent instability is to use special methods for function approximation. In particular, stability is guaranteed for function approximation methods that do not extrapolate from the observed targets. These methods, called averagers, include nearest neighbor methods and locally weighted regression, but not popular methods such as tile coding and artificial neural networks (ANNs).</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%2011/#113-the-deadly-triad","title":"11.3 The Deadly Triad","text":"<p>Instability arises when we combine the following 3 elements: 1. Function approximation 2. Bootstrapping 3. Off-policy training</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%2011/#114-linear-value-function-geometry","title":"11.4 Linear Value-function Geometry","text":"<p>TLDR: Using the geometry of the value function, we find that \\(\\overline{BE}\\) measures how far off \\(\\mathbf{w}\\) is from \\(v_\\pi\\).</p> <p>Equation 11.11: \\(\\mu\\)-norm</p> \\[ ||\\mathbf{v}||^2_\\mu \\doteq \\sum_{s \\in \\mathcal{S}} \\mu(s) v(s)^2 \\] <p>Equation 11.17 and 11.18: Bellman error</p> \\[ \\begin{align} \\bar{\\delta}_{\\mathbf{w}}(s) &amp;\\doteq \\left( \\sum_a \\pi(a \\mid s) \\sum_{s', r} p(s', r \\mid s, a)[r + \\gamma v_{\\mathbf{w}}(s')] \\right) - v_{\\mathbf{w}}(s) \\tag{11.17} \\\\ &amp;= \\mathbb{E}_\\pi[R_{t+1} - \\gamma v_{\\mathbf{w}}(S_{t+1}) - v_{\\mathbf{w}}(S_{t}) \\mid S_t = s, A_t \\sim \\pi] \\end{align} \\] <p>Equation 11.19: Mean-square Bellman error</p> \\[ \\overline{BE}({\\mathbf{w}}) \\doteq || \\bar{\\delta} ||^2_\\mu \\] <p>With linear function approximation there always exists an approximate value function (within the subspace) with zero PBE; this is the TD fixed point, wTD</p> <p>Equation 11.13: Projection matrix for linear function approximation</p> \\[ \\Pi \\doteq \\mathbf{X} (\\mathbf{X}^\\top \\mathbf{D} \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{D} \\] <p>Where:</p> <ul> <li>\\(\\mathbf{X} \\in \\mathbb{R}^{|\\mathcal{S}| \\times d}\\) is the matrix of feature vectors</li> <li>\\(\\mathbf{D} \\in \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{S}|}\\) is a diagonal matrix with \\(\\mu(s)\\) on the diagonal</li> </ul> <p>Equation 11.22: Mean square Projected Bellman error</p> \\[ \\overline{PBE} = || \\Pi \\bar{\\delta}_{\\mathbf{w}} ||^2_\\mu \\] <p>Where:</p> <ul> <li>\\(\\Pi\\) is the projection matrix</li> <li>\\(\\bar{\\delta}_{\\mathbf{w}}\\) is the Bellman error</li> </ul>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%2011/#115-gradient-descent-in-the-bellman-error","title":"11.5 Gradient Descent in the Bellman Error","text":"<p>TLDR: Semi-gradient SGD might diverge, but true SGD doesn't! Sadly, both TDE and BE yield bad minima.</p> <p>Let's first take an easier case, minimizing \\(\\overline{TDE}\\).</p> <p>Mean-squared temporal difference error</p> \\[ \\begin{align} \\overline{TDE}(\\mathbf{w}) &amp;= \\sum_{s \\in \\mathcal{S}} \\mu(s) \\mathbb{E}\\left[\\delta_t^2 \\mid S_t = s, A_t \\sim \\pi \\right] \\\\ &amp;= \\sum_{s \\in \\mathcal{S}} \\mu(s) \\mathbb{E}\\left[\\rho_t \\delta_t^2 \\mid S_t = s, A_t \\sim b \\right] \\\\ &amp;= \\mathbb{E}_b\\left[\\rho_t \\delta_t^2 \\right] \\end{align} \\] <p>This yields the following.</p> <p>Equation 11.23: Weight update of naive residual-gradient algorithm</p> \\[ \\begin{align} \\mathbf{w}_{t+1} &amp;= \\mathbf{w}_t - \\frac{1}{2} \\alpha \\nabla(\\rho_t \\delta_t^2) \\\\ &amp;= \\mathbf{w}_t - \\alpha \\rho_t \\delta_t \\nabla(\\delta_t) \\\\ &amp;= \\mathbf{w}_t - \\alpha \\rho_t \\delta_t (\\nabla \\hat{v}(S_t, \\mathbf{w}_t) - \\gamma \\nabla \\hat{v}(S_{t+1}, \\mathbf{w}_t)) \\tag{11.23} \\\\ \\end{align} \\] <p>Conclusion: </p> <p>Minimizing the TDE is naive; by penalizing all TD errors it achieves something more like temporal smoothing than accurate prediction. Although the naive residual-gradient algorithm converges robustly, it does not necessarily converge to a desirable place.</p> <p>Doing the same for \\(\\overline{BE}\\) yields the residual-gradient algorithm. But it's not a good choice either.</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%2011/#116-the-bellman-error-is-not-learnable","title":"11.6 The Bellman Error is Not Learnable","text":"<p>TLDR: \\(\\overline{BE}\\) is not learnable but \\(\\overline{TDE}\\)  and \\(\\overline{PBE}\\) are. Since minimizing \\(\\overline{TDE}\\) is naive, next section: minimize \\(\\overline{PBE}\\).</p> <p>Here we use the term in a more basic way, to mean learnable at all, with any amount of experience. It turns out many quantities of apparent interest in reinforcement learning cannot be learned even from an infinite amount of experiential data. These quantities are well defined and can be computed given knowledge of the internal structure of the environment, but cannot be computed or estimated from the observed sequence of feature vectors, actions, and rewards</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%2011/#117-gradient-td-methods","title":"11.7 Gradient-TD Methods","text":"<p>TLDR: To minimize \\(\\overline{PBE}\\) using SGD efficiently we use two separate estimates for dependent expectations. This yields two algorithms: GTD2 and TDC. </p> <p>DISCLAIMER: These methods only work with linear function approximation.</p> <p>Equation 11.27: Gradient of \\(\\overline{PBE}\\)</p> \\[ \\nabla \\overline{PBE}(\\mathbf{w}) = 2 \\mathbb{E} \\left[ \\rho_t (\\gamma_{t+1} - \\mathbf{x}_t) \\mathbf{x}_t^\\top \\right] \\mathbb{E} \\left[ \\mathbf{x}_t \\mathbf{x}_t^\\top \\right]^{-1} \\mathbb{E} \\left[ \\rho_t \\delta_t \\mathbf{x}_t \\right] \\] <p>First and last term are not independent, thus we must have separate estimates.  </p> <p>Equation 11.28: Definition of \\(\\mathbf{v}\\)</p> <p>Grouping the last two terms of the gradient of \\(\\overline{PBE}\\), we get a new vector \\(\\mathbf{v}\\in \\mathbb{R}^d\\) which we can estimate and store efficiently:</p> \\[ \\mathbf{v} \\approx \\mathbb{E} \\left[ \\mathbf{x}_t \\mathbf{x}_t^\\top \\right]^{-1} \\mathbb{E} \\left[ \\rho_t \\delta_t \\mathbf{x}_t \\right] \\] <p>Update rule for \\(\\mathbf{v}\\)</p> \\[ \\mathbf{v}_{t+1} = \\mathbf{v}_t + \\beta \\rho_t (\\delta_t - \\mathbf{v}_t^\\top \\mathbf{x}_t) \\mathbf{x}_t \\] <p>Where:</p> <ul> <li>\\(\\beta\\) is a learning rate</li> </ul> <p>Using \\(\\mathbf{v}\\), we can now update \\(\\mathbf{w}\\), which yields the GTD2 algorithm:</p> <p>Equation 11.29: Update rule for GTD2</p> \\[ \\begin{align}    \\mathbf{w}_{t+1} &amp;= \\mathbf{w}_t - \\frac{1}{2}\\alpha \\nabla \\text{PBE}(\\mathbf{w}_t) \\quad \\tag{the general SGD rule} \\\\    &amp;= \\mathbf{w}_t - \\frac{1}{2}\\alpha 2 \\mathbb{E} \\left[ \\rho_t (\\gamma_{t+1} - \\mathbf{x}_t) \\mathbf{x}_t^\\top \\right] \\mathbb{E} \\left[ \\mathbf{x}_t \\mathbf{x}_t^\\top \\right]^{-1} \\mathbb{E} \\left[ \\rho_t \\delta_t \\mathbf{x}_t \\right] \\quad  \\tag{from (11.27)} \\\\    &amp;= \\mathbf{w}_t + \\alpha \\mathbb{E} \\left[ \\rho_t (\\mathbf{x}_t - \\gamma \\mathbf{x}_{t+1}) \\mathbf{x}_t^\\top \\right] \\mathbb{E} \\left[ \\mathbf{x}_t \\mathbf{x}_t^\\top \\right]^{-1} \\mathbb{E} \\left[ \\rho_t \\delta_t \\mathbf{x}_t \\right] \\quad \\tag{11.29} \\\\    &amp;\\approx \\mathbf{w}_t + \\alpha \\mathbb{E} \\left[ \\rho_t (\\mathbf{x}_t - \\gamma \\mathbf{x}_{t+1}) \\mathbf{x}_t^\\top \\right] \\mathbf{v}_t \\quad \\tag{based on (11.28)} \\\\    &amp;\\approx \\mathbf{w}_t + \\alpha \\rho_t (\\mathbf{x}_t - \\gamma \\mathbf{x}_{t+1}) \\mathbf{x}_t^\\top \\mathbf{v}_t \\quad \\tag{sampling} \\end{align} \\] What is the complexity of GTD2? <p>\\(O(d)\\) per step if \\(x_t^T v_t\\) computed first</p> <p>We can do a bit more algebra to get to a better algorithm: TDC (TD(0) with Gradient Correction) or also known as GTD(0).</p> <p>Equation: Update rule for TDC</p> \\[ \\begin{align}    \\mathbf{w}_{t+1} &amp;= \\mathbf{w}_t + \\alpha \\mathbb{E} \\left[ \\rho_t (\\mathbf{x}_t - \\gamma \\mathbf{x}_{t+1}) \\mathbf{x}_t^\\top \\right] \\mathbb{E} \\left[ \\mathbf{x}_t \\mathbf{x}_t^\\top \\right]^{-1} \\mathbb{E} \\left[ \\rho_t \\delta_t \\mathbf{x}_t \\right] \\\\    &amp;= \\mathbf{w}_t + \\alpha \\left( \\mathbb{E} \\left[ \\rho_t \\mathbf{x}_t \\mathbf{x}_t^\\top \\right] - \\gamma \\mathbb{E} \\left[ \\rho_t \\mathbf{x}_{t+1} \\mathbf{x}_t^\\top \\right] \\right) \\mathbb{E} \\left[ \\mathbf{x}_t \\mathbf{x}_t^\\top \\right]^{-1} \\mathbb{E} \\left[ \\rho_t \\delta_t \\mathbf{x}_t \\right] \\\\    &amp;= \\mathbf{w}_t + \\alpha \\left( \\mathbb{E} \\left[ \\mathbf{x}_t \\mathbf{x}_t^\\top \\right] - \\gamma \\mathbb{E} \\left[ \\rho_t \\mathbf{x}_{t+1} \\mathbf{x}_t^\\top \\right] \\right) \\mathbb{E} \\left[ \\mathbf{x}_t \\mathbf{x}_t^\\top \\right]^{-1} \\mathbb{E} \\left[ \\rho_t \\delta_t \\mathbf{x}_t \\right] \\\\    &amp;= \\mathbf{w}_t + \\alpha \\left( \\mathbb{E} \\left[ \\rho_t \\delta_t \\mathbf{x}_t \\right] - \\gamma \\mathbb{E} \\left[ \\rho_t \\mathbf{x}_{t+1} \\mathbf{x}_t^\\top \\right] \\mathbb{E} \\left[ \\mathbf{x}_t \\mathbf{x}_t^\\top \\right]^{-1} \\mathbb{E} \\left[ \\rho_t \\delta_t \\mathbf{x}_t \\right] \\right) \\\\    &amp;\\approx \\mathbf{w}_t + \\alpha \\left( \\mathbb{E} \\left[ \\rho_t \\delta_t \\mathbf{x}_t \\right] - \\gamma \\mathbb{E} \\left[ \\rho_t \\mathbf{x}_{t+1} \\mathbf{x}_t^\\top \\right] \\mathbf{v}_t \\right) \\quad &amp;\\text{(based on (11.28))} \\\\    &amp;\\approx \\mathbf{w}_t + \\alpha \\rho_t \\left( \\delta_t \\mathbf{x}_t - \\gamma \\mathbf{x}_{t+1} \\mathbf{x}_t^\\top \\mathbf{v}_t \\right), \\quad &amp;\\text{(sampling)} \\end{align} \\] <p>Summary from the lectures</p> <p>{ width=\"700\" }</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%2013/","title":"Reinforcement Learning   An Introduction   Chapter 13","text":"Properties authors Richard S. Sutton, Andrew G. Barton year 2018","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%2013/#13-policy-gradient-methods","title":"13 Policy Gradient Methods","text":"<ul> <li>Value-based methods: learn action-value estimates.</li> <li>Policy Gradient methods: learn parameterized policies, \\(\\pi(a \\mid s, \\boldsymbol{\\theta})\\). <ul> <li>(policies that don't consult a value function). </li> </ul> </li> <li>Actor-critic methods: learn both.<ul> <li>Actor \\(\\to\\) policy</li> <li>Critic \\(\\to\\) value function</li> </ul> </li> </ul> <p>Notation: - \\(\\boldsymbol{\\theta}\\): policy parameters - \\(\\mathbf{w}\\) : value function parameters</p> How do we learn parametrized policies? <ul> <li>0-order: random search, grid search, heuristics</li> <li>1-order: use first-order derivative (gradient)</li> <li>2-order: user second-order statistics (hessian, etc)  [ref:slides]</li> </ul> <p>Equation 13.1: Gradient ascent update of policy parameters</p> \\[ \\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t + \\alpha \\widehat{\\nabla J(\\boldsymbol{\\theta})} \\tag{13.1} \\]","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%2013/#131-policy-approximation-and-its-advantages","title":"13.1 Policy Approximation and its Advantages","text":"What is in practice enforced to ensure exploration for PGMs? <p>That the policy never becomes deterministic.</p> <p>Equation 13.2: Soft-max in action preferences</p> \\[ \\pi(a \\mid s, \\boldsymbol{\\theta}) \\doteq \\frac{e^{h(s, a, \\boldsymbol{\\theta})}}{\\sum_{b} e^{h(s, b, \\boldsymbol{\\theta})}} \\tag{13.2} \\] Which conditions must the problem fulfill in order to reasonably consider the Soft-max in action preferences parametrization? <p>The action space must be discrete and small enough.</p> <p>Equation 13.3: Linear parametrization of policy preferences \\(h\\)</p> \\[ h(s, a, \\boldsymbol{\\theta}) \\doteq \\boldsymbol{\\theta}^\\intercal \\mathbf{x}(s, a) \\tag{13.3} \\] <p>where \\(\\mathbf{x}(s, a)\\) is a feature vector that describes the state-action pair.</p> What are the advantage of parametrizing policies according to the soft-max in action preferences in comparison with \\(\\epsilon-\\)greedy policies? <ol> <li>Enables policies to approach a deterministic policy. \\(\\epsilon\\)-greedy policies always maintain a minimum non-greedy probability \\(\\epsilon\\).</li> <li>It enables truly stochastic policies. \\(\\epsilon\\)-greedy policies force the policy to be almost greedy, but sometimes the best policy is to do \\(x\\) with probability \\(p\\) and \\(y\\) with probability \\(1-p\\) (e.g. poker bluffing). </li> </ol> What is the most important reasons for using policy gradient methods instead of value-based methods? <ol> <li>They allow you to inject prior knowledge about the desired form of the policy. (ref:book)</li> <li>Ensures smooth updates of the policy. (ref:book/slides)</li> <li>Allows continuous action spaces. (ref:slides)</li> <li>Allows for stochastic policies. (ref:slides)</li> </ol> How do we inject inductive biases into the policy parametrization? <ul> <li>Policy form (e.g, gaussian, etc)</li> <li>Initialization</li> </ul>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%2013/#132-the-policy-gradient-theorem","title":"13.2 The Policy Gradient Theorem","text":"How does continuous policy parametrization help convergence? Compare it to VBMs. <p>With VBMs, a small change in value function could drastically change the policy. With PGMs, a small change in policy parameters will only change the policy slightly, which helps optimization.</p> <p>Equation 13.4: Performance \\(J(\\boldsymbol{\\theta})\\) for the episodic case</p> \\[ J(\\boldsymbol{\\theta}) \\doteq v_{\\pi_{\\boldsymbol{\\theta}}}(s_0) \\tag{13.4} = \\mathbb{E}_{\\pi_{\\boldsymbol{\\theta}}}[G_0] \\] What problem/question does the Policy Gradient Theorem answer? <p>How can we estimate the performance gradient with respect to the policy parameter when the gradient depends on the unknown effect of policy changes on the state distribution?</p> <p>The policy gradient theorem allows for an estimate of the performance gradient irrespective of the derivative of the state distribution.</p> <p>Equation 13.5: Policy gradient theorem</p> \\[ \\nabla J(\\boldsymbol{\\theta}) \\propto \\sum_s \\mu(s) \\sum_a q_{\\pi}(s, a) \\nabla \\pi(a \\mid s, \\boldsymbol{\\theta}) \\tag{13.5} \\]","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%2013/#133-reinforce-monte-carlo-policy-gradient","title":"13.3 REINFORCE: Monte Carlo Policy Gradient","text":"<p>Equation 13.6: All-actions policy gradient</p> \\[ \\begin{align}  \\nabla J(\\boldsymbol{\\theta}) &amp;\\propto \\sum_s \\mu(s) \\sum_a q_{\\pi}(s, a) \\nabla \\pi(a \\mid s, \\boldsymbol{\\theta}) \\\\  &amp;= \\mathbb{E}_{\\pi} \\left[ \\sum_a q_{\\pi}(S_t, a) \\nabla \\pi(a \\mid S_t, \\boldsymbol{\\theta}) \\right] \\tag{13.6} \\\\ \\end{align} \\] <p>Personal note about notation:</p> <ul> <li>\\(\\mathbb{E}_{\\pi}\\) is a bit misleading because it can have two interpretations:</li> <li>\\(\\mathbb{E}_{A_t \\sim \\pi \\mid S_t = s}\\): Expectation over actions given a state.</li> <li>\\(\\mathbb{E}_{S_t \\sim \\mu}\\): Expectation over the states given the on-policy distribution \\(\\mu\\). Since this distribution depends on the policy and the purpose of this expectation is to be sampled through experience for SGD, the abuse of notation is understandable.<ul> <li>To further this point, \\(\\mathbb{E}_{\\pi}[f]\\) effectively means: The experience gathered from the policy \\(\\pi\\) to update the parameters through SGD will correctly weigh \\(f\\) eventually.</li> </ul> </li> </ul> <p>Equation 13.7: All-actions policy gradient update rule</p> \\[ \\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t + \\alpha \\sum_a \\hat{q}(S_t, a, \\mathbf{w}) \\nabla \\pi(a \\mid S_t, \\boldsymbol{\\theta}) \\tag{13.7} \\] <p>Derivation of the REINFORCE gradient:</p> \\[ \\begin{align*} \\nabla J(\\boldsymbol{\\theta}) &amp;\\propto \\mathbb{E}_{\\pi} \\left[ \\sum_a \\pi(a | S_t, \\boldsymbol{\\theta}) q_{\\pi}(S_t, a) \\frac{\\nabla \\pi(a | S_t, \\boldsymbol{\\theta})}{\\pi(a | S_t, \\boldsymbol{\\theta})} \\right] \\\\ &amp;= \\mathbb{E}_{\\pi} \\left[ q_{\\pi}(S_t, A_t) \\frac{\\nabla \\pi(A_t | S_t, \\boldsymbol{\\theta})}{\\pi(A_t | S_t, \\boldsymbol{\\theta})} \\right] \\quad \\text{(replacing } a \\text{ by the sample } A_t \\sim \\pi) \\\\ &amp;= \\mathbb{E}_{\\pi} \\left[ G_t \\frac{\\nabla \\pi(A_t | S_t, \\boldsymbol{\\theta})}{\\pi(A_t | S_t, \\boldsymbol{\\theta})} \\right], \\quad \\text{(because } \\mathbb{E}_{\\pi}[G_t | S_t, A_t] = q_{\\pi}(S_t, A_t)) \\end{align*} \\] <p>Equation 13.8: REINFORCE update rule</p> \\[ \\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t + \\alpha G_t \\frac{\\nabla \\pi(A_t | S_t, \\boldsymbol{\\theta})}{\\pi(A_t | S_t, \\boldsymbol{\\theta})} \\tag{13.8} \\] <p>This choice of performance measure \\(G_t \\frac{ \\nabla \\pi(A_t | S_t, \\boldsymbol{\\theta})}{\\pi(A_t | S_t, \\boldsymbol{\\theta})}\\) is intuitive:</p> <ul> <li>\\(\\nabla \\pi(A_t | S_t, \\boldsymbol{\\theta})\\): Is the direction that maximizes the probability of selecting \\(A_t\\) in state \\(S_t\\).</li> <li>This gradient is proportional to \\(G_t\\), which means that the larger the return, the larger the update.</li> <li>This gradient is inversely proportional to \\(\\pi(A_t | S_t, \\boldsymbol{\\theta})\\), which means that the larger the probability of selecting \\(A_t\\), the smaller the update. <ul> <li>This is importance because it prevents frequency bias: actions should be chosen not because they are frequent, but because they have high return.</li> </ul> </li> </ul> <p>This vector is called the eligibility vector. </p> <p> Note: - For simplicity, \\(\\frac{\\nabla \\pi(A_t | S_t, \\boldsymbol{\\theta})}{\\pi(A_t | S_t, \\boldsymbol{\\theta})}\\) is written as \\(\\nabla \\ln \\pi(A_t | S_t, \\boldsymbol{\\theta})\\). - REINFORCE has good theoretical convergence properties.</p> Why does REINFORCE yield slow learning? <p>Because as a Monte Carlo method, it has high variance.</p> <p>DISCLAIMER: For uva-rl1, this method is called REINFORCE v2. REINFORCE v1 uses the full return at each step</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%2013/#134-reinforce-with-baseline","title":"13.4 REINFORCE with Baseline","text":"<p>The policy gradient can be generalized to include any baseline function \\(b(s)\\), as long as it is independent of the action. </p> <p>Equation 13.10: Baseline policy gradient</p> \\[ \\nabla J(\\boldsymbol{\\theta}) \\propto \\sum_s \\mu(s) \\sum_a \\left( q_{\\pi}(s, a) - b(s) \\right) \\nabla \\pi(a \\mid s, \\boldsymbol{\\theta}) \\tag{13.10} \\] Why is \\(b(s)\\)'s independence to \\(a\\) a requirement for this generalization to be valid? <p>If \\(b(s)\\) is independent of \\(a\\), then the term involving \\(b(s)\\) will zero-out:</p> \\[ \\sum_a b(s) \\nabla \\pi(a \\mid s, \\boldsymbol{\\theta}) = b(s) \\nabla \\sum_a \\pi(a \\mid s, \\boldsymbol{\\theta}) = 0 \\] <p>Equation 13.11: Baseline policy gradient update rule</p> \\[ \\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t + \\alpha \\left( G_t - b(S_t) \\right) \\nabla \\pi(A_t \\mid S_t, \\boldsymbol{\\theta}) \\tag{13.11} \\] <ul> <li>In general, the baseline leaves the expected value of the update unchanged, but affects the variance.<ul> <li>Think about as a normalization, the baseline can act as a way to zero-mean the value distribution per each state, thus helping the learning algorithm to differentiate better between bad and good actions relative to each state's particular action-value distribution.</li> </ul> </li> <li>Natural choice: \\(b(s) = \\hat{v}(S_t, \\mathbf{w})\\).</li> </ul> <p> - Setting the learning rate for \\(\\mathbf{w}\\) is as normal, but for \\(\\boldsymbol{\\theta}\\), it is not obvious.</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%2013/#135-actor-critic-methods","title":"13.5 Actor-Critic Methods","text":"<p>TLDR: Expand usage of the baseline/value function with multi-step returns, lambda TD, etc. Helps with variance. Adds bias but can be controlled with lambda TD, etc.</p> <p>2 changes from REINFORCE:  - Use value function as baseline - Use one-step returns with value bootstrap as target</p> <p>Equation 13.12, 13.13 and 13.14: One-step Actor-critic update rule</p> \\[ \\begin{align}    \\theta_{t+1} &amp;\\doteq \\theta_t + \\alpha \\left( G_{t:t+1} - \\hat{v}(S_t, \\mathbf{w}) \\right) \\frac{\\nabla \\pi(A_t | S_t, \\theta_t)}{\\pi(A_t | S_t, \\theta_t)} \\quad \\tag{13.12} \\\\    &amp;= \\theta_t + \\alpha \\left( R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}) - \\hat{v}(S_t, \\mathbf{w}) \\right) \\frac{\\nabla \\pi(A_t | S_t, \\theta_t)}{\\pi(A_t | S_t, \\theta_t)} \\quad \\tag{13.13} \\\\    &amp;= \\theta_t + \\alpha \\delta_t \\frac{\\nabla \\pi(A_t | S_t, \\theta_t)}{\\pi(A_t | S_t, \\theta_t)} \\quad \\tag{13.14} \\end{align} \\] <p>{ width=\"800\" }</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%2013/#136-policy-gradient-for-continuing-problems","title":"13.6 Policy Gradient for Continuing Problems","text":"<p>Equation 13.15: Average rate of reward per time step</p> \\[ \\begin{align}     J(\\boldsymbol{\\theta}) &amp;\\doteq  r(\\pi) \\doteq \\lim_{h \\to \\infty} \\frac{1}{h} \\sum_{t=1}^h\\mathbb{E} \\left[ R_t \\mid S_t, A_{0:t-1} \\sim \\pi \\right] \\tag{13.15} \\\\     &amp;= \\lim_{t \\to \\infty} \\mathbb{E} \\left[R_t \\mid S_t, A_{0:t-1} \\sim \\pi \\right] \\\\\\     &amp;= \\sum_s \\mu(s) \\sum_a \\pi(a \\mid s) \\sum_{s', r} p(s', r \\mid s, a) r \\end{align} \\] <p>Where:</p> <ul> <li>\\(\\mu(s) \\doteq \\lim_{t\\to \\infty} \\mathbb{P} \\left[S_t = s \\mid A_{0:t} \\sim \\pi \\right]\\) is the steady-state distribution of states under \\(\\pi\\), which is assumed to exist and to be independent of \\(S_0\\) (an ergodicity assumption).</li> </ul> <p>Note: not part of the course readings, missing remaining notes for this subsection.</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%2013/#137-policy-parameterization-for-continuous-actions","title":"13.7 Policy Parameterization for Continuous Actions","text":"<p>TLDR: parametrize policy by a distribution statistics, for example, mean and variance of gaussian.</p> <p>todo: add notes</p> <p>\\(d \\tau\\)</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%2013/#extra-deterministic-policy-gradients","title":"Extra: Deterministic Policy Gradients","text":"<ul> <li>Use deterministic policy as target policy</li> <li>Use stochastic policy as behavior policy (example: target + noise)</li> </ul> <p>{ width=\"500\" }</p> <p>{ width=\"500\" }</p> <p>DPG with q-learning update { width=\"500\" }</p> <p>Only works with continous actions Discrete actions will cause gradient inconsistencies</p> <p>{ width=\"500\" }</p> <p>Deep DPG = DPG + modification to use neural nets to generalise - Use experience replay - \"double-q learning\"</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%2016/","title":"Reinforcement Learning   An Introduction   Chapter 16","text":"Properties authors Richard S. Sutton, Andrew G. Barton year 2018","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%2016/#16-applications-and-case-studies","title":"16 Applications and Case Studies","text":"","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%2016/#165-human-level-video-game-play","title":"16.5 Human-level Video Game Play","text":"<p>DQN's Network architecture: Conv2d + RELU blocks  for feature extraction and linear layers for output. </p> <p>Equation 16.3: DQN Semi-Gradient update rule</p> \\[ \\mathbf{w}_{t+1} = \\mathbf{w}_{t} + \\alpha \\left[ R_{t+1} + \\gamma \\max_{a} \\hat{q}(S_{t+1}, a; \\mathbf{w}_{t}) - \\hat{q}(S_t, A_t; \\mathbf{w}_{t}) \\right] \\nabla \\hat{q}(S_t, A_t; \\mathbf{w}_{t}) \\] What are the three modifications to Q-learning that make DQN? <ol> <li>Experience Replay: Useful to use data better and remove the dependence of successive experiences on the current weights.</li> <li>\"Double Q-learning\": Keep a copy of the network at the previous step to provide targets to avoid divergence and oscillations.</li> <li>Clip the error term \\(R_{t+1} + \\gamma \\max_{a} q(S_{t+1}, a; \\mathbf{w}_{t}) - q(S_t, A_t; \\mathbf{w}_{t})\\) to \\([-1, 1]\\) to improve stability.</li> </ol>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%202/","title":"Reinforcement Learning   An Introduction   Chapter 2","text":"Properties authors Richard S. Sutton, Andrew G. Barton year 2018","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%202/#2-multi-armed-bandits","title":"2 Multi-armed Bandits","text":"","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%202/#22-action-value-methods","title":"2.2 Action-value Methods","text":"<p>Equation 2.1: Sample-average Method</p> \\[ Q_t(a) \\doteq \\frac{\\text{sum of rewards when } a \\text{ taken prior to } t}{\\text{number of times } a \\text{ taken prior to } t} = \\frac{\\sum_{i=1}^{t-1} R_i \\cdot \\mathbb{1}_{A_i = a}}{\\sum_{i=1}^{t-1} \\mathbb{1}_{A_i = a}} \\tag{2.1} \\] <p>Equation 2.2: Greedy Action Selection</p> \\[ A_t \\doteq \\underset{a}{\\arg\\max} Q_t(a) \\tag{2.2} \\]","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%202/#24-incremental-implementation","title":"2.4 Incremental Implementation","text":"<p>Equation 2.4: Incremental Sample-average method</p> \\[ Q_{n+1} = Q_n + \\frac{1}{n}[R_n - Q_n] \\tag{2.4} \\] <p>Where:</p> <ul> <li>\\(Q_1\\)  is usually initialized to zero.</li> <li>\\(R_n\\) is the reward received after the \\(n\\)-th selection of action \\(a\\)</li> <li>\\(Q_n\\) denote the estimate of its action value after it has been selected \\(n-1\\) times</li> </ul>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%202/#25-tracking-a-nonstationary-problem","title":"2.5 Tracking a Nonstationary Problem","text":"<p>Equation 2.7: Two learning rate conditions to ensure convergence</p> \\[ \\sum_{n=1}^{\\infty} \\alpha_n(a) = \\infty \\quad \\text{and} \\quad \\sum_{n=1}^{\\infty} \\alpha_n^2(a) &lt; \\infty \\tag{2.7} \\]","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%202/#26-optimistic-initial-values","title":"2.6 Optimistic Initial Values","text":"<p>TLDR: Initializing \\(Q_1(a)\\) to a positive non-zero number encourages exploration.</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%202/#27-upper-confidence-bound-action-selection","title":"2.7 Upper-Confidence-Bound Action Selection","text":"<p>Equation 2.10: UCB action selection</p> \\[ A_t \\doteq \\underset{a}{\\arg\\max} \\left[ Q_t(a) + c \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right] \\tag{2.10} \\] <p>Where:</p> <ul> <li>\\(c &gt; 0\\) controls the degree of exploration.</li> </ul>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%203/","title":"Reinforcement Learning   An Introduction   Chapter 3","text":"Properties authors Richard S. Sutton, Andrew G. Barton year 2018","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%203/#31-the-agent-environment-interface","title":"3.1 The Agent-Environment Interface","text":"<p>Equation 3.1: Trajectory</p> \\[  S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,R_3, \\dots  \\tag{3.1} \\] <p>Equation 3.2: MDP dynamics</p> \\[  p(s', r \\mid s, a) \\doteq \\Pr \\{ S_t = s', R_t = r \\mid S_{t-1} = s, A_{t-1} = a \\} \\tag{3.2} \\] <p>You can obtain the state-transition probabilities and the with the law of total probability. You can obtain the expected reward also.</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%203/#32-goals-and-rewards","title":"3.2  Goals and Rewards","text":"What is the reward hypothesis? <p>The reward hypothesis is the idea that all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward). </p> <ul> <li>The reward signal is your way of communicating to the agent what you want it to achieve not how you want it to achieve it.</li> </ul>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%203/#33-returns-and-episodes","title":"3.3 Returns and Episodes","text":"<p>Equation 3.7: Undiscounted return</p> \\[ G_t \\doteq R_{t+1} + R_{t+2} + R_{t+3} + \\dots + R_T \\tag{3.7} \\] <p>Equation 3.8: Discounted return</p> \\[ G_t \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\tag{3.8} \\] <p>Where \\(\\gamma\\) is the discount rate.</p> <p>Equation 3.9: Recursive definition of return</p> <p>You can group Eq 3.8 into a recursive definition of the return.</p> \\[ G_t \\doteq R_{t+1} + \\gamma G_{t+1} \\tag{3.9} \\]","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%203/#34-unified-notation-for-episodic-and-continuing-tasks","title":"3.4 Unified Notation for Episodic and Continuing Tasks","text":"","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%203/#35-policies-and-value-functions","title":"3.5 Policies and Value Functions","text":"<p>A policy \\(\\pi(a \\mid s)\\) is a probability distribution over actions given states.</p> <p>Equation 3.12: State-value function</p> \\[ v_{\\pi}(s) \\doteq \\mathbb{E}_{\\pi}[G_t \\mid S_t = s] \\;\\; \\forall s \\in \\mathcal{S} \\tag{3.12} \\] <p>Equation 3.13: Action-value function</p> \\[ q_{\\pi}(s, a) \\doteq \\mathbb{E}_{\\pi}[G_t \\mid S_t = s, A_t = a] \\;\\; \\forall s \\in \\mathcal{S}, a \\in \\mathcal{A} \\tag{3.13} \\] <p>Writing \\(v_{\\pi}\\) in terms of \\(q_{\\pi}\\)</p> \\[ v_{\\pi}(s) = \\sum_{a} \\pi(a \\mid s) q_{\\pi}(s, a) \\] <p>Equation 3.14: Bellman equation for \\(v_{\\pi}\\)</p> \\[ \\begin{align} v_\\pi(s) &amp;\\doteq \\mathbb{E}_{\\pi}[G_t \\mid S_t = s] \\\\ &amp;= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} \\mid S_t = s] \\tag{by (3.9)} \\\\ &amp;= \\sum_{a} \\pi(a \\mid s) \\sum_{s', r} p(s', r \\mid s, a) \\left[r + \\gamma \\mathbb{E}_{\\pi}\\left[G_{t+1} \\mid S_{t+1} = s'\\right]\\right] \\\\ &amp;= \\sum_{a} \\pi(a \\mid s) \\sum_{s', r} p(s', r \\mid s, a) [r + \\gamma v_\\pi(s')] \\tag{3.14} \\end{align} \\]","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%203/#36-optimal-policies-and-optimal-value-functions","title":"3.6 Optimal Policies and Optimal Value Functions","text":"<p>A policy \\(\\pi^*\\) is the optimal policy if:</p> \\[ v_{\\pi^*} (s) \\geq v_{\\pi'}(s) \\quad \\forall s, \\pi' \\in \\mathcal{S} \\times \\mathcal{\\Pi} \\] <p>Equation 3.15: Optimal state-value function</p> \\[ v_*(s) \\doteq \\max_{\\pi} v_{\\pi}(s) \\tag{3.15} \\] <p>Equation 3.16: Optimal action-value function</p> \\[ q_*(s, a) \\doteq \\max_{\\pi} q_{\\pi}(s, a) \\tag{3.16} \\] <p>Equation 3.17</p> \\[ q_*(s, a) = \\mathbb{E}[R_{t+1} + \\gamma v_*(S_{t+1}) \\mid S_t = s, A_t = a] \\tag{3.17} \\] <p>Equation 3.18 and 3.19: Bellman optimality equations for \\(v_*\\)</p> \\[ \\begin{align} v_*(s) &amp;= \\max_{a \\in \\mathcal{A}(s)} q_{\\pi_*}(s, a) \\\\ &amp;= \\max_{a} \\mathbb{E}_{\\pi_*}[G_t \\mid S_t = s, A_t = a] \\tag{by (3.9)}\\\\ &amp;= \\max_{a} \\mathbb{E}_{\\pi_*}[R_{t+1} + \\gamma G_{t+1} \\mid S_t = s, A_t = a] \\\\ &amp;= \\max_{a} \\mathbb{E}[R_{t+1} + \\gamma v_*(S_{t+1}) \\mid S_t = s, A_t = a] \\tag{3.18} \\\\ &amp;= \\max_{a} \\sum_{s', r} p(s', r \\mid s, a) [r + \\gamma v_*(s')] \\tag{3.19} \\\\ \\end{align} \\] <p>Equation 3.20: Bellman optimality equation for \\(q_*\\)</p> \\[ \\begin{align} q_*(s, a) &amp;= \\mathbb{E}[R_{t+1} + \\gamma \\max_{a'} q_*(S_{t+1}, a') \\mid S_t = s, A_t = a] \\\\ &amp;= \\sum_{s', r} p(s', r \\mid s, a) [r + \\gamma \\max_{a'} q_*(s', a')] \\tag{3.20} \\end{align} \\] <p>Any policy that is greedy with respect to the optimal evaluation function \\(v_*\\) is an optimal policy.</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%204/","title":"Reinforcement Learning   An Introduction   Chapter 4","text":"Properties authors Richard S. Sutton, Andrew G. Barton year 2018","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%204/#4-dynamic-programming","title":"4 Dynamic Programming","text":"","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%204/#41-policy-evaluation","title":"4.1 Policy evaluation","text":"<p>Equations 4.3 and 4.4</p> \\[ \\begin{align} v_{\\pi}(s) &amp;\\doteq \\mathbb{E}_{\\pi}[G_t \\mid S_t = s] \\\\ &amp;= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} \\mid S_t = s] &amp;&amp; (\\text{from (3.9)})\\\\ &amp;= \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) \\mid S_t = s] &amp;&amp; (4.3)\\\\ &amp;= \\sum_a \\pi(a \\mid s) \\sum_{s',r} p(s', r \\mid s, a) \\left[ r + \\gamma v_{\\pi}(s') \\right] &amp;&amp; (4.4), \\end{align} \\] <p>Equation 4.5</p> \\[ \\begin{align} v_{k+1}(s) &amp;\\doteq \\mathbb{E}_{\\pi} [ R_{t+1} + \\gamma v_k(S_{t+1}) \\mid S_t = s ] \\\\ &amp; = \\sum_a \\pi(a \\mid s) \\sum_{s', r} p(s', r \\mid s, a) \\left[ r + \\gamma v_k(s') \\right] &amp;&amp; (4.5), \\end{align} \\] <p>{ width=\"600\" }</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%204/#42-policy-improvement","title":"4.2 Policy Improvement","text":"<p>Equation 4.6</p> \\[ \\begin{align} q_\\pi(s, a) &amp;\\doteq  \\mathbb{E}[R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\mid S_t = s, A_t = a] &amp;&amp; (4.6)\\\\             &amp;= \\sum_{s', r}p(s', r \\mid s, a)[r + \\gamma v_\\pi(s')] \\\\  \\end{align} \\]","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%204/#43-policy-iteration","title":"4.3 Policy Iteration","text":"<p>{ width=\"600\" }</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%204/#44-value-iteration","title":"4.4 Value Iteration","text":"<p>\"This algorithm is called value iteration. It can be written as a particularly simple update operation that combines the policy improvement and truncated policy evaluation steps.\"</p> <p>Equation 4.10</p> \\[ \\begin{align} v_{k+1} &amp;\\doteq \\max_{a} \\mathbb{E} [R_{t+1} + \\gamma v_k(S_{t+1}) \\mid S_t =s, A = a] \\\\         &amp;= \\max_{a} \\sum_{s', r}p(s', r \\mid s, a)[r + \\gamma v_k(s')] &amp;&amp; (4.10) \\\\  \\end{align} \\] <p>{ width=\"600\" }</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%204/#45-asynchronous-dynamic-programming","title":"4.5 Asynchronous Dynamic Programming","text":"<p>\"These algorithms update the values of states in any order whatsoever, using whatever values of other states happen to be available. [...] To converge correctly, however, an asynchronous algorithm must continue to update the values of all the states: it can\u2019t ignore any state after some point in the computation. Asynchronous DP algorithms allow great flexibility in selecting states to update.\"</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%205/","title":"Reinforcement Learning   An Introduction   Chapter 5","text":"Properties authors Richard S. Sutton, Andrew G. Barton year 2018","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%205/#51-monte-carlo-prediction","title":"5.1 Monte Carlo prediction","text":"<p>first-visit mc - independence assumptions, easier theoretically every-visit mc</p> <ul> <li> TODO: finish notes</li> </ul>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%205/#53-monte-carlo-control","title":"5.3 Monte Carlo Control","text":"<p>{ width=\"600\" }</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%205/#54-monte-carlo-control-without-exploring-starts","title":"5.4 Monte Carlo Control without Exploring Starts","text":"<ul> <li> <p>\\(\\epsilon-\\)greedy policy</p> <ul> <li>All non-greedy actions have minimum probability of \\(\\frac{\\epsilon}{|\\mathcal{A}|}\\)</li> <li>Greedy action has probability \\((1 - \\epsilon) + \\frac{\\epsilon}{|\\mathcal{A}|}\\)</li> </ul> </li> <li> <p> TODO: finish notes { width=\"600\" }</p> </li> </ul>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%205/#55-off-policy-prediction-via-importance-sampling","title":"5.5 Off-policy Prediction via Importance Sampling","text":"<p>Given a starting state \\(S_t\\), the probability of the subsequent state-action trajectory, \\(A_t, S_{t+1}, A_{t+1}, \\dots, S_T\\), under the policy \\(\\pi\\) is given by:</p> \\[ \\begin{align} Pr\\{A_t, S_{t+1}, A_{t+1}, \\dots, S_T \\mid S_t, A_{t:T-1} \\sim \\pi\\} &amp; = \\prod_{k=t}^{T-1} \\pi(A_k \\mid S_k) p(S_{k+1} \\mid S_k, A_k) \\end{align} \\] <p>Equation 5.3: Important sampling ratio</p> \\[ \\rho_{t:T-1} \\doteq \\frac{\\prod_{k=t}^{T-1} \\pi(A_k \\mid S_k) p(S_{k+1} \\mid S_k, A_k)}{\\prod_{k=t}^{T-1} b(A_k \\mid S_k) p(S_{k+1} \\mid S_k, A_k)} = \\prod_{k=t}^{T-1} \\frac{\\pi(A_k \\mid S_k)}{b(A_k \\mid S_k)} \\tag{5.3} \\] <p>Equation 5.4: Value function for target function \\(\\pi\\) under behavior policy \\(b\\)</p> <p>The importance sampling ratio allows us to compute the correct expected value to compute \\(v_\\pi\\):</p> \\[ \\begin{align} v_\\pi(s) &amp;\\doteq \\mathbb{E}_b[\\rho_{t:T - 1}G_t \\mid S_t = s] \\tag{5.4} \\\\ \\end{align} \\] <p>Equation 5.5: Ordinary importance sampling</p> \\[ V(s) \\doteq \\frac{\\sum_{t \\in \\mathcal{T}(s)} \\rho_{t:T-1} G_t}{|\\mathcal{T}(s)|} \\tag{5.5} \\] <p>Equation 5.6: Weighted importance sampling</p> \\[ V(s) \\doteq \\frac{\\sum_{t \\in \\mathcal{T}(s)} \\rho_{t:T-1} G_t}{\\sum_{t \\in \\mathcal{T}(s)} \\rho_{t:T-1}} \\tag{5.6} \\] <p>{ width=\"600\" }</p> <p>In practice, weighted importance sampling has much lower error at the beginning.</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%205/#56-incremental-implementation","title":"5.6 Incremental Implementation","text":"","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%205/#todo","title":"todo","text":"","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%206/","title":"Reinforcement Learning   An Introduction   Chapter 6","text":"Properties authors Richard S. Sutton, Andrew G. Barton year 2018","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%206/#6-temporal-difference-learning","title":"6 Temporal-Difference Learning","text":"","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%206/#61-td-prediction","title":"6.1 TD Prediction","text":"<p>Equation 6.2: TD(0) update</p> \\[ \\begin{align} V(S_t) &amp;\\leftarrow V(S_t) + \\alpha \\left[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right]  \\tag{6.2}  \\\\ \\end{align} \\] <p>Equations 6.3 and 6.4: Relationship between TD(0), MC and DP</p> \\[ \\begin{align} v_\\pi(s) &amp;\\doteq \\mathbb{E}_\\pi[G_t \\mid S_t = s] \\tag{6.3} \\\\ &amp;= \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1} \\mid S_t = s]  \\tag{from (3.9)} \\\\ &amp;= \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\mid S_t = s] \\tag{6.4} \\\\ \\end{align} \\] Why is (6.3) called the Monte Carlo estimate? <p>Because the expected value is not known, and sampled returns are used in its place.</p> Why is (6.4) called the Dynamic Programming estimate? <p>Although the expectation is known, the value function is not, as we use the estimate \\(V(S_t)\\).</p> By looking at the previous two answers, what does TD(0) estimate and how does that differ from the previous methods? <p>TD(0) maintains both an estimate of the value function and uses a sample reward as the estimate to the expectation.</p> <p>Equation 6.5: TD error</p> \\[ \\begin{align} \\delta_t &amp;\\doteq R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\tag{6.5}  \\end{align} \\]","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%206/#64-sarsa-on-policy-td-control","title":"6.4 Sarsa: On-policy TD Control","text":"<p>Equation 6.7</p> \\[ \\begin{align} Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \\right]   \\end{align} \\] <p>{ width=\"900\" }</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%206/#65-q-learning-off-policy-td-control","title":"6.5 Q-learning: Off-policy TD Control","text":"<p>Equation 6.8</p> \\[ \\begin{align} Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a) - Q(S_t, A_t) \\right]   \\end{align} \\] <p>{ width=\"700\" }</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%206/#66-expected-sarsa","title":"6.6 Expected SARSA","text":"<p>Equation 6.9</p> \\[ \\begin{align} Q(S_t, A_t) &amp;\\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\mathbb{E}_\\pi [Q(S_{t+1}, A_{t+1}) \\mid S_{t+1}] - Q(S_t, A_t) \\right]   \\\\ &amp;= Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\sum_a \\pi(a \\mid S_{t+1}) Q(S_{t+1}, a)  - Q(S_t, A_t) \\right]  &amp;&amp; (6.9) \\end{align} \\] <p>It's more computationally demanding but it's more stable and fares better than q learning and sarsa.</p> <p>Can also be used as is for off-policy case.</p> Why doesn't Expected SARSA off-policy need importance sampling? <p>I wasn't convinced by the slides explanation, so I'll have to check a proper explanation later.</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%206/#67-maximization-bias-and-double-learning","title":"6.7 Maximization Bias and Double Learning","text":"<p>\"All the control algorithms that we have discussed so far involve maximization in the construction of their target policies\"</p> <p>this causes maximization bias: - think of estimating the mean of N(-0.1, 1) - this estimate might at some point be 0.1 and the other option might be correctly 0 - the optimal choice is to pick 0, but because we take the max of an estimate, we positively bias ourselves</p> <p>The general way to solve it is to estimate two different value functions, one for getting the value (\\(Q_2\\)) and the other for obtaining the best action \\(Q_1\\).</p> \\[ \\begin{align} A^* &amp;= \\text{argmax}_a Q_1(a) \\\\ Q_2(A^*) &amp;= Q_2(\\text{argmax}_a Q_1(a)) \\end{align} \\] <p>This effectively debiases the estimate \\(\\mathbb{E}[Q_2(A^*)] = q(A^*)\\)</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%207/","title":"Reinforcement Learning   An Introduction   Chapter 7","text":"Properties authors Richard S. Sutton, Andrew G. Barton year 2018","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%207/#71-n-step-td-prediction","title":"7.1 \\(n\\)-step TD prediction","text":"<p>One-step return:</p> \\[ G_{t:t+1} \\doteq R_{t+1} + \\gamma V_t(S_{t+1}) \\] <p>Equation 7.1: \\(n\\)-step return</p> \\[ G_{t:t+n} \\doteq R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^{n-1} R_{t+n} + \\gamma^n V_{t + n - 1}(S_{t+n}) \\tag{7.1} \\]","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%208/","title":"Reinforcement Learning   An Introduction   Chapter 8","text":"Properties authors Richard S. Sutton, Andrew G. Barton year 2018","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%208/#planning-and-learning-with-tabular-methods","title":"Planning and Learning with Tabular Methods","text":"","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%208/#81-models-and-planning","title":"8.1 Models and Planning","text":"","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%208/#82-dyna-integrated-planning-acting-and-learning","title":"8.2 Dyna: Integrated Planning, Acting, and Learning","text":"","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%209/","title":"Reinforcement Learning   An Introduction   Chapter 9","text":"Properties authors Richard S. Sutton, Andrew G. Barton year 2018","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%209/#9-on-policy-prediction-with-approximation","title":"9. On-policy  prediction with approximation","text":"<p>Problem setting: In most real scenarios, the number of states is too large for tabular learning algorithms, so we will approximate the value function by a learned, parametrized function: \\(\\(\\hat{v}(s, \\mathbf{w}) \\approx v_\\pi(s)\\)\\) - Examples of possible modelling choices for this function could be linear functions, non linear functions, neural networks, etc. - \\(\\mathbf{w} \\in R^d\\) , \\(d \\ll |\\mathcal{S}|\\) , which means that updating on state affects multiple: generalization - This formulation allows for partially observable states. - Side note: not all convergence proofs apply to all function classes (for more info see UCL x DeepMind 7/13)</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%209/#91-value-function-approximation","title":"9.1 Value-function approximation","text":"<p>New notation! (\\(s\\to u\\) is an update rule for \\(v(s)\\) using new expression \\(u\\))  </p> How does the learning setting differ between neural networks (supervised) and reinforcement learning? <p>RL requires modeling to allow:</p> <ul> <li>online learning (while interacting with environment), incrementally acquire data<ul> <li>Remember that supervised learning suffers from catastrophic forgetting</li> </ul> </li> <li>Non-stationary target functions</li> </ul> <p>Supervised Learning assumes iid sampling from a fixed but unknown data distribution</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%209/#92-the-prediction-objective-overlineve","title":"9.2 The Prediction Objective (\\(\\overline{VE}\\))","text":"Why do we need a prediction objective now? What has changed? <p>In the tabular setting we had two nice properties:</p> <ul> <li>the learned value function could actually converge exactly to the true value function</li> <li>the value of a state was decoupled from other states</li> </ul> <p>Without these two, we must say which states are most important to us.</p> <p>Equation 9.1: Value Error</p> \\[ \\begin{align} \\overline{VE}(\\mathbf{w}) &amp;\\doteq \\sum_{s \\in \\mathcal{S}}  \\mu(s) \\left[v_{\\pi}(s) - \\hat{v}(s, \\mathbf{w})\\right]^2 &amp;&amp; \\tag{9.1} \\end{align} \\] <p>Where: - \\(\\mu(s)\\) is the state distribution (reminder: non-negative, sums to one)</p> <p>For on-policy episodic tasks, \\(\\mu(s)\\) is called the on-policy distribution, which can be defined as follows:</p> <p>Equations 9.2 and 9.3</p> \\[ \\begin{align} \\eta(s) = h(s) + \\sum_{\\bar{s}} \\eta(\\bar{s}) \\sum_a \\pi(a \\mid \\bar{s})p(s \\mid \\bar{s}, a), &amp;&amp; \\text{for all } s \\in S  &amp;&amp; \\tag{9.2} \\end{align} \\] \\[ \\begin{align} \\mu(s) = \\frac{\\eta(s)}{\\sum_{s'}\\eta(s')} &amp;&amp; \\tag{9.3} \\end{align} \\] <p>Where: -  \\(h(s)\\) is the probability that an episode begins in a state \\(s\\). - \\(\\eta(s)\\) is the number of time steps spent on average in a state \\(s\\) for a single episode.     - Interpretation of 2 terms: Time is spent in a \\(s\\) if an episode starts in \\(s\\) or if another state transitions into \\(s\\).</p> <ul> <li>\\(\\overline{VE}\\) only guarantees local optimality.</li> </ul>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%209/#93-stochastic-gradient-and-semi-gradient-methods","title":"9.3 Stochastic-gradient and Semi-gradient Methods","text":"<p>Equations 9.4 and 9.5</p> \\[ \\begin{align}     \\mathbf{w}_{t+1} &amp;= \\mathbf{w}_t - \\frac{1}{2} \\alpha \\nabla \\left[v_{\\pi}(S_t) - \\hat{v}(S_t, \\mathbf{w}_t) \\right] &amp;&amp; \\tag{9.4} \\\\      &amp;= \\mathbf{w}_t + \\alpha \\left[v_{\\pi}(S_t) - \\hat{v}(S_t, \\mathbf{w}_t) \\right] \\nabla \\hat{v}(S_t, \\mathbf{w}_t) &amp;&amp; \\tag{9.5} \\end{align} \\] <p>However, since we don't know the true  \\(v_\\pi(s)\\), we can replace it with the target output \\(U_t\\): </p> <p>Equation 9.7</p> \\[ \\begin{align}     \\mathbf{w}_{t+1} &amp;= \\mathbf{w}_t + \\alpha \\left[U_t - \\hat{v}(S_t, \\mathbf{w}_t) \\right] \\nabla \\hat{v}(S_t, \\mathbf{w}_t) &amp;&amp; \\tag{9.7} \\end{align} \\] <p>Where: - \\(U_t\\) should be an unbiased estimate of \\(v_\\pi(s)\\), that is:     - \\(\\mathbb{E}[U_t \\mid S_t=s] = v_\\pi(s)\\)     - With local optimum convergence guarantees.</p> <p>{ width=\"800\" }</p> <p>Examples of \\(U_t\\): - Monte Carlo target: \\(U_t = G_t\\)  (that is, the reward achieved until the end of the episode), unbiased. -  Bootstrapping targets are biased because they depend on \\(\\mathbf{w}\\) through \\(\\hat{v}(S_t, \\mathbf{w})\\) .     - To make them unbiased, you can treat the dependent expressions as constants (stop the gradient flow). This yields semi-gradient methods.</p> <p>Semi-gradient methods: - Do not converge as robustly as gradient methods, aside from the linear case. - Faster, enable online/continual learning.</p> <p>{ width=\"800\" }</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%209/#94-linear-methods","title":"9.4 Linear Methods","text":"<p>Equation 9.8</p> \\[ \\begin{align}     \\hat{v}(s, \\mathbf{w}) \\doteq \\mathbf{w}^\\intercal \\mathbf{x}(s) = \\sum_{i=1}^d w_i x_i(s) &amp;&amp; \\tag{9.8} \\end{align} \\] <p>Where:</p> <ul> <li>\\(\\mathbf{x}(s) = \\left(x_1(s), \\dots, x_d(s)\\right)^\\intercal\\)</li> </ul> <ul> <li>The gradient Monte Carlo algorithm converges to the global optimum of the VE under linear function approximation if \\(\\alpha\\) is reduced over time according to the usual conditions.</li> <li>Chapter also explores the convergence of TD(0) with SGD and linear approximation and finds it converges to the TD fixed point (Eqs. 9.11, 9.12), \\(\\mathbf{w}_{TD}\\). This is not the global optimum, but a point near the local optimum. </li> </ul> <p>Equation 9.11 and 9.12: TD fixed point</p> <p>Semi-gradient TD(0) under linear approximation converges to the TD fixed point:</p> \\[ \\mathbf{w}_{TD} = \\mathbf{A}^{-1}\\mathbf{b} \\tag{9.12} \\] <p>Where: $$ \\mathbf{b} \\doteq \\mathbb{E} \\left[ R_{t+1} \\mathbf{x}t \\right] \\in \\mathbb{R}^d \\quad \\text{and} \\quad \\mathbf{A} \\doteq \\mathbb{E} \\left[ \\mathbf{x}_t (\\mathbf{x}_t - \\gamma \\mathbf{x}{t+1})^\\intercal \\right] \\in \\mathbb{R}^{d \\times d} \\tag{9.11} $$</p> Is \\(\\mathbf{w}_{TD}\\) the minimiser of \\(\\overline{VE}\\)? <p>No, but it is a point near a local optimum w.r.t \\(\\overline{VE}\\).</p> What are the convergence guarantees of semi-gradient TD(0) with non-linear features? <p>No guarantees.</p> When should you choose Semi-gradient TD(0) over Gradient Monte Carlo? <p>Since Semi-gradient TD(0) learns faster, it is better when there is a fixed computational budget. However, if you can train for longer, Gradient Monte Carlo is better.</p> <p>Equation 9.14</p> <p>Interpretation: The asymptotic error of the TD method is no more than \\(\\frac{1}{1-\\gamma}\\) times the smallest possible error.</p> \\[ \\begin{align}     \\overline{VE}(\\mathbf{w}_{TD}) &amp; \\leq \\frac{1}{1-\\gamma} \\min_{\\mathbf{w}} \\overline{VE}(\\mathbf{w}) \\tag{9.14} \\end{align} \\] <p>{ width=\"800\" }</p> <p>Equation 9.15</p> \\[ \\mathbf{w}_{t+n} \\doteq \\mathbf{w}_{t+n-1} + \\alpha \\left[ G_{t:t+n} - \\hat{v}(S_t, \\mathbf{w}_{t+n-1}) \\right] \\nabla \\hat{v}(S_t, \\mathbf{w}_{t+n-1}), \\quad 0 \\leq t &lt; T, \\tag{9.15} \\] <p>Equation 9.16</p> \\[ G_{t:t+n} \\doteq R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{n-1} R_{t+n} + \\gamma^n \\hat{v}(S_{t+n}, \\mathbf{w}_{t+n-1}), \\quad 0 \\leq t \\leq T - n. \\tag{9.16} \\]","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%209/#95-feature-construction-for-linear-methods","title":"9.5 Feature Construction for Linear Methods","text":"<ul> <li>9.5.1 Polynomials</li> <li>9.5.2 Fourier Basis</li> <li>9.5.3 Coarse coding</li> <li>9.5.4 Tile Coding</li> <li>9.5.5 Radial Basis Functions</li> </ul>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%209/#96-selecting-step-size-parameters-manually","title":"9.6 Selecting Step-Size Parameters Manually","text":"<p>\u201cThe classical choice t =1/t, which produces sample averages in tabular MC methods, is not appropriate for TD methods, for nonstationary problems, or for any method using function approximation.\u201d (Sutton and Barto, 2020, p. 244)</p> <p>Equation 9.19</p> <p>Suppose you wanted to learn in about \\(\\tau\\) experiences with substantially the same feature vector. A good rule of thumb for setting the step-size parameter of linear SGD methods is:</p> \\[ \\begin{align}     \\alpha \\doteq \\left(\\tau \\mathbb{E}\\left[\\mathbf{x}^\\intercal\\mathbf{x}\\right]\\right)^{-1} \\tag{9.19} \\end{align} \\]","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%209/#97-nonlinear-function-approximation-artificial-neural-networks","title":"9.7 Nonlinear Function Approximation: Artificial Neural Networks","text":"<p>TLDR: Using neural networks for function approximation. Discusses common techniques: architecture, dropout, batchnorm,  etc.</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction%20-%20Chapter%209/#98-least-squares-td","title":"9.8 Least-Squares TD","text":"<p>Equation 9.20 and 9.21: LSTD update</p> \\[ \\mathbf{w}_t \\doteq \\widehat{\\mathbf{A}}_t^{-1}\\widehat{\\mathbf{b}}_t \\tag{9.21} \\] <p>Where:</p> \\[ \\widehat{\\mathbf{A}}_t \\doteq \\sum_{k=0}^{t-1} \\mathbf{x}_k(\\mathbf{x}_k - \\gamma \\mathbf{x}_{k+1})^\\intercal + \\epsilon \\mathbf{I} \\quad \\text{and} \\quad  \\widehat{\\mathbf{b}}_t \\doteq \\sum_{k=0}^{t-1} \\mathbf{x}_k R_{k+1} \\tag{9.20} \\] Shouldn't both sums be divided by \\(t\\) like normal sample averages? <p>No, the \\(t\\) term cancels out in the update.</p> Why id there a \\(\\epsilon \\mathbf{I}\\) term in the LSTD update? <p>This is a regularization term to ensure the matrix is invertible.</p> What is the computational complexity of LSTD? <p>By naively computing the inverse, it is \\(O(d^3)\\), but there are more efficient methods.</p> How can we improve the computational complexity of LSTD? <p>By using the Sherman-Morrison formula, we can reduce the complexity to \\(O(d^2)\\). This method allows us to iteratively compute \\(\\widehat{\\mathbf{A}}_t^{-1}\\). from \\(\\widehat{\\mathbf{A}}_{t-1}^{-1}\\) in \\(O(d^2)\\) time.</p> Does LSTD require specifying any hyperparameters (e.g. step size)? <p>Yes, but not step size. The only hyperparameter is \\(\\epsilon\\). It has a similar effect to step size: If \\(\\epsilon\\) is too small, the inverses will vary a lot and if \\(\\epsilon\\) is  is too large, learning will be slow.</p> How does LSTD compare to Semi-Gradient TD? Name 4 differences. <ol> <li>More sample efficient.</li> <li>Requires more computation: \\(O(d^2)\\) vs \\(O(d)\\).</li> <li>Does not require a step-size parameter, only \\(\\epsilon\\).</li> <li>LSTD never forgets. </li> </ol> <p>{ width=\"800\" } - Note: Details not on exam</p>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/Reinforcement%20Learning%20-%20An%20Introduction/","title":"Reinforcement Learning   An Introduction","text":"Properties authors Richard S. Sutton, Andrew G. Barton year 2018 <ul> <li>Reinforcement Learning - An Introduction - Chapter 3</li> <li>Reinforcement Learning - An Introduction - Chapter 4</li> <li>Reinforcement Learning - An Introduction - Chapter 6</li> <li>Reinforcement Learning - An Introduction - Chapter 9</li> </ul>","tags":["textbook","rl1"]},{"location":"100%20Reference%20notes/104%20Other/TinyML%20and%20Efficient%20Deep%20Learning%20Computing%20-%20Lecture%2012/","title":"TinyML and Efficient Deep Learning Computing   Lecture 12","text":"Properties authors Song Han year 2023 url https://www.dropbox.com/scl/fi/spgvr9owflz6s1lt5po17/lec12.pdf?rlkey=cwqpteopgvsdgnxd8xtcniadr&amp;e=2&amp;dl=0","tags":["lecture"]},{"location":"100%20Reference%20notes/104%20Other/TinyML%20and%20Efficient%20Deep%20Learning%20Computing%20-%20Lecture%203/","title":"TinyML and Efficient Deep Learning Computing   Lecture 3","text":"Properties authors Song Han year 2023 url https://www.dropbox.com/scl/fi/2oxmtvoeccyuw47yfambb/lec03.pdf?rlkey=3ykm0g21ibsoqn7xnw43v7aaw&amp;e=1&amp;dl=0","tags":["lecture"]},{"location":"100%20Reference%20notes/104%20Other/TinyML%20and%20Efficient%20Deep%20Learning%20Computing%20-%20Lecture%205/","title":"TinyML and Efficient Deep Learning Computing   Lecture 5","text":"Properties authors Song Han year 2023 url https://www.dropbox.com/scl/fi/eos92o2fgys6gk0gizogl/lec05.pdf?rlkey=2hohvi8jcvjw3f8m8vugfa2mz&amp;e=1&amp;dl=0 <p>Content: 1. Reviews numeric datatypes (floating point, etc) 2. Learns basic concept of quantization 3. Introduces three types of common neural network quantization:     - K-Means-based Quantization     - Linear Quantization     - [[Binary and Ternary Quantization|Binary and Ternary Quantization]] (will be covered on Lecture 6)</p>","tags":["lecture"]},{"location":"100%20Reference%20notes/104%20Other/TinyML%20and%20Efficient%20Deep%20Learning%20Computing%20-%20Lecture%206/","title":"TinyML and Efficient Deep Learning Computing   Lecture 6","text":"Properties authors Song Han year 2023 url https://www.dropbox.com/scl/fi/1mo0umu0qtq7uxap2l5m3/lec06.pdf?rlkey=bdl2mgusgajddjuvjxb0fot36&amp;e=2&amp;dl=0 <p>Content: 1. Quantization Granularity     1. Per tensor quantization: same quantization parameters for the entire matrix     2. Per channel quantization: sometimes each channels have considerably different weight distributions, have different quantization parameters per channel/row     3. Group quantization: similar idea 2. Dynamic Range Clipping     - To quantize activations, we must keep track of activations statistics     - Use KL divergence to measure information loss     - Allocating dynamic range to outliers hurts representation ability (see below image)     -  3. Rounding</p> <p>Quantization Aware Training - To minimize the loss of accuracy, especially aggressive quantization with 4 bits and lower bit width, neural network will be trained/fine-tuned with quantized weights and activations. - Usually, fine-tuning a pre-trained floating point model provides better accuracy than training from scratch.</p>","tags":["lecture"]},{"location":"100%20Reference%20notes/104%20Other/TinyML%20and%20Efficient%20Deep%20Learning%20Computing/","title":"TinyML and Efficient Deep Learning Computing","text":"Properties authors Song Han year 2023 url https://hanlab.mit.edu/courses/2023-fall-65940","tags":["course"]},{"location":"100%20Reference%20notes/104%20Other/Tweet%20-%20Stable%20Diffusion%20XL%20on%20iPhone%20with%20Core%20ML%21/","title":"Tweet   Stable Diffusion XL on iPhone with Core ML!","text":"Properties authors Atila Orhon year 2023 url https://x.com/atiorh/status/1707402410870862002 <p>We compressed the diffusion model using our Mixed-Bit Palettization technique (described in https://huggingface.co/blog/stable-diffusion-xl-coreml\u2026) which yields an average of 4.04-bits (5.2GB -&gt; 1.3GB) while maintaining higher accuracy than linear 8-bit quantization. Compressed model runs faster too</p> <p>Notes - 4 times smaller memory footprint - Better than linear 8-bit quantization - Faster inference time</p>","tags":["efficient_dl","tweet"]}]}