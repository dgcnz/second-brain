---
authors:
  - "[[Prabhant Singh|Prabhant Singh]]"
  - "[[Sibylle Hess|Sibylle Hess]]"
  - "[[Joaquin Vanschoren|Joaquin Vanschoren]]"
year: 2025
tags:
  - paper
url: http://arxiv.org/abs/2502.06925
share: true
---


> [!tldr] Abstract
> Fine-tuning models that have been pre-trained on large datasets has become a cornerstone of modern machine learning workflows. With the widespread availability of online model repositories, such as Hugging Face, it is now easier than ever to fine-tune pre-trained models for specific tasks. This raises a critical question: which pretrained model is most suitable for a given task? This problem is called transferability estimation. In this work, we introduce two novel and effective metrics for estimating the transferability of pre-trained models. Our approach is grounded in viewing transferability as a measure of how easily a pre-trained model’s representations can be trained to separate target classes, providing a unique perspective on transferability estimation. We rigorously evaluate the proposed metrics against state-of-the-art alternatives across diverse problem settings, demonstrating their robustness and practical utility. Additionally, we present theoretical insights that explain our metrics’ efficacy and adaptability to various scenarios. We experimentally show that our metrics increase Kendall’s Tau by up to 32% compared to the state-of-the-art baselines.



## Notes

[Zotero Link](zotero://select/library/items/HBTA9UUQ)


