---
authors:
  - "[[Mahmoud Assran|Mahmoud Assran]]"
  - "[[Randall Balestriero|Randall Balestriero]]"
  - "[[Quentin Duval|Quentin Duval]]"
  - "[[Florian Bordes|Florian Bordes]]"
  - "[[Ishan Misra|Ishan Misra]]"
  - "[[Piotr Bojanowski|Piotr Bojanowski]]"
  - "[[Pascal Vincent|Pascal Vincent]]"
  - "[[Michael Rabbat|Michael Rabbat]]"
  - "[[Nicolas Ballas|Nicolas Ballas]]"
year: 2022
tags:
  - paper
  - dl_theory
  - ssl
url: http://arxiv.org/abs/2210.07277
share: true
---


> [!tldr] Abstract
> A successful paradigm in representation learning is to perform self-supervised pretraining using tasks based on mini-batch statistics (e.g., SimCLR, VICReg, SwAV, MSN). We show that in the formulation of all these methods is an overlooked prior to learn features that enable uniform clustering of the data. While this prior has led to remarkably semantic representations when pretraining on class-balanced data, such as ImageNet, we demonstrate that it can hamper performance when pretraining on class-imbalanced data. By moving away from conventional uniformity priors and instead preferring power-law distributed feature clusters, we show that one can improve the quality of the learned representations on real-world class-imbalanced datasets. To demonstrate this, we develop an extension of the Masked Siamese Networks (MSN) method to support the use of arbitrary features priors.



## Notes

[Zotero Link](zotero://select/library/items/ZJRMX6LC)


