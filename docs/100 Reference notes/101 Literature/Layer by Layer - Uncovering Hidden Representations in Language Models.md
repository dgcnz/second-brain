---
authors:
  - "[[Oscar Skean|Oscar Skean]]"
  - "[[Md Rifat Arefin|Md Rifat Arefin]]"
  - "[[Dan Zhao|Dan Zhao]]"
  - "[[Niket Patel|Niket Patel]]"
  - "[[Jalal Naghiyev|Jalal Naghiyev]]"
  - "[[Yann LeCun|Yann LeCun]]"
  - "[[Ravid Shwartz-Ziv|Ravid Shwartz-Ziv]]"
year: 2025
tags:
  - paper
  - interpretability
url: http://arxiv.org/abs/2502.02013
share: true
---


> [!tldr] Abstract
> From extracting features to generating text, the outputs of large language models (LLMs) typically rely on the final layers, following the conventional wisdom that earlier layers capture only low-level cues. However, our analysis shows that intermediate layers can encode even richer representations, often improving performance on a range of downstream tasks. To explain and quantify these hidden-layer properties, we propose a unified framework of representation quality metrics based on information theory, geometry, and invariance to input perturbations. Our framework highlights how each layer balances information compression and signal preservation, revealing why mid-depth embeddings can exceed the last layerâ€™s performance. Through extensive experiments on 32 text-embedding tasks across various architectures (transformers, state-space models) and domains (language, vision), we demonstrate that intermediate layers consistently provide stronger features, challenging the standard view on final-layer embeddings and opening new directions on using mid-layer representations for more robust and accurate representations.



## Notes

[Zotero Link](zotero://select/library/items/89CG36XM)


